<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Context Engineering Daily - Context Management</title>
    <link>https://your-username.github.io/context-engineering-news#context_management</link>
    <description>Latest Context Management news in Context Engineering</description>
    <language>en-us</language>
    <item>
      <title>SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention</title>
      <link>https://arxiv.org/abs/2512.20724</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.20724</guid>
      <description>arXiv:2512.20724v1 Announce Type: new 
Abstract: Diffusion based approaches to long form text generation suffer from prohibitive computational cost and memory overhead as sequence length increases. We introduce SA-DiffuSeq, a diffusion framework that integrates sparse attention to fundamentally impr...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; context, arxiv, model, framework, experiment | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 25 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Context Management</category>
      <category>context</category>
      <category>arxiv</category>
      <category>model</category>
    </item>
    <item>
      <title>SHRP: Specialized Head Routing and Pruning for Efficient Encoder Compression</title>
      <link>https://arxiv.org/abs/2512.20635</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.20635</guid>
      <description>arXiv:2512.20635v1 Announce Type: new 
Abstract: Transformer encoders are widely deployed in large-scale web services for natural language understanding tasks such as text classification, semantic retrieval, and content ranking. However, their high inference latency and memory consumption pose signi...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; transformer, arxiv, model, framework, experiment | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 25 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Context Management</category>
      <category>transformer</category>
      <category>arxiv</category>
      <category>model</category>
    </item>
    <item>
      <title>Data-Free Pruning of Self-Attention Layers in LLMs</title>
      <link>https://arxiv.org/abs/2512.20636</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.20636</guid>
      <description>arXiv:2512.20636v1 Announce Type: new 
Abstract: Many self-attention sublayers in large language models (LLMs) can be removed with little to no loss. We attribute this to the Attention Suppression Hypothesis: during pre-training, some deep attention layers learn to mute their own contribution, leavi...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; arxiv, RAG, model, large language model, attention | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 25 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Context Management</category>
      <category>arxiv</category>
      <category>RAG</category>
      <category>model</category>
    </item>
  </channel>
</rss>