<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Context Engineering Daily - Research Papers</title>
    <link>https://your-username.github.io/context-engineering-news#research_papers</link>
    <description>Latest Research Papers news in Context Engineering</description>
    <language>en-us</language>
    <item>
      <title>Table Question Answering in the Era of Large Language Models: A Comprehensive Survey of Tasks, Methods, and Evaluation</title>
      <link>https://arxiv.org/abs/2510.09671</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.09671</guid>
      <description>arXiv:2510.09671v1 Announce Type: new 
Abstract: Table Question Answering (TQA) aims to answer natural language questions about tabular data, often accompanied by additional contexts such as text passages. The task spans diverse settings, varying in table representation, question/answer complexity, ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; API, context, arxiv, research, large language model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 14 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>API</category>
      <category>context</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs Human Fallacy Detection</title>
      <link>https://arxiv.org/abs/2510.09695</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.09695</guid>
      <description>arXiv:2510.09695v1 Announce Type: new 
Abstract: Logical fallacies are common in public communication and can mislead audiences; fallacious arguments may still appear convincing despite lacking soundness, because convincingness is inherently subjective. We present the first computational study of ho...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; context, arxiv, large language model, RAG, LLM | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 14 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>context</category>
      <category>arxiv</category>
      <category>large language model</category>
    </item>
    <item>
      <title>ReaLM: Residual Quantization Bridging Knowledge Graph Embeddings and Large Language Models</title>
      <link>https://arxiv.org/abs/2510.09711</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.09711</guid>
      <description>arXiv:2510.09711v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have recently emerged as a powerful paradigm for Knowledge Graph Completion (KGC), offering strong reasoning and generalization capabilities beyond traditional embedding-based approaches. However, existing LLM-based method...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; experiment, context, vector, arxiv, framework | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 14 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>experiment</category>
      <category>context</category>
      <category>vector</category>
    </item>
    <item>
      <title>VisRAG 2.0: Evidence-Guided Multi-Image Reasoning in Visual Retrieval-Augmented Generation</title>
      <link>https://arxiv.org/abs/2510.09733</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.09733</guid>
      <description>arXiv:2510.09733v1 Announce Type: new 
Abstract: Visual retrieval-augmented generation (VRAG) augments vision-language models (VLMs) with external visual knowledge to ground reasoning and reduce hallucinations. Yet current VRAG systems often fail to reliably perceive and integrate evidence across mu...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; experiment, paper, analysis, retrieval, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 14 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>experiment</category>
      <category>paper</category>
      <category>analysis</category>
    </item>
    <item>
      <title>The Geometry of Reasoning: Flowing Logics in Representation Space</title>
      <link>https://arxiv.org/abs/2510.09782</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.09782</guid>
      <description>arXiv:2510.09782v1 Announce Type: new 
Abstract: We study how large language models (LLMs) ``think&amp;#x27;&amp;#x27; through their representation space. We propose a novel geometric framework that models an LLM&amp;#x27;s reasoning as flows -- embedding trajectories evolving where logic goes. We disentangle logical structur...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; experiment, analysis, tool, arxiv, framework | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 14 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>experiment</category>
      <category>analysis</category>
      <category>tool</category>
    </item>
    <item>
      <title>Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics</title>
      <link>https://arxiv.org/abs/2510.09901</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.09901</guid>
      <description>arXiv:2510.09901v1 Announce Type: new 
Abstract: Computing has long served as a cornerstone of scientific discovery. Recently, a paradigm shift has emerged with the rise of large language models (LLMs), introducing autonomous systems, referred to as agents, that accelerate discovery across varying l...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; experiment, paper, analysis, arxiv, framework | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 14 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>experiment</category>
      <category>paper</category>
      <category>analysis</category>
    </item>
    <item>
      <title>RIPRAG: Hack a Black-box Retrieval-Augmented Generation Question-Answering System with Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2510.10008</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.10008</guid>
      <description>arXiv:2510.10008v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems based on Large Language Models (LLMs) have become a core technology for tasks such as question-answering (QA) and content generation. However, by injecting poisoned documents into the database of RAG system...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; experiment, paper, retrieval, arxiv, framework | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 14 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>experiment</category>
      <category>paper</category>
      <category>retrieval</category>
    </item>
    <item>
      <title>Energy-Driven Steering: Reducing False Refusals in Large Language Models</title>
      <link>https://arxiv.org/abs/2510.08646</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.08646</guid>
      <description>arXiv:2510.08646v1 Announce Type: new 
Abstract: Safety alignment of large language models (LLMs) faces a key challenge: current alignment techniques often only focus on improving safety against harmful prompts, causing LLMs to become over-cautious and refuse to respond to benign prompts. Therefore,...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; experiment, paper, example, arxiv, framework | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 14 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>experiment</category>
      <category>paper</category>
      <category>example</category>
    </item>
    <item>
      <title>Provably Robust Adaptation for Language-Empowered Foundation Models</title>
      <link>https://arxiv.org/abs/2510.08659</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.08659</guid>
      <description>arXiv:2510.08659v1 Announce Type: new 
Abstract: Language-empowered foundation models (LeFMs), such as CLIP and GraphCLIP, have transformed multimodal learning by aligning visual (or graph) features with textual representations, enabling powerful downstream capabilities like few-shot learning. Howev...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; experiment, few-shot, arxiv, multimodal, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 14 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>experiment</category>
      <category>few-shot</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching</title>
      <link>https://arxiv.org/abs/2510.08669</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.08669</guid>
      <description>arXiv:2510.08669v1 Announce Type: new 
Abstract: The application of diffusion transformers is suffering from their significant inference costs. Recently, feature caching has been proposed to solve this problem by reusing features from previous timesteps, thereby skipping computation in future timest...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; experiment, paper, analysis, memory, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 14 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>experiment</category>
      <category>paper</category>
      <category>analysis</category>
    </item>
    <item>
      <title>How Scale Breaks &amp;quot;Normalized Stress&amp;quot; and KL Divergence: Rethinking Quality Metrics</title>
      <link>https://arxiv.org/abs/2510.08660</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.08660</guid>
      <description>arXiv:2510.08660v1 Announce Type: new 
Abstract: Complex, high-dimensional data is ubiquitous across many scientific disciplines, including machine learning, biology, and the social sciences. One of the primary methods of visualizing these datasets is with two-dimensional scatter plots that visually...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; embedding, arxiv, research | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 80%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 14 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>embedding</category>
      <category>arxiv</category>
      <category>research</category>
    </item>
    <item>
      <title>CATS-Linear: Classification Auxiliary Linear Model for Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2510.08661</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.08661</guid>
      <description>arXiv:2510.08661v1 Announce Type: new 
Abstract: Recent research demonstrates that linear models achieve forecasting performance competitive with complex architectures, yet methodologies for enhancing linear models remain underexplored. Motivated by the hypothesis that distinct time series instances...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; experiment, arxiv, framework, research, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 60%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 14 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>experiment</category>
      <category>arxiv</category>
      <category>framework</category>
    </item>
    <item>
      <title>Inner-Instance Normalization for Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2510.08657</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.08657</guid>
      <description>arXiv:2510.08657v1 Announce Type: new 
Abstract: Real-world time series are influenced by numerous factors and exhibit complex non-stationary characteristics. Non-stationarity can lead to distribution shifts, where the statistical properties of time series change over time, negatively impacting mode...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; experiment, arxiv, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 40%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 14 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>experiment</category>
      <category>arxiv</category>
      <category>model</category>
    </item>
  </channel>
</rss>