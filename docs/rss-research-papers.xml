<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Context Engineering Daily - Research Papers</title>
    <link>https://your-username.github.io/context-engineering-news#research_papers</link>
    <description>Latest Research Papers news in Context Engineering</description>
    <language>en-us</language>
    <item>
      <title>Enhancing Reliability across Short and Long-Form QA via Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2512.08944</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.08944</guid>
      <description>arXiv:2512.08944v1 Announce Type: new 
Abstract: While reinforcement learning has unlocked unprecedented complex reasoning in large language models, it has also amplified their propensity for hallucination, creating a critical trade-off between capability and reliability. This work confronts this ch...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; context, reasoning, experiment, model, research | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 11 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>context</category>
      <category>reasoning</category>
      <category>experiment</category>
    </item>
    <item>
      <title>The Linguistic Architecture of Reflective Thought: Evaluation of a Large Language Model as a Tool to Isolate the Formal Structure of Mentalization</title>
      <link>https://arxiv.org/abs/2512.08945</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.08945</guid>
      <description>arXiv:2512.08945v1 Announce Type: new 
Abstract: Background: Mentalization integrates cognitive, affective, and intersubjective components. Large Language Models (LLMs) display an increasing ability to generate reflective texts, raising questions regarding the relationship between linguistic form an...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; context, study, model, large language model, LLM | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 11 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>context</category>
      <category>study</category>
      <category>model</category>
    </item>
    <item>
      <title>MindShift: Analyzing Language Models&amp;#x27; Reactions to Psychological Prompts</title>
      <link>https://arxiv.org/abs/2512.09149</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.09149</guid>
      <description>arXiv:2512.09149v1 Announce Type: new 
Abstract: Large language models (LLMs) hold the potential to absorb and reflect personality traits and attitudes specified by users. In our study, we investigated this potential using robust psychometric measures. We adapted the most studied test in psychologic...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; alignment, study, model, prompt, large language model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 11 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>alignment</category>
      <category>study</category>
      <category>model</category>
    </item>
    <item>
      <title>Targeting Misalignment: A Conflict-Aware Framework for Reward-Model-based LLM Alignment</title>
      <link>https://arxiv.org/abs/2512.09212</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.09212</guid>
      <description>arXiv:2512.09212v1 Announce Type: new 
Abstract: Reward-model-based fine-tuning is a central paradigm in aligning Large Language Models with human preferences. However, such approaches critically rely on the assumption that proxy reward models accurately reflect intended supervision, a condition oft...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; alignment, experiment, model, fine-tuning, paper | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 11 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>alignment</category>
      <category>experiment</category>
      <category>model</category>
    </item>
    <item>
      <title>Training-free Context-adaptive Attention for Efficient Long Context Modeling</title>
      <link>https://arxiv.org/abs/2512.09238</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.09238</guid>
      <description>arXiv:2512.09238v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. Ho...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; context, compression, experiment, paper, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 11 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>context</category>
      <category>compression</category>
      <category>experiment</category>
    </item>
    <item>
      <title>Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study</title>
      <link>https://arxiv.org/abs/2512.09088</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.09088</guid>
      <description>arXiv:2512.09088v1 Announce Type: new 
Abstract: Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users&amp;#x27; trust in LLMs and users&amp;#x27; interaction with LLMs. To explore this in every...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; context, study, model, paper, large language model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 11 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>context</category>
      <category>study</category>
      <category>model</category>
    </item>
    <item>
      <title>A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem</title>
      <link>https://arxiv.org/abs/2512.09117</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.09117</guid>
      <description>arXiv:2512.09117v1 Announce Type: new 
Abstract: This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but c...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, paper, framework, analysis, large language model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 11 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>model</category>
      <category>paper</category>
      <category>framework</category>
    </item>
    <item>
      <title>Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration</title>
      <link>https://arxiv.org/abs/2512.09340</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.09340</guid>
      <description>arXiv:2512.09340v1 Announce Type: new 
Abstract: Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; alignment, reasoning, model, paper, framework | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 11 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>alignment</category>
      <category>reasoning</category>
      <category>model</category>
    </item>
    <item>
      <title>Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search</title>
      <link>https://arxiv.org/abs/2512.09566</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.09566</guid>
      <description>arXiv:2512.09566v1 Announce Type: new 
Abstract: Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, di...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; alignment, context, experiment, model, framework | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 11 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>alignment</category>
      <category>context</category>
      <category>experiment</category>
    </item>
    <item>
      <title>An Electrocardiogram Multi-task Benchmark with Comprehensive Evaluations and Insightful Findings</title>
      <link>https://arxiv.org/abs/2512.08954</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.08954</guid>
      <description>arXiv:2512.08954v1 Announce Type: new 
Abstract: In the process of patient diagnosis, non-invasive measurements are widely used due to their low risks and quick results. Electrocardiogram (ECG), as a non-invasive method to collect heart activities, is used to diagnose cardiac conditions. Analyzing t...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; study, experiment, model, research, analysis | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 11 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>study</category>
      <category>experiment</category>
      <category>model</category>
    </item>
    <item>
      <title>SEA: Spectral Edge Attacks on Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2512.08964</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.08964</guid>
      <description>arXiv:2512.08964v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) achieve strong performance on graph-structured data, but are notoriously vulnerable to small, carefully crafted perturbations of the graph structure. Most existing structure-based attacks rely on gradient-based heuristics ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; experiment, model, embedding, paper, RAG | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 11 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>experiment</category>
      <category>model</category>
      <category>embedding</category>
    </item>
    <item>
      <title>Learning When to Ask: Simulation-Trained Humanoids for Mental-Health Diagnosis</title>
      <link>https://arxiv.org/abs/2512.08952</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.08952</guid>
      <description>arXiv:2512.08952v1 Announce Type: new 
Abstract: Testing humanoid robots with users is slow, causes wear, and limits iteration and diversity. Yet screening agents must master conversational timing, prosody, backchannels, and what to attend to in faces and speech for Depression and PTSD. Most simulat...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; study, RAG, arxiv, prompt | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 80%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 11 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>study</category>
      <category>RAG</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>Analyzing Planner Design Trade-offs for MAPF under Realistic Simulation</title>
      <link>https://arxiv.org/abs/2512.09736</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.09736</guid>
      <description>arXiv:2512.09736v1 Announce Type: new 
Abstract: Multi-Agent Path Finding (MAPF) algorithms are increasingly deployed in industrial warehouses and automated manufacturing facilities, where robots must operate reliably under real-world physical constraints. However, existing MAPF evaluation framework...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; platform, study, model, research, framework | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 60%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 11 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>platform</category>
      <category>study</category>
      <category>model</category>
    </item>
    <item>
      <title>Resolving Conflicts in Lifelong Learning via Aligning Updates in Subspaces</title>
      <link>https://arxiv.org/abs/2512.08960</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.08960</guid>
      <description>arXiv:2512.08960v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) enables efficient Continual Learning but often suffers from catastrophic forgetting due to destructive interference between tasks. Our analysis reveals that this degradation is primarily driven by antagonistic directional up...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; experiment, framework, analysis, vision, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 60%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 11 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>experiment</category>
      <category>framework</category>
      <category>analysis</category>
    </item>
  </channel>
</rss>