{
  "generated_at": "2025-10-22T20:06:26.180258",
  "total_items": 48,
  "items": [
    {
      "title": "Modeling Layered Consciousness with Multi-Agent Large Language Models",
      "url": "https://arxiv.org/abs/2510.17844",
      "description": "arXiv:2510.17844v1 Announce Type: new \nAbstract: We propose a multi-agent framework for modeling artificial consciousness in large language models (LLMs), grounded in psychoanalytic theory. Our \\textbf{Psychodynamic Model} simulates self-awareness, preconsciousness, and unconsciousness through agent...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "framework",
        "fine-tuning",
        "large language model",
        "arxiv",
        "model",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "Outraged AI: Large language models prioritise emotion over cost in fairness enforcement",
      "url": "https://arxiv.org/abs/2510.17880",
      "description": "arXiv:2510.17880v1 Announce Type: new \nAbstract: Emotions guide human decisions, but whether large language models (LLMs) use emotion similarly remains unknown. We tested this using altruistic third-party punishment, where an observer incurs a personal cost to enforce fairness, a hallmark of human m...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "context",
        "prompting",
        "large language model",
        "RAG",
        "GPT",
        "reasoning",
        "model",
        "prompt",
        "LLM",
        "arxiv"
      ],
      "score": 1.0
    },
    {
      "title": "POPI: Personalizing LLMs via Optimized Natural Language Preference Inference",
      "url": "https://arxiv.org/abs/2510.17881",
      "description": "arXiv:2510.17881v1 Announce Type: new \nAbstract: Large language models (LLMs) achieve strong benchmark performance, yet user experiences remain inconsistent due to diverse preferences in style, tone, and reasoning mode. Nevertheless, existing alignment techniques such as reinforcement learning from ...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "in-context",
        "experiment",
        "framework",
        "fine-tuning",
        "alignment",
        "RAG",
        "large language model",
        "RLHF",
        "reasoning",
        "model",
        "context",
        "arxiv"
      ],
      "score": 1.0
    },
    {
      "title": "Advances in Pre-trained Language Models for Domain-Specific Text Classification: A Systematic Review",
      "url": "https://arxiv.org/abs/2510.17892",
      "description": "arXiv:2510.17892v1 Announce Type: new \nAbstract: The exponential increase in scientific literature and online information necessitates efficient methods for extracting knowledge from textual data. Natural language processing (NLP) plays a crucial role in addressing this challenge, particularly in te...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "tool",
        "experiment",
        "transformer",
        "API",
        "research",
        "large language model",
        "study",
        "arxiv",
        "model",
        "ICL",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "Atomic Literary Styling: Mechanistic Manipulation of Prose Generation in Neural Language Models",
      "url": "https://arxiv.org/abs/2510.17909",
      "description": "arXiv:2510.17909v1 Announce Type: new \nAbstract: We present a mechanistic analysis of literary style in GPT-2, identifying individual neurons that discriminate between exemplary prose and rigid AI-generated text. Using Herman Melville's Bartleby, the Scrivener as a corpus, we extract activation patt...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "alignment",
        "research",
        "GPT",
        "arxiv",
        "model",
        "analysis"
      ],
      "score": 1.0
    },
    {
      "title": "JT-Safe: Intrinsically Enhancing the Safety and Trustworthiness of LLMs",
      "url": "https://arxiv.org/abs/2510.17918",
      "description": "arXiv:2510.17918v1 Announce Type: new \nAbstract: The hallucination and credibility concerns of large language models (LLMs) are global challenges that the industry is collectively addressing. Recently, a significant amount of advances have been made on post-training and inference techniques to mitig...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "paper",
        "RAG",
        "large language model",
        "arxiv",
        "model",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections",
      "url": "https://arxiv.org/abs/2510.17921",
      "description": "arXiv:2510.17921v1 Announce Type: new \nAbstract: Recent advances in enhancing the reasoning ability of large language models (LLMs) have been remarkably successful. LLMs trained with reinforcement learning (RL) for reasoning demonstrate strong performance in challenging tasks such as mathematics and...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "attention",
        "research",
        "RAG",
        "large language model",
        "reasoning",
        "model",
        "prompt",
        "LLM",
        "arxiv"
      ],
      "score": 1.0
    },
    {
      "title": "Select-Then-Decompose: From Empirical Analysis to Adaptive Selection Strategy for Task Decomposition in Large Language Models",
      "url": "https://arxiv.org/abs/2510.17922",
      "description": "arXiv:2510.17922v1 Announce Type: new \nAbstract: Large language models (LLMs) have demonstrated remarkable reasoning and planning capabilities, driving extensive research into task decomposition. Existing task decomposition methods focus primarily on memory, tool usage, and feedback mechanisms, achi...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "tool",
        "analysis",
        "memory",
        "research",
        "large language model",
        "study",
        "reasoning",
        "model",
        "ICL",
        "LLM",
        "arxiv"
      ],
      "score": 1.0
    },
    {
      "title": "Efficient Toxicity Detection in Gaming Chats: A Comparative Study of Embeddings, Fine-Tuned Transformers and LLMs",
      "url": "https://arxiv.org/abs/2510.17924",
      "description": "arXiv:2510.17924v1 Announce Type: new \nAbstract: This paper presents a comprehensive comparative analysis of Natural Language Processing (NLP) methods for automated toxicity detection in online gaming chats. Traditional machine learning models with embeddings, large language models (LLMs) with zero-...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "paper",
        "transformer",
        "prompting",
        "analysis",
        "framework",
        "augmented",
        "embedding",
        "zero-shot",
        "few-shot",
        "large language model",
        "RAG",
        "study",
        "arxiv",
        "model",
        "prompt",
        "LLM",
        "retrieval"
      ],
      "score": 1.0
    },
    {
      "title": "Activation Manifold Projection: Liberating Task-Specific Behaviors from LLM Architectures",
      "url": "https://arxiv.org/abs/2510.17902",
      "description": "arXiv:2510.17902v1 Announce Type: new \nAbstract: The proliferation of Large Language Model (LLM) architectures presents a fundamental challenge: valuable, task-specific behaviors learned through fine-tuning methods like Low-Rank Adaptation (LoRA) are effectively trapped within their source model's a...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "paper",
        "framework",
        "fine-tuning",
        "zero-shot",
        "large language model",
        "arxiv",
        "model",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "Beyond More Context: Retrieval Diversity Boosts Multi-Turn Intent Understanding",
      "url": "https://arxiv.org/abs/2510.17940",
      "description": "arXiv:2510.17940v1 Announce Type: new \nAbstract: Multi turn intent understanding is central to task oriented chatbots, yet real deployments face tight token budgets and noisy contexts, and most retrieval pipelines emphasize relevance while overlooking set level diversity and confounds such as more c...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "LLM",
        "framework",
        "RAG",
        "study",
        "arxiv",
        "prompt",
        "context",
        "retrieval"
      ],
      "score": 1.0
    },
    {
      "title": "FABRIC: Framework for Agent-Based Realistic Intelligence Creation",
      "url": "https://arxiv.org/abs/2510.17995",
      "description": "arXiv:2510.17995v1 Announce Type: new \nAbstract: Large language models (LLMs) are increasingly deployed as agents, expected to decompose goals, invoke tools, and verify results in dynamic environments. Realizing these capabilities requires access to agentic data- structured interaction records that ...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "tool",
        "paper",
        "framework",
        "alignment",
        "vision",
        "large language model",
        "arxiv",
        "model",
        "prompt",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "OPTAGENT: Optimizing Multi-Agent LLM Interactions Through Verbal Reinforcement Learning for Enhanced Reasoning",
      "url": "https://arxiv.org/abs/2510.18032",
      "description": "arXiv:2510.18032v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have shown remarkable reasoning capabilities in mathematical and scientific tasks. To enhance complex reasoning, multi-agent systems have been proposed to harness the collective intelligence of LLM agents. However, existin...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "prompting",
        "framework",
        "large language model",
        "reasoning",
        "model",
        "prompt",
        "LLM",
        "arxiv"
      ],
      "score": 1.0
    },
    {
      "title": "CompactPrompt: A Unified Pipeline for Prompt Data Compression in LLM Workflows",
      "url": "https://arxiv.org/abs/2510.18043",
      "description": "arXiv:2510.18043v1 Announce Type: new \nAbstract: Large Language Models (LLMs) deliver powerful reasoning and generation capabilities but incur substantial run-time costs when operating in agentic workflows that chain together lengthy prompts and process rich data streams. We introduce CompactPrompt,...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "large language model",
        "GPT",
        "reasoning",
        "model",
        "prompt",
        "LLM",
        "arxiv",
        "compression"
      ],
      "score": 1.0
    },
    {
      "title": "Planned Diffusion",
      "url": "https://arxiv.org/abs/2510.18087",
      "description": "arXiv:2510.18087v1 Announce Type: new \nAbstract: A central challenge in large language model inference is the trade-off between generation speed and output quality. Autoregressive models produce high-quality text but generate tokens sequentially. Diffusion models can generate tokens in parallel but ...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "large language model",
        "arxiv",
        "prompt",
        "instruction",
        "model",
        "analysis"
      ],
      "score": 1.0
    },
    {
      "title": "SMaRT: Select, Mix, and ReinvenT - A Strategy Fusion Framework for LLM-Driven Reasoning and Planning",
      "url": "https://arxiv.org/abs/2510.18095",
      "description": "arXiv:2510.18095v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have redefined complex task automation with exceptional generalization capabilities. Despite these advancements, state-of-the-art methods rely on single-strategy prompting, missing the synergy of diverse reasoning approach...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "prompting",
        "framework",
        "large language model",
        "reasoning",
        "model",
        "prompt",
        "LLM",
        "arxiv"
      ],
      "score": 1.0
    },
    {
      "title": "Measuring Reasoning in LLMs: a New Dialectical Angle",
      "url": "https://arxiv.org/abs/2510.18134",
      "description": "arXiv:2510.18134v1 Announce Type: new \nAbstract: What does it truly mean for a language model to \"reason\"? Most current evaluations and benchmarks reward models' correct standalone answers--but correctness alone reveals little about the process that produced them. In this work, we explore a differen...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "chain_of_thought",
      "keywords": [
        "framework",
        "GPT",
        "reasoning",
        "model",
        "LLM",
        "arxiv"
      ],
      "score": 1.0
    },
    {
      "title": "Learning from Generalization Patterns: An Evaluation-Driven Approach to Enhanced Data Augmentation for Fine-Tuning Small Language Models",
      "url": "https://arxiv.org/abs/2510.18143",
      "description": "arXiv:2510.18143v1 Announce Type: new \nAbstract: Small Language Models (SLMs) offer compelling advantages in deployment cost and latency, but their accuracy often lags behind larger models, particularly for complex domain-specific tasks. While supervised fine-tuning can help bridge this performance ...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "experiment",
        "fine-tuning",
        "arxiv",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "GRETEL: A Goal-driven Retrieval and Execution-based Trial Framework for LLM Tool Selection Enhancing",
      "url": "https://arxiv.org/abs/2510.17843",
      "description": "arXiv:2510.17843v1 Announce Type: new \nAbstract: Despite remarkable advances in Large Language Model capabilities, tool retrieval for agent-based systems remains fundamentally limited by reliance on semantic similarity, which fails to capture functional viability. Current methods often retrieve text...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "tool",
        "framework",
        "large language model",
        "arxiv",
        "model",
        "LLM",
        "retrieval"
      ],
      "score": 1.0
    },
    {
      "title": "CARLE: A Hybrid Deep-Shallow Learning Framework for Robust and Explainable RUL Estimation of Rolling Element Bearings",
      "url": "https://arxiv.org/abs/2510.17846",
      "description": "arXiv:2510.17846v1 Announce Type: new \nAbstract: Prognostic Health Management (PHM) systems monitor and predict equipment health. A key task is Remaining Useful Life (RUL) estimation, which predicts how long a component, such as a rolling element bearing, will operate before failure. Many RUL method...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "attention",
        "paper",
        "framework",
        "arxiv",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Hierarchical Federated Unlearning for Large Language Models",
      "url": "https://arxiv.org/abs/2510.17895",
      "description": "arXiv:2510.17895v1 Announce Type: new \nAbstract: Large Language Models (LLMs) are increasingly integrated into real-world applications, raising concerns about privacy, security and the need to remove undesirable knowledge. Machine Unlearning has emerged as a promising solution, yet faces two key cha...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "experiment",
        "large language model",
        "arxiv",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Long-Context Attention Benchmark: From Kernel Efficiency to Distributed Context Parallelism",
      "url": "https://arxiv.org/abs/2510.17896",
      "description": "arXiv:2510.17896v1 Announce Type: new \nAbstract: Transformer-based large language models (LLMs) have achieved remarkable success, yet their standard attention mechanism incurs quadratic computation and memory costs with respect to sequence length, posing a major bottleneck for long-context training....",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "experiment",
        "attention",
        "transformer",
        "framework",
        "memory",
        "large language model",
        "arxiv",
        "model",
        "context",
        "analysis"
      ],
      "score": 1.0
    },
    {
      "title": "L-MoE: End-to-End Training of a Lightweight Mixture of Low-Rank Adaptation Experts",
      "url": "https://arxiv.org/abs/2510.17898",
      "description": "arXiv:2510.17898v1 Announce Type: new \nAbstract: The Mixture of Experts (MoE) architecture enables the scaling of Large Language Models (LLMs) to trillions of parameters by activating a sparse subset of weights for each input, maintaining constant computational cost during inference. Concurrently, L...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "framework",
        "fine-tuning",
        "RAG",
        "large language model",
        "arxiv",
        "model",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "Automated Algorithm Design for Auto-Tuning Optimizers",
      "url": "https://arxiv.org/abs/2510.17899",
      "description": "arXiv:2510.17899v1 Announce Type: new \nAbstract: Automatic performance tuning (auto-tuning) is essential for optimizing high-performance applications, where vast and irregular parameter spaces make manual exploration infeasible. Traditionally, auto-tuning relies on well-established optimization algo...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "framework",
        "platform",
        "RAG",
        "large language model",
        "arxiv",
        "model",
        "prompt",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "The Sherpa.ai Blind Vertical Federated Learning Paradigm to Minimize the Number of Communications",
      "url": "https://arxiv.org/abs/2510.17901",
      "description": "arXiv:2510.17901v1 Announce Type: new \nAbstract: Federated Learning (FL) enables collaborative decentralized training across multiple parties (nodes) while keeping raw data private. There are two main paradigms in FL: Horizontal FL (HFL), where all participant nodes share the same feature space but ...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "paper",
        "RAG",
        "arxiv",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Context-Engineering - \"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" — Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "url": "https://github.com/davidkimai/Context-Engineering",
      "description": "\"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" — Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "published_date": "2025-06-29T00:16:36+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "prompt engineering",
        "prompt",
        "context window",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "ThinkSound - [NeurIPS 2025] PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "url": "https://github.com/FunAudioLLM/ThinkSound",
      "description": "[NeurIPS 2025] PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "published_date": "2025-06-27T02:27:00+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "chain-of-thought",
        "audio",
        "framework",
        "CoT",
        "reasoning"
      ],
      "score": 1.0
    },
    {
      "title": "mcp-context-forge - A Model Context Protocol (MCP) Gateway & Registry. Serves as a central management point for tools, resources, and prompts that can be accessed by MCP-compatible LLM applications. Converts REST API endpoints to MCP, composes virtual MCP servers with added security and observability, and converts between protocols (stdio, SSE, Streamable HTTP).",
      "url": "https://github.com/IBM/mcp-context-forge",
      "description": "A Model Context Protocol (MCP) Gateway & Registry. Serves as a central management point for tools, resources, and prompts that can be accessed by MCP-compatible LLM applications. Converts REST API endpoints to MCP, composes virtual MCP servers with added security and observability, and converts between protocols (stdio, SSE, Streamable HTTP).",
      "published_date": "2025-05-08T08:16:59+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "LLM",
        "tool",
        "API",
        "model",
        "prompt",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "Cline-Recursive-Chain-of-Thought-System-CRCT- - A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "url": "https://github.com/RPG-fan/Cline-Recursive-Chain-of-Thought-System-CRCT-",
      "description": "A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "published_date": "2025-02-18T15:45:30+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "chain-of-thought",
        "framework",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "LightRAG - [EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "url": "https://github.com/HKUDS/LightRAG",
      "description": "[EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "published_date": "2024-10-02T11:57:54+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "retrieval",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "KAG - KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "url": "https://github.com/OpenSPG/KAG",
      "description": "KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "published_date": "2024-09-21T13:56:44+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "vector",
        "framework",
        "knowledge base",
        "RAG",
        "reasoning",
        "model",
        "LLM",
        "retrieval"
      ],
      "score": 1.0
    },
    {
      "title": "Kiln - The easiest tool for fine-tuning LLM models, synthetic data generation, and collaborating on datasets.",
      "url": "https://github.com/Kiln-AI/Kiln",
      "description": "The easiest tool for fine-tuning LLM models, synthetic data generation, and collaborating on datasets.",
      "published_date": "2024-07-23T23:10:13+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "tool",
        "model",
        "LLM",
        "fine-tuning"
      ],
      "score": 1.0
    },
    {
      "title": "graphrag - A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "url": "https://github.com/microsoft/graphrag",
      "description": "A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "published_date": "2024-03-27T17:57:52+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "retrieval",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "R2R - SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "url": "https://github.com/SciPhi-AI/R2R",
      "description": "SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "published_date": "2024-02-12T03:24:27+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "augmented",
        "API",
        "product",
        "RAG",
        "retrieval"
      ],
      "score": 1.0
    },
    {
      "title": "openlit - Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. 🚀💻 Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "url": "https://github.com/openlit/openlit",
      "description": "Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. 🚀💻 Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "published_date": "2024-01-23T17:40:59+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "vector",
        "framework",
        "prompt",
        "platform",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "AutoRAG - AutoRAG: An Open-Source Framework for Retrieval-Augmented Generation (RAG) Evaluation & Optimization with AutoML-Style Automation",
      "url": "https://github.com/Marker-Inc-Korea/AutoRAG",
      "description": "AutoRAG: An Open-Source Framework for Retrieval-Augmented Generation (RAG) Evaluation & Optimization with AutoML-Style Automation",
      "published_date": "2024-01-10T12:25:00+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "framework",
        "RAG",
        "retrieval",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "Diagnosing Representation Dynamics in NER Model Extension",
      "url": "https://arxiv.org/abs/2510.17930",
      "description": "arXiv:2510.17930v1 Announce Type: new \nAbstract: Extending Named Entity Recognition (NER) models to new PII entities in noisy spoken-language data is a common need. We find that jointly fine-tuning a BERT model on standard semantic entities (PER, LOC, ORG) and new pattern-based PII (EMAIL, PHONE) re...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "industry_news",
      "keywords": [
        "tool",
        "fine-tuning",
        "release",
        "arxiv",
        "model"
      ],
      "score": 0.8
    },
    {
      "title": "fastapi_mcp - Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "url": "https://github.com/tadata-org/fastapi_mcp",
      "description": "Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "published_date": "2025-03-08T11:15:43+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "API",
        "tool",
        "model",
        "context"
      ],
      "score": 0.8
    },
    {
      "title": "cosmos-reason1 - Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "url": "https://github.com/nvidia-cosmos/cosmos-reason1",
      "description": "Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "published_date": "2025-03-02T15:23:55+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "reasoning",
        "chain-of-thought",
        "model"
      ],
      "score": 0.8
    },
    {
      "title": "Subject-Event Ontology Without Global Time: Foundations and Execution Semantics",
      "url": "https://arxiv.org/abs/2510.18040",
      "description": "arXiv:2510.18040v1 Announce Type: new \nAbstract: A formalization of a subject-event ontology is proposed for modeling complex dynamic systems without reliance on global time. Key principles: (1) event as an act of fixation - a subject discerns and fixes changes according to models (conceptual templa...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "attention",
        "template",
        "arxiv",
        "platform",
        "model"
      ],
      "score": 0.6
    },
    {
      "title": "excel-mcp-server - A Model Context Protocol server for Excel file manipulation",
      "url": "https://github.com/haris-musa/excel-mcp-server",
      "description": "A Model Context Protocol server for Excel file manipulation",
      "published_date": "2025-02-12T06:39:48+00:00",
      "source": "GitHub",
      "category": "industry_news",
      "keywords": [
        "model",
        "context"
      ],
      "score": 0.6
    },
    {
      "title": "mcp-agent - Build effective agents using Model Context Protocol and simple workflow patterns",
      "url": "https://github.com/lastmile-ai/mcp-agent",
      "description": "Build effective agents using Model Context Protocol and simple workflow patterns",
      "published_date": "2024-12-18T01:55:10+00:00",
      "source": "GitHub",
      "category": "industry_news",
      "keywords": [
        "model",
        "context"
      ],
      "score": 0.6
    },
    {
      "title": "AlphaCodium - Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "url": "https://github.com/Codium-ai/AlphaCodium",
      "description": "Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "published_date": "2024-01-14T15:17:18+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "prompt engineering",
        "prompt",
        "paper"
      ],
      "score": 0.6
    },
    {
      "title": "MIN-Merging: Merge the Important Neurons for Model Merging",
      "url": "https://arxiv.org/abs/2510.17890",
      "description": "arXiv:2510.17890v1 Announce Type: new \nAbstract: Recent advances in deep learning have led to a surge of open-source models across diverse domains. While model merging offers a promising way to combine their strengths, existing approaches often suffer from parameter conflicts that degrade performanc...",
      "published_date": "2025-10-22T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "framework",
        "vision",
        "arxiv",
        "model"
      ],
      "score": 0.4
    },
    {
      "title": "Sentence Transformers is joining Hugging Face!",
      "url": "https://huggingface.co/blog/sentence-transformers-joins-hf",
      "description": "...",
      "published_date": "2025-10-22T00:00:00",
      "source": "Hugging Face Blog",
      "category": "prompt_engineering",
      "keywords": [
        "transformer"
      ],
      "score": 0.4
    },
    {
      "title": "optillm - Optimizing inference proxy for LLMs",
      "url": "https://github.com/codelion/optillm",
      "description": "Optimizing inference proxy for LLMs",
      "published_date": "2024-08-22T19:46:07+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "LLM"
      ],
      "score": 0.4
    },
    {
      "title": "Supercharge your OCR Pipelines with Open Models",
      "url": "https://huggingface.co/blog/ocr-open-models",
      "description": "...",
      "published_date": "2025-10-21T00:00:00",
      "source": "Hugging Face Blog",
      "category": "industry_news",
      "keywords": [
        "model"
      ],
      "score": 0.2
    },
    {
      "title": "Unlock the power of images with AI Sheets",
      "url": "https://huggingface.co/blog/aisheets-unlock-images",
      "description": "...",
      "published_date": "2025-10-21T00:00:00",
      "source": "Hugging Face Blog",
      "category": "multimodal_context",
      "keywords": [
        "image"
      ],
      "score": 0.2
    }
  ]
}