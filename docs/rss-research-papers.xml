<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Context Engineering Daily - Research Papers</title>
    <link>https://your-username.github.io/context-engineering-news#research_papers</link>
    <description>Latest Research Papers news in Context Engineering</description>
    <language>en-us</language>
    <item>
      <title>The Perplexity Paradox: Why Code Compresses Better Than Math in LLM Prompts</title>
      <link>https://arxiv.org/abs/2602.15843</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.15843</guid>
      <description>arXiv:2602.15843v1 Announce Type: new 
Abstract: In &amp;quot;Compress or Route?&amp;quot; (Johnson, 2026), we found that code generation tolerates aggressive prompt compression (r &amp;gt;= 0.6) while chain-of-thought reasoning degrades gradually. That study was limited to HumanEval (164 problems), left the &amp;quot;perplexity par...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; prompt, chain-of-thought, study, paper, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 19 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>prompt</category>
      <category>chain-of-thought</category>
      <category>study</category>
    </item>
    <item>
      <title>KD4MT: A Survey of Knowledge Distillation for Machine Translation</title>
      <link>https://arxiv.org/abs/2602.15845</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.15845</guid>
      <description>arXiv:2602.15845v1 Announce Type: new 
Abstract: Knowledge Distillation (KD) as a research area has gained a lot of traction in recent years as a compression tool to address challenges related to ever-larger models in NLP. Remarkably, Machine Translation (MT) offers a much more nuanced take on this ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; research, model, paper, arxiv, tool | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 19 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>research</category>
      <category>model</category>
      <category>paper</category>
    </item>
    <item>
      <title>Do Personality Traits Interfere? Geometric Limitations of Steering in Large Language Models</title>
      <link>https://arxiv.org/abs/2602.15847</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.15847</guid>
      <description>arXiv:2602.15847v1 Announce Type: new 
Abstract: Personality steering in large language models (LLMs) commonly relies on injecting trait-specific steering vectors, implicitly assuming that personality traits can be controlled independently. In this work, we examine whether this assumption holds by a...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; vector, study, arxiv, model, LLM | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 19 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>vector</category>
      <category>study</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>Can LLMs Assess Personality? Validating Conversational AI for Trait Profiling</title>
      <link>https://arxiv.org/abs/2602.15848</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.15848</guid>
      <description>arXiv:2602.15848v1 Announce Type: new 
Abstract: This study validates Large Language Models (LLMs) as a dynamic alternative to questionnaire-based personality assessment. Using a within-subjects experiment (N=33), we compared Big Five personality scores derived from guided LLM conversations against ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; study, experiment, arxiv, model, LLM | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 19 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>study</category>
      <category>experiment</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>Narrative Theory-Driven LLM Methods for Automatic Story Generation and Understanding: A Survey</title>
      <link>https://arxiv.org/abs/2602.15851</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.15851</guid>
      <description>arXiv:2602.15851v1 Announce Type: new 
Abstract: Applications of narrative theories using large language models (LLMs) deliver promising use-cases in automatic story generation and understanding tasks. Our survey examines how natural language processing (NLP) research engages with fields of narrativ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; fine-tuning, prompt, context, research, experiment | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 19 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>fine-tuning</category>
      <category>prompt</category>
      <category>context</category>
    </item>
    <item>
      <title>How Uncertain Is the Grade? A Benchmark of Uncertainty Metrics for LLM-Based Automatic Assessment</title>
      <link>https://arxiv.org/abs/2602.16039</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.16039</guid>
      <description>arXiv:2602.16039v1 Announce Type: new 
Abstract: The rapid rise of large language models (LLMs) is reshaping the landscape of automatic assessment in education. While these systems demonstrate substantial advantages in adaptability to diverse question types and flexibility in output formats, they al...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; context, research, study, model, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 19 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>context</category>
      <category>research</category>
      <category>study</category>
    </item>
    <item>
      <title>Improving Interactive In-Context Learning from Natural Language Feedback</title>
      <link>https://arxiv.org/abs/2602.16066</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.16066</guid>
      <description>arXiv:2602.16066v1 Announce Type: new 
Abstract: Adapting one&amp;#x27;s thought process based on corrective feedback is an essential ability in human learning, particularly in collaborative settings. In contrast, the current large language model training paradigm relies heavily on modeling vast, static corp...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; framework, context, in-context, arxiv, analysis | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 19 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>framework</category>
      <category>context</category>
      <category>in-context</category>
    </item>
    <item>
      <title>Revolutionizing Long-Term Memory in AI: New Horizons with High-Capacity and High-Speed Storage</title>
      <link>https://arxiv.org/abs/2602.16192</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.16192</guid>
      <description>arXiv:2602.16192v1 Announce Type: new 
Abstract: Driven by our mission of &amp;quot;uplifting the world with memory,&amp;quot; this paper explores the design concept of &amp;quot;memory&amp;quot; that is essential for achieving artificial superintelligence (ASI). Rather than proposing novel methods, we focus on several alternative app...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; research, experiment, paper, RAG, memory | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 19 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>research</category>
      <category>experiment</category>
      <category>paper</category>
    </item>
    <item>
      <title>Memes-as-Replies: Can Models Select Humorous Manga Panel Responses?</title>
      <link>https://arxiv.org/abs/2602.15842</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.15842</guid>
      <description>arXiv:2602.15842v1 Announce Type: new 
Abstract: Memes are a popular element of modern web communication, used not only as static artifacts but also as interactive replies within conversations. While computational research has focused on analyzing the intrinsic properties of memes, the dynamic and c...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; context, research, arxiv, analysis, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 19 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>context</category>
      <category>research</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>IT-OSE: Exploring Optimal Sample Size for Industrial Data Augmentation</title>
      <link>https://arxiv.org/abs/2602.15878</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.15878</guid>
      <description>arXiv:2602.15878v1 Announce Type: new 
Abstract: In industrial scenarios, data augmentation is an effective approach to improve model performance. However, its benefits are not unidirectionally beneficial. There is no theoretical research or established estimation for the optimal sample size (OSS) i...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; research, experiment, RAG, arxiv, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 19 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>research</category>
      <category>experiment</category>
      <category>RAG</category>
    </item>
    <item>
      <title>Building Safe and Deployable Clinical Natural Language Processing under Temporal Leakage Constraints</title>
      <link>https://arxiv.org/abs/2602.15852</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.15852</guid>
      <description>arXiv:2602.15852v1 Announce Type: new 
Abstract: Clinical natural language processing (NLP) models have shown promise for supporting hospital discharge planning by leveraging narrative clinical documentation. However, note-based models are particularly vulnerable to temporal and lexical leakage, whe...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; RAG, study, arxiv, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 80%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 19 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>RAG</category>
      <category>study</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>R$^2$Energy: A Large-Scale Benchmark for Robust Renewable Energy Forecasting under Diverse and Extreme Conditions</title>
      <link>https://arxiv.org/abs/2602.15961</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.15961</guid>
      <description>arXiv:2602.15961v1 Announce Type: new 
Abstract: The rapid expansion of renewable energy, particularly wind and solar power, has made reliable forecasting critical for power system operations. While recent deep learning models have achieved strong average accuracy, the increasing frequency and inten...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; paper, RAG, arxiv, API, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 80%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 19 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>paper</category>
      <category>RAG</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>Towards Efficient Constraint Handling in Neural Solvers for Routing Problems</title>
      <link>https://arxiv.org/abs/2602.16012</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.16012</guid>
      <description>arXiv:2602.16012v1 Announce Type: new 
Abstract: Neural solvers have achieved impressive progress in addressing simple routing problems, particularly excelling in computational efficiency. However, their advantages under complex constraints remain nascent, for which current constraint-handling schem...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; arxiv, paper, framework | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 40%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 19 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>arxiv</category>
      <category>paper</category>
      <category>framework</category>
    </item>
    <item>
      <title>Genetic Generalized Additive Models</title>
      <link>https://arxiv.org/abs/2602.15877</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.15877</guid>
      <description>arXiv:2602.15877v1 Announce Type: new 
Abstract: Generalized Additive Models (GAMs) balance predictive accuracy and interpretability, but manually configuring their structure is challenging. We propose using the multi-objective genetic algorithm NSGA-II to automatically optimize GAMs, jointly minimi...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, experiment, arxiv, framework | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 40%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 19 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>model</category>
      <category>experiment</category>
      <category>arxiv</category>
    </item>
  </channel>
</rss>