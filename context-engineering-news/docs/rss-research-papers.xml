<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Context Engineering Daily - Research Papers</title>
    <link>https://your-username.github.io/context-engineering-news#research_papers</link>
    <description>Latest Research Papers news in Context Engineering</description>
    <language>en-us</language>
    <item>
      <title>RAGVUE: A Diagnostic View for Explainable and Automated Evaluation of Retrieval-Augmented Generation</title>
      <link>https://arxiv.org/abs/2601.04196</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.04196</guid>
      <description>arXiv:2601.04196v1 Announce Type: new 
Abstract: Evaluating Retrieval-Augmented Generation (RAG) systems remains a challenging task: existing metrics often collapse heterogeneous behaviors into single scores and provide little insight into whether errors arise from retrieval,reasoning, or grounding....&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; reasoning, paper, augmented, API, framework | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 09 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>reasoning</category>
      <category>paper</category>
      <category>augmented</category>
    </item>
    <item>
      <title>Automatic Construction of Chinese Verb Collostruction Database</title>
      <link>https://arxiv.org/abs/2601.04197</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.04197</guid>
      <description>arXiv:2601.04197v1 Announce Type: new 
Abstract: This paper proposes a fully unsupervised approach to the construction of verb collostruction database for Chinese language, aimed at complementing LLMs by providing explicit and interpretable rules for application scenarios where explanation and inter...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; paper, analysis, LLM, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 09 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>paper</category>
      <category>analysis</category>
      <category>LLM</category>
    </item>
    <item>
      <title>FronTalk: Benchmarking Front-End Development as Conversational Code Generation with Multi-Modal Feedback</title>
      <link>https://arxiv.org/abs/2601.04203</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.04203</guid>
      <description>arXiv:2601.04203v1 Announce Type: new 
Abstract: We present FronTalk, a benchmark for front-end code generation that pioneers the study of a unique interaction dynamic: conversational code generation with multi-modal feedback. In front-end development, visual artifacts such as sketches, mockups and ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; framework, study, vision, research, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 09 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>framework</category>
      <category>study</category>
      <category>vision</category>
    </item>
    <item>
      <title>Ideology as a Problem: Lightweight Logit Steering for Annotator-Specific Alignment in Social Media Analysis</title>
      <link>https://arxiv.org/abs/2601.04207</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.04207</guid>
      <description>arXiv:2601.04207v1 Announce Type: new 
Abstract: LLMs internally organize political ideology along low-dimensional structures that are partially, but not fully aligned with human ideological space. This misalignment is systematic, model specific, and measurable. We introduce a lightweight linear pro...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; reasoning, paper, analysis, LLM, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 09 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>reasoning</category>
      <category>paper</category>
      <category>analysis</category>
    </item>
    <item>
      <title>Digital Red Queen: Adversarial Program Evolution in Core War with LLMs</title>
      <link>https://arxiv.org/abs/2601.03335</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.03335</guid>
      <description>arXiv:2601.03335v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly being used to evolve solutions to problems in many domains, in a process inspired by biological evolution. However, unlike biological evolution, most LLM-evolution frameworks are formulated as static optim...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; large language model, framework, study, LLM, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 09 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>large language model</category>
      <category>framework</category>
      <category>study</category>
    </item>
    <item>
      <title>Personalization of Large Foundation Models for Health Interventions</title>
      <link>https://arxiv.org/abs/2601.03482</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.03482</guid>
      <description>arXiv:2601.03482v1 Announce Type: new 
Abstract: Large foundation models (LFMs) transform healthcare AI in prevention, diagnostics, and treatment. However, whether LFMs can provide truly personalized treatment recommendations remains an open question. Recent research has revealed multiple challenges...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; multimodal, paper, API, framework, experiment | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 09 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>multimodal</category>
      <category>paper</category>
      <category>API</category>
    </item>
    <item>
      <title>STAR-S: Improving Safety Alignment through Self-Taught Reasoning on Safety Rules</title>
      <link>https://arxiv.org/abs/2601.03537</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.03537</guid>
      <description>arXiv:2601.03537v1 Announce Type: new 
Abstract: Defending against jailbreak attacks is crucial for the safe deployment of Large Language Models (LLMs). Recent research has attempted to improve safety by training models to reason over safety rules before responding. However, a key issue lies in dete...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; large language model, reasoning, prompt, framework, experiment | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 09 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>large language model</category>
      <category>reasoning</category>
      <category>prompt</category>
    </item>
    <item>
      <title>The Forgotten Shield: Safety Grafting in Parameter-Space for Medical MLLMs</title>
      <link>https://arxiv.org/abs/2601.04199</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.04199</guid>
      <description>arXiv:2601.04199v1 Announce Type: new 
Abstract: Medical Multimodal Large Language Models (Medical MLLMs) have achieved remarkable progress in specialized medical tasks; however, research into their safety has lagged, posing potential risks for real-world deployment. In this paper, we first establis...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; large language model, multimodal, cross-modal, paper, framework | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 09 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>large language model</category>
      <category>multimodal</category>
      <category>cross-modal</category>
    </item>
    <item>
      <title>Safety-Utility Conflicts Are Not Global: Surgical Alignment via Head-Level Diagnosis</title>
      <link>https://arxiv.org/abs/2601.04262</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.04262</guid>
      <description>arXiv:2601.04262v1 Announce Type: new 
Abstract: Safety alignment in Large Language Models (LLMs) inherently presents a multi-objective optimization conflict, often accompanied by an unintended degradation of general capabilities. Existing mitigation strategies typically rely on global gradient geom...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; large language model, attention, transformer, framework, experiment | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 09 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>large language model</category>
      <category>attention</category>
      <category>transformer</category>
    </item>
    <item>
      <title>MemKD: Memory-Discrepancy Knowledge Distillation for Efficient Time Series Classification</title>
      <link>https://arxiv.org/abs/2601.04264</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.04264</guid>
      <description>arXiv:2601.04264v1 Announce Type: new 
Abstract: Deep learning models, particularly recurrent neural networks and their variants, such as long short-term memory, have significantly advanced time series data analysis. These models capture complex, sequential patterns in time series, enabling real-tim...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; platform, framework, experiment, analysis, vision | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 09 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>platform</category>
      <category>framework</category>
      <category>experiment</category>
    </item>
    <item>
      <title>Unlocking the Pre-Trained Model as a Dual-Alignment Calibrator for Post-Trained LLMs</title>
      <link>https://arxiv.org/abs/2601.04277</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.04277</guid>
      <description>arXiv:2601.04277v1 Announce Type: new 
Abstract: Post-training improves large language models (LLMs) but often worsens confidence calibration, leading to systematic overconfidence. Recent unsupervised post-hoc methods for post-trained LMs (PoLMs) mitigate this by aligning PoLM confidence to that of ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; large language model, framework, experiment, LLM, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 09 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>large language model</category>
      <category>framework</category>
      <category>experiment</category>
    </item>
    <item>
      <title>Learning to Reason: Temporal Saliency Distillation for Interpretable Knowledge Transfer</title>
      <link>https://arxiv.org/abs/2601.04263</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.04263</guid>
      <description>arXiv:2601.04263v1 Announce Type: new 
Abstract: Knowledge distillation has proven effective for model compression by transferring knowledge from a larger network called the teacher to a smaller network called the student. Current knowledge distillation in time series is predominantly based on logit...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; reasoning, compression, vision, analysis, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 80%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 09 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>reasoning</category>
      <category>compression</category>
      <category>vision</category>
    </item>
    <item>
      <title>LEGATO: Good Identity Unlearning Is Continuous</title>
      <link>https://arxiv.org/abs/2601.04282</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.04282</guid>
      <description>arXiv:2601.04282v1 Announce Type: new 
Abstract: Machine unlearning has become a crucial role in enabling generative models trained on large datasets to remove sensitive, private, or copyright-protected data. However, existing machine unlearning methods face three challenges in learning to forget id...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, experiment, fine-tuning, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 80%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 09 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>model</category>
      <category>experiment</category>
      <category>fine-tuning</category>
    </item>
    <item>
      <title>Exploration Through Introspection: A Self-Aware Reward Model</title>
      <link>https://arxiv.org/abs/2601.03389</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.03389</guid>
      <description>arXiv:2601.03389v1 Announce Type: new 
Abstract: Understanding how artificial agents model internal mental states is central to advancing Theory of Mind in AI. Evidence points to a unified system for self- and other-awareness. We explore this self-awareness by having reinforcement learning agents in...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, framework, study, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 40%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 09 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>model</category>
      <category>framework</category>
      <category>study</category>
    </item>
    <item>
      <title>Toward Maturity-Based Certification of Embodied AI: Quantifying Trustworthiness Through Measurement Mechanisms</title>
      <link>https://arxiv.org/abs/2601.03470</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.03470</guid>
      <description>arXiv:2601.03470v2 Announce Type: new 
Abstract: We propose a maturity-based framework for certifying embodied AI systems through explicit measurement mechanisms. We argue that certifiable embodied AI requires structured assessment frameworks, quantitative scoring mechanisms, and methods for navigat...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; framework, study, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 40%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 09 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>framework</category>
      <category>study</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>Making Tunable Parameters State-Dependent in Weather and Climate Models with Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2601.04268</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.04268</guid>
      <description>arXiv:2601.04268v1 Announce Type: new 
Abstract: Weather and climate models rely on parametrisations to represent unresolved sub-grid processes. Traditional schemes rely on fixed coefficients that are weakly constrained and tuned offline, contributing to persistent biases that limit their ability to...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, framework, study, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 40%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 09 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>model</category>
      <category>framework</category>
      <category>study</category>
    </item>
  </channel>
</rss>