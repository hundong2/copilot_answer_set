{
  "generated_at": "2025-10-28T20:06:15.583666",
  "total_items": 47,
  "items": [
    {
      "title": "Policy Optimization Prefers The Path of Least Resistance",
      "url": "https://arxiv.org/abs/2510.21853",
      "description": "arXiv:2510.21853v1 Announce Type: new \nAbstract: Policy optimization (PO) algorithms are used to refine Large Language Models for complex, multi-step reasoning. Current state-of-the-art pipelines enforce a strict think-then-answer format to elicit chain-of-thought (CoT); however, the behavior of PO ...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "chain_of_thought",
      "keywords": [
        "chain-of-thought",
        "CoT",
        "model",
        "experiment",
        "large language model",
        "arxiv",
        "alignment",
        "reasoning"
      ],
      "score": 1.0
    },
    {
      "title": "Language Ranker: A Lightweight Ranking framework for LLM Decoding",
      "url": "https://arxiv.org/abs/2510.21883",
      "description": "arXiv:2510.21883v1 Announce Type: new \nAbstract: Conventional research on large language models (LLMs) has primarily focused on refining output distributions, while paying less attention to the decoding process that transforms these distributions into final responses. Recent advances, such as scalin...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "model",
        "attention",
        "experiment",
        "paper",
        "research",
        "large language model",
        "arxiv",
        "framework"
      ],
      "score": 1.0
    },
    {
      "title": "Framework for Machine Evaluation of Reasoning Completeness in Large Language Models For Classification Tasks",
      "url": "https://arxiv.org/abs/2510.21884",
      "description": "arXiv:2510.21884v1 Announce Type: new \nAbstract: The growing adoption of machine learning (ML) in sensitive domains has heightened the demand for transparent and interpretable artificial intelligence. Large Language Models (LLMs) are increasingly capable of producing natural language explanations, y...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "model",
        "paper",
        "large language model",
        "RAG",
        "arxiv",
        "framework",
        "alignment",
        "reasoning"
      ],
      "score": 1.0
    },
    {
      "title": "Preventing Catastrophic Forgetting: Behavior-Aware Sampling for Safer Language Model Fine-Tuning",
      "url": "https://arxiv.org/abs/2510.21885",
      "description": "arXiv:2510.21885v1 Announce Type: new \nAbstract: Large language models often lose previously aligned safety behaviors when fine-tuned on benign data, a phenomenon known as catastrophic forgetting. Prior work shows that adding random safety examples can mitigate this effect, but it remains unclear wh...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "example",
        "model",
        "fine-tuning",
        "instruction",
        "large language model",
        "arxiv",
        "framework"
      ],
      "score": 1.0
    },
    {
      "title": "Embedding Trust: Semantic Isotropy Predicts Nonfactuality in Long-Form Text Generation",
      "url": "https://arxiv.org/abs/2510.21891",
      "description": "arXiv:2510.21891v1 Announce Type: new \nAbstract: To deploy large language models (LLMs) in high-stakes application domains that require substantively accurate responses to open-ended prompts, we need reliable, computationally inexpensive methods that assess the trustworthiness of long-form responses...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "LLM",
        "prompt",
        "model",
        "fine-tuning",
        "embedding",
        "large language model",
        "arxiv"
      ],
      "score": 1.0
    },
    {
      "title": "Understanding Network Behaviors through Natural Language Question-Answering",
      "url": "https://arxiv.org/abs/2510.21894",
      "description": "arXiv:2510.21894v1 Announce Type: new \nAbstract: Modern large-scale networks introduce significant complexity in understanding network behaviors, increasing the risk of misconfiguration. Prior work proposed to understand network behaviors by mining network configurations, typically relying on domain...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "model",
        "experiment",
        "research",
        "large language model",
        "RAG",
        "arxiv",
        "framework",
        "reasoning",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "Deep Literature Survey Automation with an Iterative Workflow",
      "url": "https://arxiv.org/abs/2510.21900",
      "description": "arXiv:2510.21900v1 Announce Type: new \nAbstract: Automatic literature survey generation has attracted increasing attention, yet most existing systems follow a one-shot paradigm, where a large set of papers is retrieved at once and a static outline is generated before drafting. This design often lead...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "attention",
        "experiment",
        "retrieval",
        "paper",
        "research",
        "RAG",
        "arxiv",
        "multimodal",
        "framework",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "Model-Aware Tokenizer Transfer",
      "url": "https://arxiv.org/abs/2510.21954",
      "description": "arXiv:2510.21954v1 Announce Type: new \nAbstract: Large Language Models (LLMs) are trained to support an increasing number of languages, yet their predefined tokenizers remain a bottleneck for adapting models to lower-resource or distinct-script languages. Existing tokenizer transfer methods typicall...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "LLM",
        "model",
        "attention",
        "experiment",
        "embedding",
        "large language model",
        "RAG",
        "arxiv"
      ],
      "score": 1.0
    },
    {
      "title": "A Multi-Component AI Framework for Computational Psychology: From Robust Predictive Modeling to Deployed Generative Dialogue",
      "url": "https://arxiv.org/abs/2510.21720",
      "description": "arXiv:2510.21720v1 Announce Type: new \nAbstract: The confluence of Artificial Intelligence and Computational Psychology presents an opportunity to model, understand, and interact with complex human psychological states through computational means. This paper presents a comprehensive, multi-faceted f...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "model",
        "transformer",
        "paper",
        "research",
        "large language model",
        "arxiv",
        "framework",
        "analysis"
      ],
      "score": 1.0
    },
    {
      "title": "PREFINE: Personalized Story Generation via Simulated User Critics and User-Specific Rubric Generation",
      "url": "https://arxiv.org/abs/2510.21721",
      "description": "arXiv:2510.21721v1 Announce Type: new \nAbstract: While recent advances in Large Language Models (LLMs) have improved the quality of creative text generation, significant challenges remain in producing personalized stories that reflect individual user preferences. Conventional approaches rely on expl...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "model",
        "fine-tuning",
        "large language model",
        "arxiv",
        "framework",
        "analysis"
      ],
      "score": 1.0
    },
    {
      "title": "SIGN: Schema-Induced Games for Naming",
      "url": "https://arxiv.org/abs/2510.21855",
      "description": "arXiv:2510.21855v1 Announce Type: new \nAbstract: Real-world AI systems are tackling increasingly complex problems, often through interactions among large language model (LLM) agents. When these agents develop inconsistent conventions, coordination can break down. Applications such as collaborative c...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "large language model",
        "LLM",
        "arxiv",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Capability Ceilings in Autoregressive Language Models: Empirical Evidence from Knowledge-Intensive Tasks",
      "url": "https://arxiv.org/abs/2510.21866",
      "description": "arXiv:2510.21866v1 Announce Type: new \nAbstract: We document empirical capability ceilings in decoder-only autoregressive language models across knowledge-intensive tasks. Systematic evaluation of OPT and Pythia model families (70M-30B parameters, spanning 240 times scaling) reveals that knowledge r...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "model",
        "attention",
        "experiment",
        "retrieval",
        "arxiv"
      ],
      "score": 1.0
    },
    {
      "title": "GeoThought: A Dataset for Enhancing Mathematical Geometry Reasoning in Vision-Language Models",
      "url": "https://arxiv.org/abs/2510.21881",
      "description": "arXiv:2510.21881v1 Announce Type: new \nAbstract: Large language models (LLMs) have demonstrated strong reasoning capabilities in text-based mathematical problem solving; however, when adapted to visual reasoning tasks, particularly geometric problem solving, their performance substantially declines ...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "chain_of_thought",
      "keywords": [
        "LLM",
        "CoT",
        "chain-of-thought",
        "model",
        "image",
        "large language model",
        "augmented",
        "arxiv",
        "vision",
        "multimodal",
        "reasoning",
        "step-by-step"
      ],
      "score": 1.0
    },
    {
      "title": "Performance Trade-offs of Optimizing Small Language Models for E-Commerce",
      "url": "https://arxiv.org/abs/2510.21970",
      "description": "arXiv:2510.21970v1 Announce Type: new \nAbstract: Large Language Models (LLMs) offer state-of-the-art performance in natural language understanding and generation tasks. However, the deployment of leading commercial models for specialized tasks, such as e-commerce, is often hindered by high computati...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "model",
        "paper",
        "large language model",
        "arxiv",
        "analysis",
        "GPT"
      ],
      "score": 1.0
    },
    {
      "title": "Distribution Shift Alignment Helps LLMs Simulate Survey Response Distributions",
      "url": "https://arxiv.org/abs/2510.21977",
      "description": "arXiv:2510.21977v1 Announce Type: new \nAbstract: Large language models (LLMs) offer a promising way to simulate human survey responses, potentially reducing the cost of large-scale data collection. However, existing zero-shot methods suffer from prompt sensitivity and low accuracy, while conventiona...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "LLM",
        "prompt",
        "model",
        "fine-tuning",
        "large language model",
        "arxiv",
        "zero-shot",
        "alignment"
      ],
      "score": 1.0
    },
    {
      "title": "Foundation of Intelligence: Review of Math Word Problems from Human Cognition Perspective",
      "url": "https://arxiv.org/abs/2510.21999",
      "description": "arXiv:2510.21999v1 Announce Type: new \nAbstract: Math word problem (MWP) serves as a fundamental research topic in artificial intelligence (AI) dating back to 1960s. This research aims to advance the reasoning abilities of AI by mirroring the human-like cognitive intelligence. The mainstream technol...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "API",
        "model",
        "paper",
        "release",
        "research",
        "large language model",
        "memory",
        "arxiv",
        "reasoning"
      ],
      "score": 1.0
    },
    {
      "title": "Incentivizing Consistent, Effective and Scalable Reasoning Capability in Audio LLMs via Reasoning Process Rewards",
      "url": "https://arxiv.org/abs/2510.20867",
      "description": "arXiv:2510.20867v1 Announce Type: new \nAbstract: The role of reasoning in Audio Large Language Models remains widely underexplored, as introducing a reasoning process often degrades rather than improves performance during inference, a phenomenon we term test-time inverse scaling, where longer reason...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "multimodal_context",
      "keywords": [
        "LLM",
        "model",
        "large language model",
        "arxiv",
        "multimodal",
        "audio",
        "framework",
        "reasoning",
        "GPT"
      ],
      "score": 1.0
    },
    {
      "title": "MOBO-OSD: Batch Multi-Objective Bayesian Optimization via Orthogonal Search Directions",
      "url": "https://arxiv.org/abs/2510.20872",
      "description": "arXiv:2510.20872v1 Announce Type: new \nAbstract: Bayesian Optimization (BO) is a powerful tool for optimizing expensive black-box objective functions. While extensive research has been conducted on the single-objective optimization problem, the multi-objective optimization problem remains challengin...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "tool",
        "experiment",
        "paper",
        "research",
        "RAG",
        "arxiv",
        "analysis"
      ],
      "score": 1.0
    },
    {
      "title": "HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data Placement",
      "url": "https://arxiv.org/abs/2510.20878",
      "description": "arXiv:2510.20878v1 Announce Type: new \nAbstract: Retrieval-Augmented Generation (RAG) improves model output accuracy by leveraging external knowledge bases, serving as an effective solution to address hallucination issues and knowledge-update delays in Large Language Models (LLMs). However, the intr...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "LLM",
        "model",
        "experiment",
        "retrieval",
        "paper",
        "research",
        "large language model",
        "RAG",
        "augmented",
        "memory",
        "arxiv",
        "context",
        "knowledge base"
      ],
      "score": 1.0
    },
    {
      "title": "Meta-Learning for Cross-Task Generalization in Protein Mutation Property Prediction",
      "url": "https://arxiv.org/abs/2510.20943",
      "description": "arXiv:2510.20943v1 Announce Type: new \nAbstract: Protein mutations can have profound effects on biological function, making accurate prediction of property changes critical for drug discovery, protein engineering, and precision medicine. Current approaches rely on fine-tuning protein-specific transf...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "API",
        "model",
        "fine-tuning",
        "transformer",
        "experiment",
        "analysis",
        "arxiv",
        "framework",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "LLM-Integrated Bayesian State Space Models for Multimodal Time-Series Forecasting",
      "url": "https://arxiv.org/abs/2510.20952",
      "description": "arXiv:2510.20952v1 Announce Type: new \nAbstract: Forecasting in the real world requires integrating structured time-series data with unstructured textual information, but existing methods are architecturally limited by fixed input/output horizons and are unable to model or quantify uncertainty. We a...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "model",
        "experiment",
        "large language model",
        "arxiv",
        "multimodal",
        "framework",
        "reasoning"
      ],
      "score": 1.0
    },
    {
      "title": "Context-Engineering - \"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" â€” Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "url": "https://github.com/davidkimai/Context-Engineering",
      "description": "\"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" â€” Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "published_date": "2025-06-29T00:16:36+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "context window",
        "prompt engineering",
        "prompt",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "ThinkSound - [NeurIPS 2025] PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "url": "https://github.com/FunAudioLLM/ThinkSound",
      "description": "[NeurIPS 2025] PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "published_date": "2025-06-27T02:27:00+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "chain-of-thought",
        "CoT",
        "audio",
        "framework",
        "reasoning"
      ],
      "score": 1.0
    },
    {
      "title": "mcp-context-forge - A Model Context Protocol (MCP) Gateway & Registry. Serves as a central management point for tools, resources, and prompts that can be accessed by MCP-compatible LLM applications. Converts REST API endpoints to MCP, composes virtual MCP servers with added security and observability, and converts between protocols (stdio, SSE, Streamable HTTP).",
      "url": "https://github.com/IBM/mcp-context-forge",
      "description": "A Model Context Protocol (MCP) Gateway & Registry. Serves as a central management point for tools, resources, and prompts that can be accessed by MCP-compatible LLM applications. Converts REST API endpoints to MCP, composes virtual MCP servers with added security and observability, and converts between protocols (stdio, SSE, Streamable HTTP).",
      "published_date": "2025-05-08T08:16:59+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "LLM",
        "API",
        "prompt",
        "model",
        "tool",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "Cline-Recursive-Chain-of-Thought-System-CRCT- - A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "url": "https://github.com/RPG-fan/Cline-Recursive-Chain-of-Thought-System-CRCT-",
      "description": "A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "published_date": "2025-02-18T15:45:30+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "chain-of-thought",
        "framework",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "LightRAG - [EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "url": "https://github.com/HKUDS/LightRAG",
      "description": "[EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "published_date": "2024-10-02T11:57:54+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "retrieval",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "KAG - KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "url": "https://github.com/OpenSPG/KAG",
      "description": "KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "published_date": "2024-09-21T13:56:44+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "LLM",
        "model",
        "retrieval",
        "RAG",
        "vector",
        "framework",
        "reasoning",
        "knowledge base"
      ],
      "score": 1.0
    },
    {
      "title": "Kiln - The easiest tool for fine-tuning LLM models, synthetic data generation, and collaborating on datasets.",
      "url": "https://github.com/Kiln-AI/Kiln",
      "description": "The easiest tool for fine-tuning LLM models, synthetic data generation, and collaborating on datasets.",
      "published_date": "2024-07-23T23:10:13+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "LLM",
        "model",
        "fine-tuning",
        "tool"
      ],
      "score": 1.0
    },
    {
      "title": "graphrag - A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "url": "https://github.com/microsoft/graphrag",
      "description": "A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "published_date": "2024-03-27T17:57:52+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "retrieval",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "R2R - SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "url": "https://github.com/SciPhi-AI/R2R",
      "description": "SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "published_date": "2024-02-12T03:24:27+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "API",
        "retrieval",
        "product",
        "RAG",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "openlit - Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. ðŸš€ðŸ’» Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "url": "https://github.com/openlit/openlit",
      "description": "Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. ðŸš€ðŸ’» Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "published_date": "2024-01-23T17:40:59+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "LLM",
        "prompt",
        "platform",
        "vector",
        "framework"
      ],
      "score": 1.0
    },
    {
      "title": "AutoRAG - AutoRAG: An Open-Source Framework for Retrieval-Augmented Generation (RAG) Evaluation & Optimization with AutoML-Style Automation",
      "url": "https://github.com/Marker-Inc-Korea/AutoRAG",
      "description": "AutoRAG: An Open-Source Framework for Retrieval-Augmented Generation (RAG) Evaluation & Optimization with AutoML-Style Automation",
      "published_date": "2024-01-10T12:25:00+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "retrieval",
        "framework",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "Computational Hardness of Reinforcement Learning with Partial $q^{\\pi}$-Realizability",
      "url": "https://arxiv.org/abs/2510.21888",
      "description": "arXiv:2510.21888v1 Announce Type: new \nAbstract: This paper investigates the computational complexity of reinforcement learning in a novel linear function approximation regime, termed partial $q^{\\pi}$-realizability. In this framework, the objective is to learn an $\\epsilon$-optimal policy with resp...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "model",
        "paper",
        "arxiv",
        "vector",
        "framework"
      ],
      "score": 0.8
    },
    {
      "title": "Global Dynamics of Heavy-Tailed SGDs in Nonconvex Loss Landscape: Characterization and Control",
      "url": "https://arxiv.org/abs/2510.20905",
      "description": "arXiv:2510.20905v1 Announce Type: new \nAbstract: Stochastic gradient descent (SGD) and its variants enable modern artificial intelligence. However, theoretical understanding lags far behind their empirical success. It is widely believed that SGD has a curious ability to avoid sharp local minima in t...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "arxiv",
        "analysis",
        "paper"
      ],
      "score": 0.8
    },
    {
      "title": "fastapi_mcp - Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "url": "https://github.com/tadata-org/fastapi_mcp",
      "description": "Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "published_date": "2025-03-08T11:15:43+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "model",
        "API",
        "context",
        "tool"
      ],
      "score": 0.8
    },
    {
      "title": "cosmos-reason1 - Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "url": "https://github.com/nvidia-cosmos/cosmos-reason1",
      "description": "Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "published_date": "2025-03-02T15:23:55+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "model",
        "chain-of-thought",
        "reasoning"
      ],
      "score": 0.8
    },
    {
      "title": "A Multi-lingual Dataset of Classified Paragraphs from Open Access Scientific Publications",
      "url": "https://arxiv.org/abs/2510.21762",
      "description": "arXiv:2510.21762v1 Announce Type: new \nAbstract: We present a dataset of 833k paragraphs extracted from CC-BY licensed scientific publications, classified into four categories: acknowledgments, data mentions, software/code mentions, and clinical trial mentions. The paragraphs are primarily in Englis...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "in_context_learning",
      "keywords": [
        "RAG",
        "arxiv",
        "model",
        "ICL"
      ],
      "score": 0.6
    },
    {
      "title": "Crisis-Resilient Portfolio Management via Graph-based Spatio-Temporal Learning",
      "url": "https://arxiv.org/abs/2510.20868",
      "description": "arXiv:2510.20868v1 Announce Type: new \nAbstract: Financial time series forecasting faces a fundamental challenge: predicting optimal asset allocations requires understanding regime-dependent correlation structures that transform during crisis periods. Existing graph-based spatio-temporal learning ap...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "context_management",
      "keywords": [
        "arxiv",
        "attention",
        "framework"
      ],
      "score": 0.6
    },
    {
      "title": "CC-GRMAS: A Multi-Agent Graph Neural System for Spatiotemporal Landslide Risk Assessment in High Mountain Asia",
      "url": "https://arxiv.org/abs/2510.20875",
      "description": "arXiv:2510.20875v1 Announce Type: new \nAbstract: Landslides are a growing climate induced hazard with severe environmental and human consequences, particularly in high mountain Asia. Despite increasing access to satellite and temporal datasets, timely detection and disaster response remain underdeve...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "arxiv",
        "RAG",
        "framework"
      ],
      "score": 0.6
    },
    {
      "title": "Learning from Interval Targets",
      "url": "https://arxiv.org/abs/2510.20925",
      "description": "arXiv:2510.20925v1 Announce Type: new \nAbstract: We study the problem of regression with interval targets, where only upper and lower bounds on target values are available in the form of intervals. This problem arises when the exact target label is expensive or impossible to obtain, due to inherent ...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "arxiv",
        "study"
      ],
      "score": 0.6
    },
    {
      "title": "excel-mcp-server - A Model Context Protocol server for Excel file manipulation",
      "url": "https://github.com/haris-musa/excel-mcp-server",
      "description": "A Model Context Protocol server for Excel file manipulation",
      "published_date": "2025-02-12T06:39:48+00:00",
      "source": "GitHub",
      "category": "industry_news",
      "keywords": [
        "model",
        "context"
      ],
      "score": 0.6
    },
    {
      "title": "mcp-agent - Build effective agents using Model Context Protocol and simple workflow patterns",
      "url": "https://github.com/lastmile-ai/mcp-agent",
      "description": "Build effective agents using Model Context Protocol and simple workflow patterns",
      "published_date": "2024-12-18T01:55:10+00:00",
      "source": "GitHub",
      "category": "industry_news",
      "keywords": [
        "model",
        "context"
      ],
      "score": 0.6
    },
    {
      "title": "AlphaCodium - Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "url": "https://github.com/Codium-ai/AlphaCodium",
      "description": "Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "published_date": "2024-01-14T15:17:18+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "prompt engineering",
        "prompt",
        "paper"
      ],
      "score": 0.6
    },
    {
      "title": "Explaining and Mitigating Crosslingual Tokenizer Inequities",
      "url": "https://arxiv.org/abs/2510.21909",
      "description": "arXiv:2510.21909v1 Announce Type: new \nAbstract: The number of tokens it takes to encode parallel text in different languages is known to vary. These disparities are called token premiums. Having high token premiums leads to less throughput during training and increases costs at inference. In this p...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "compression",
        "paper"
      ],
      "score": 0.4
    },
    {
      "title": "Exploration through Generation: Applying GFlowNets to Structured Search",
      "url": "https://arxiv.org/abs/2510.21886",
      "description": "arXiv:2510.21886v1 Announce Type: new \nAbstract: This work applies Generative Flow Networks (GFlowNets) to three graph optimization problems: the Traveling Salesperson Problem, Minimum Spanning Tree, and Shortest Path. GFlowNets are generative models that learn to sample solutions proportionally to ...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "experiment",
        "framework",
        "model"
      ],
      "score": 0.4
    },
    {
      "title": "Multimodal Negative Learning",
      "url": "https://arxiv.org/abs/2510.20877",
      "description": "arXiv:2510.20877v1 Announce Type: new \nAbstract: Multimodal learning systems often encounter challenges related to modality imbalance, where a dominant modality may overshadow others, thereby hindering the learning of weak modalities. Conventional approaches often force weak modalities to align with...",
      "published_date": "2025-10-28T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "multimodal",
        "experiment",
        "framework"
      ],
      "score": 0.4
    },
    {
      "title": "optillm - Optimizing inference proxy for LLMs",
      "url": "https://github.com/codelion/optillm",
      "description": "Optimizing inference proxy for LLMs",
      "published_date": "2024-08-22T19:46:07+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "LLM"
      ],
      "score": 0.4
    }
  ]
}