{
  "generated_at": "2025-10-14T20:05:37.157493",
  "total_items": 46,
  "items": [
    {
      "title": "Table Question Answering in the Era of Large Language Models: A Comprehensive Survey of Tasks, Methods, and Evaluation",
      "url": "https://arxiv.org/abs/2510.09671",
      "description": "arXiv:2510.09671v1 Announce Type: new \nAbstract: Table Question Answering (TQA) aims to answer natural language questions about tabular data, often accompanied by additional contexts such as text passages. The task spans diverse settings, varying in table representation, question/answer complexity, ...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "API",
        "context",
        "arxiv",
        "research",
        "large language model",
        "LLM",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs Human Fallacy Detection",
      "url": "https://arxiv.org/abs/2510.09695",
      "description": "arXiv:2510.09695v1 Announce Type: new \nAbstract: Logical fallacies are common in public communication and can mislead audiences; fallacious arguments may still appear convincing despite lacking soundness, because convincingness is inherently subjective. We present the first computational study of ho...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "context",
        "arxiv",
        "large language model",
        "RAG",
        "LLM",
        "model",
        "study"
      ],
      "score": 1.0
    },
    {
      "title": "The Idola Tribus of AI: Large Language Models tend to perceive order where none exists",
      "url": "https://arxiv.org/abs/2510.09709",
      "description": "arXiv:2510.09709v1 Announce Type: new \nAbstract: We present a tendency of large language models (LLMs) to generate absurd patterns despite their clear inappropriateness in a simple task of identifying regularities in number series. Several approaches have been proposed to apply LLMs to complex real-...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "chain_of_thought",
      "keywords": [
        "experiment",
        "retrieval",
        "arxiv",
        "framework",
        "large language model",
        "chain-of-thought",
        "model",
        "LLM",
        "reasoning",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework for Trustworthy RAG",
      "url": "https://arxiv.org/abs/2510.09710",
      "description": "arXiv:2510.09710v1 Announce Type: new \nAbstract: Retrieval-augmented generation (RAG) systems enhance large language models (LLMs) with external knowledge but are vulnerable to corpus poisoning and contamination attacks, which can compromise output integrity. Existing defenses often apply aggressive...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "experiment",
        "retrieval",
        "arxiv",
        "framework",
        "large language model",
        "RAG",
        "model",
        "LLM",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "ReaLM: Residual Quantization Bridging Knowledge Graph Embeddings and Large Language Models",
      "url": "https://arxiv.org/abs/2510.09711",
      "description": "arXiv:2510.09711v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have recently emerged as a powerful paradigm for Knowledge Graph Completion (KGC), offering strong reasoning and generalization capabilities beyond traditional embedding-based approaches. However, existing LLM-based method...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "context",
        "vector",
        "arxiv",
        "framework",
        "large language model",
        "LLM",
        "embedding",
        "reasoning",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language",
      "url": "https://arxiv.org/abs/2510.09714",
      "description": "arXiv:2510.09714v1 Announce Type: new \nAbstract: Detecting harmful AI actions is important as AI agents gain adoption. Chain-of-thought (CoT) monitoring is one method widely used to detect adversarial attacks and AI misalignment. However, attackers and misaligned models might evade CoT monitoring th...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "chain_of_thought",
      "keywords": [
        "arxiv",
        "CoT",
        "alignment",
        "chain-of-thought",
        "model",
        "prompt",
        "reasoning",
        "fine-tuning"
      ],
      "score": 1.0
    },
    {
      "title": "Preference-Aware Memory Update for Long-Term LLM Agents",
      "url": "https://arxiv.org/abs/2510.09720",
      "description": "arXiv:2510.09720v1 Announce Type: new \nAbstract: One of the key factors influencing the reasoning capabilities of LLM-based agents is their ability to leverage long-term memory. Integrating long-term memory mechanisms allows agents to make informed decisions grounded in historical interactions. Whil...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "experiment",
        "memory",
        "context",
        "retrieval",
        "vector",
        "arxiv",
        "RAG",
        "LLM",
        "reasoning"
      ],
      "score": 1.0
    },
    {
      "title": "Layout-Aware Parsing Meets Efficient LLMs: A Unified, Scalable Framework for Resume Information Extraction and Evaluation",
      "url": "https://arxiv.org/abs/2510.09722",
      "description": "arXiv:2510.09722v1 Announce Type: new \nAbstract: Automated resume information extraction is critical for scaling talent acquisition, yet its real-world deployment faces three major challenges: the extreme heterogeneity of resume layouts and content, the high cost and latency of large language models...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "experiment",
        "instruction",
        "tool",
        "prompting",
        "framework",
        "arxiv",
        "platform",
        "large language model",
        "LLM",
        "prompt",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "VisRAG 2.0: Evidence-Guided Multi-Image Reasoning in Visual Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2510.09733",
      "description": "arXiv:2510.09733v1 Announce Type: new \nAbstract: Visual retrieval-augmented generation (VRAG) augments vision-language models (VLMs) with external visual knowledge to ground reasoning and reduce hallucinations. Yet current VRAG systems often fail to reliably perceive and integrate evidence across mu...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "paper",
        "analysis",
        "retrieval",
        "arxiv",
        "framework",
        "image",
        "RAG",
        "vision",
        "model",
        "reasoning",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "Judge's Verdict: A Comprehensive Analysis of LLM Judge Capability Through Human Agreement",
      "url": "https://arxiv.org/abs/2510.09738",
      "description": "arXiv:2510.09738v1 Announce Type: new \nAbstract: This research introduces the Judge's Verdict Benchmark, a novel two-step methodology to evaluate Large Language Models (LLMs) as judges for response accuracy evaluation tasks. We assess how well 54 LLMs can replicate human judgment when scoring respon...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "analysis",
        "retrieval",
        "arxiv",
        "research",
        "large language model",
        "GPT",
        "RAG",
        "alignment",
        "model",
        "LLM",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "The Geometry of Reasoning: Flowing Logics in Representation Space",
      "url": "https://arxiv.org/abs/2510.09782",
      "description": "arXiv:2510.09782v1 Announce Type: new \nAbstract: We study how large language models (LLMs) ``think'' through their representation space. We propose a novel geometric framework that models an LLM's reasoning as flows -- embedding trajectories evolving where logic goes. We disentangle logical structur...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "analysis",
        "tool",
        "arxiv",
        "framework",
        "large language model",
        "LLM",
        "embedding",
        "reasoning",
        "model",
        "study"
      ],
      "score": 1.0
    },
    {
      "title": "How can we assess human-agent interactions? Case studies in software agent design",
      "url": "https://arxiv.org/abs/2510.09801",
      "description": "arXiv:2510.09801v1 Announce Type: new \nAbstract: LLM-powered agents are both a promising new technology and a source of complexity, where choices about models, tools, and prompting can affect their usefulness. While numerous benchmarks measure agent accuracy across domains, they mostly assume full a...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "paper",
        "memory",
        "tool",
        "prompting",
        "framework",
        "arxiv",
        "platform",
        "GPT",
        "LLM",
        "prompt",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Beyond AlphaEarth: Toward Human-Centered Spatial Representation via POI-Guided Contrastive Learning",
      "url": "https://arxiv.org/abs/2510.09894",
      "description": "arXiv:2510.09894v1 Announce Type: new \nAbstract: General-purpose spatial representations are essential for building transferable geospatial foundation models (GFMs). Among them, the AlphaEarth Foundation (AE) represents a major step toward a global, unified representation of the Earth's surface, lea...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "analysis",
        "context",
        "arxiv",
        "framework",
        "RAG",
        "alignment",
        "multimodal",
        "model",
        "embedding"
      ],
      "score": 1.0
    },
    {
      "title": "Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics",
      "url": "https://arxiv.org/abs/2510.09901",
      "description": "arXiv:2510.09901v1 Announce Type: new \nAbstract: Computing has long served as a cornerstone of scientific discovery. Recently, a paradigm shift has emerged with the rise of large language models (LLMs), introducing autonomous systems, referred to as agents, that accelerate discovery across varying l...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "paper",
        "analysis",
        "arxiv",
        "framework",
        "research",
        "large language model",
        "vision",
        "LLM",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "The Personalization Trap: How User Memory Alters Emotional Reasoning in LLMs",
      "url": "https://arxiv.org/abs/2510.09905",
      "description": "arXiv:2510.09905v1 Announce Type: new \nAbstract: When an AI assistant remembers that Sarah is a single mother working two jobs, does it interpret her stress differently than if she were a wealthy executive? As personalized AI systems increasingly incorporate long-term user memory, understanding how ...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "chain_of_thought",
      "keywords": [
        "memory",
        "arxiv",
        "large language model",
        "LLM",
        "reasoning",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Follow My Lead: Logical Fallacy Classification with Knowledge-Augmented LLMs",
      "url": "https://arxiv.org/abs/2510.09970",
      "description": "arXiv:2510.09970v1 Announce Type: new \nAbstract: Large Language Models (LLMs) suffer from critical reasoning gaps, including a tendency to hallucinate and poor accuracy in classifying logical fallacies. This limitation stems from their default System 1 processing, which is fast and intuitive, wherea...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "instruction",
        "arxiv",
        "large language model",
        "model",
        "LLM",
        "reasoning",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "Deliberative Dynamics and Value Alignment in LLM Debates",
      "url": "https://arxiv.org/abs/2510.10002",
      "description": "arXiv:2510.10002v1 Announce Type: new \nAbstract: As large language models (LLMs) are increasingly deployed in sensitive everyday contexts - offering personal advice, mental health support, and moral guidance - understanding their elicited values in navigating complex moral reasoning is essential. Mo...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "context",
        "prompting",
        "arxiv",
        "large language model",
        "GPT",
        "alignment",
        "vision",
        "LLM",
        "prompt",
        "reasoning",
        "model",
        "study"
      ],
      "score": 1.0
    },
    {
      "title": "RIPRAG: Hack a Black-box Retrieval-Augmented Generation Question-Answering System with Reinforcement Learning",
      "url": "https://arxiv.org/abs/2510.10008",
      "description": "arXiv:2510.10008v1 Announce Type: new \nAbstract: Retrieval-Augmented Generation (RAG) systems based on Large Language Models (LLMs) have become a core technology for tasks such as question-answering (QA) and content generation. However, by injecting poisoned documents into the database of RAG system...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "paper",
        "retrieval",
        "arxiv",
        "framework",
        "research",
        "large language model",
        "RAG",
        "model",
        "LLM",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "Energy-Driven Steering: Reducing False Refusals in Large Language Models",
      "url": "https://arxiv.org/abs/2510.08646",
      "description": "arXiv:2510.08646v1 Announce Type: new \nAbstract: Safety alignment of large language models (LLMs) faces a key challenge: current alignment techniques often only focus on improving safety against harmful prompts, causing LLMs to become over-cautious and refuse to respond to benign prompts. Therefore,...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "paper",
        "example",
        "arxiv",
        "framework",
        "large language model",
        "alignment",
        "LLM",
        "prompt",
        "model",
        "fine-tuning"
      ],
      "score": 1.0
    },
    {
      "title": "Inverse-Free Wilson Loops for Transformers: A Practical Diagnostic for Invariance and Order Sensitivity",
      "url": "https://arxiv.org/abs/2510.08648",
      "description": "arXiv:2510.08648v1 Announce Type: new \nAbstract: Large language models can change answers under harmless edits that matter in practice: RAG outputs flip when passages are reordered, fine-tuning erodes invariances learned at pretraining, debate or chain-of-thought prompts take path-dependent routes, ...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "context",
        "arxiv",
        "large language model",
        "RAG",
        "transformer",
        "chain-of-thought",
        "model",
        "prompt",
        "fine-tuning"
      ],
      "score": 1.0
    },
    {
      "title": "Provably Robust Adaptation for Language-Empowered Foundation Models",
      "url": "https://arxiv.org/abs/2510.08659",
      "description": "arXiv:2510.08659v1 Announce Type: new \nAbstract: Language-empowered foundation models (LeFMs), such as CLIP and GraphCLIP, have transformed multimodal learning by aligning visual (or graph) features with textual representations, enabling powerful downstream capabilities like few-shot learning. Howev...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "few-shot",
        "arxiv",
        "multimodal",
        "model",
        "embedding",
        "few-shot learning",
        "study"
      ],
      "score": 1.0
    },
    {
      "title": "FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching",
      "url": "https://arxiv.org/abs/2510.08669",
      "description": "arXiv:2510.08669v1 Announce Type: new \nAbstract: The application of diffusion transformers is suffering from their significant inference costs. Recently, feature caching has been proposed to solve this problem by reusing features from previous timesteps, thereby skipping computation in future timest...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "paper",
        "analysis",
        "memory",
        "arxiv",
        "image",
        "transformer",
        "release",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting",
      "url": "https://arxiv.org/abs/2510.08696",
      "description": "arXiv:2510.08696v1 Announce Type: new \nAbstract: Reinforcement learning with verifiable rewards (RLVR) has become a standard recipe for improving large language models (LLMs) on reasoning tasks, with Group Relative Policy Optimization (GRPO) widely used in practice. Yet GRPO wastes substantial compu...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "chain_of_thought",
      "keywords": [
        "arxiv",
        "large language model",
        "RAG",
        "vision",
        "LLM",
        "reasoning",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Context-Engineering - \"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" â€” Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "url": "https://github.com/davidkimai/Context-Engineering",
      "description": "\"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" â€” Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "published_date": "2025-06-29T00:16:36+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "prompt",
        "prompt engineering",
        "context window",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "ThinkSound - [NeurIPS 2025] PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "url": "https://github.com/FunAudioLLM/ThinkSound",
      "description": "[NeurIPS 2025] PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "published_date": "2025-06-27T02:27:00+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "framework",
        "audio",
        "chain-of-thought",
        "CoT",
        "reasoning"
      ],
      "score": 1.0
    },
    {
      "title": "mcp-context-forge - A Model Context Protocol (MCP) Gateway & Registry. Serves as a central management point for tools, resources, and prompts that can be accessed by MCP-compatible LLM applications. Converts REST API endpoints to MCP, composes virtual MCP servers with added security and observability, and converts between protocols (stdio, SSE, Streamable HTTP).",
      "url": "https://github.com/IBM/mcp-context-forge",
      "description": "A Model Context Protocol (MCP) Gateway & Registry. Serves as a central management point for tools, resources, and prompts that can be accessed by MCP-compatible LLM applications. Converts REST API endpoints to MCP, composes virtual MCP servers with added security and observability, and converts between protocols (stdio, SSE, Streamable HTTP).",
      "published_date": "2025-05-08T08:16:59+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "API",
        "tool",
        "context",
        "LLM",
        "prompt",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Cline-Recursive-Chain-of-Thought-System-CRCT- - A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "url": "https://github.com/RPG-fan/Cline-Recursive-Chain-of-Thought-System-CRCT-",
      "description": "A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "published_date": "2025-02-18T15:45:30+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "context",
        "chain-of-thought",
        "framework"
      ],
      "score": 1.0
    },
    {
      "title": "LightRAG - [EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "url": "https://github.com/HKUDS/LightRAG",
      "description": "[EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "published_date": "2024-10-02T11:57:54+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "retrieval",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "KAG - KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "url": "https://github.com/OpenSPG/KAG",
      "description": "KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "published_date": "2024-09-21T13:56:44+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "knowledge base",
        "retrieval",
        "vector",
        "framework",
        "RAG",
        "LLM",
        "reasoning",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Kiln - The easiest tool for fine-tuning LLM models, synthetic data generation, and collaborating on datasets.",
      "url": "https://github.com/Kiln-AI/Kiln",
      "description": "The easiest tool for fine-tuning LLM models, synthetic data generation, and collaborating on datasets.",
      "published_date": "2024-07-23T23:10:13+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "LLM",
        "tool",
        "model",
        "fine-tuning"
      ],
      "score": 1.0
    },
    {
      "title": "graphrag - A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "url": "https://github.com/microsoft/graphrag",
      "description": "A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "published_date": "2024-03-27T17:57:52+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "retrieval",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "R2R - SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "url": "https://github.com/SciPhi-AI/R2R",
      "description": "SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "published_date": "2024-02-12T03:24:27+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "product",
        "API",
        "retrieval",
        "RAG",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "openlit - Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. ðŸš€ðŸ’» Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "url": "https://github.com/openlit/openlit",
      "description": "Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. ðŸš€ðŸ’» Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "published_date": "2024-01-23T17:40:59+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "vector",
        "framework",
        "platform",
        "LLM",
        "prompt"
      ],
      "score": 1.0
    },
    {
      "title": "AutoRAG - AutoRAG: An Open-Source Framework for Retrieval-Augmented Generation (RAG) Evaluation & Optimization with AutoML-Style Automation",
      "url": "https://github.com/Marker-Inc-Korea/AutoRAG",
      "description": "AutoRAG: An Open-Source Framework for Retrieval-Augmented Generation (RAG) Evaluation & Optimization with AutoML-Style Automation",
      "published_date": "2024-01-10T12:25:00+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "retrieval",
        "augmented",
        "framework"
      ],
      "score": 1.0
    },
    {
      "title": "Failure-Driven Workflow Refinement",
      "url": "https://arxiv.org/abs/2510.10035",
      "description": "arXiv:2510.10035v1 Announce Type: new \nAbstract: Optimizing LLM-based workflows is typically formulated as a global search, where candidate workflows are evaluated based on a scalar metric. This paradigm, however, suffers from a critical flaw: information collapse. By reducing rich, multi-step execu...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "API",
        "example",
        "arxiv",
        "framework",
        "LLM",
        "model"
      ],
      "score": 0.8
    },
    {
      "title": "How Scale Breaks \"Normalized Stress\" and KL Divergence: Rethinking Quality Metrics",
      "url": "https://arxiv.org/abs/2510.08660",
      "description": "arXiv:2510.08660v1 Announce Type: new \nAbstract: Complex, high-dimensional data is ubiquitous across many scientific disciplines, including machine learning, biology, and the social sciences. One of the primary methods of visualizing these datasets is with two-dimensional scatter plots that visually...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "embedding",
        "arxiv",
        "research"
      ],
      "score": 0.8
    },
    {
      "title": "fastapi_mcp - Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "url": "https://github.com/tadata-org/fastapi_mcp",
      "description": "Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "published_date": "2025-03-08T11:15:43+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "model",
        "context",
        "API",
        "tool"
      ],
      "score": 0.8
    },
    {
      "title": "cosmos-reason1 - Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "url": "https://github.com/nvidia-cosmos/cosmos-reason1",
      "description": "Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "published_date": "2025-03-02T15:23:55+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "model",
        "reasoning",
        "chain-of-thought"
      ],
      "score": 0.8
    },
    {
      "title": "CATS-Linear: Classification Auxiliary Linear Model for Time Series Forecasting",
      "url": "https://arxiv.org/abs/2510.08661",
      "description": "arXiv:2510.08661v1 Announce Type: new \nAbstract: Recent research demonstrates that linear models achieve forecasting performance competitive with complex architectures, yet methodologies for enhancing linear models remain underexplored. Motivated by the hypothesis that distinct time series instances...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "arxiv",
        "framework",
        "research",
        "model"
      ],
      "score": 0.6
    },
    {
      "title": "DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops",
      "url": "https://arxiv.org/abs/2510.08662",
      "description": "arXiv:2510.08662v1 Announce Type: new \nAbstract: Genomic Selection (GS) uses whole-genome information to predict crop phenotypes and accelerate breeding. Traditional GS methods, however, struggle with prediction accuracy for complex traits and large datasets. We propose DPCformer, a deep learning mo...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "chain_of_thought",
      "keywords": [
        "tool",
        "arxiv",
        "attention",
        "model",
        "CoT"
      ],
      "score": 0.6
    },
    {
      "title": "excel-mcp-server - A Model Context Protocol server for Excel file manipulation",
      "url": "https://github.com/haris-musa/excel-mcp-server",
      "description": "A Model Context Protocol server for Excel file manipulation",
      "published_date": "2025-02-12T06:39:48+00:00",
      "source": "GitHub",
      "category": "industry_news",
      "keywords": [
        "model",
        "context"
      ],
      "score": 0.6
    },
    {
      "title": "mcp-agent - Build effective agents using Model Context Protocol and simple workflow patterns",
      "url": "https://github.com/lastmile-ai/mcp-agent",
      "description": "Build effective agents using Model Context Protocol and simple workflow patterns",
      "published_date": "2024-12-18T01:55:10+00:00",
      "source": "GitHub",
      "category": "industry_news",
      "keywords": [
        "model",
        "context"
      ],
      "score": 0.6
    },
    {
      "title": "AlphaCodium - Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "url": "https://github.com/Codium-ai/AlphaCodium",
      "description": "Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "published_date": "2024-01-14T15:17:18+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "prompt",
        "paper",
        "prompt engineering"
      ],
      "score": 0.6
    },
    {
      "title": "Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis",
      "url": "https://arxiv.org/abs/2510.08655",
      "description": "arXiv:2510.08655v1 Announce Type: new \nAbstract: Rare genetic disease diagnosis faces critical challenges: insufficient patient data, inaccessible full genome sequencing, and the immense number of possible causative genes. These limitations cause prolonged diagnostic journeys, inappropriate treatmen...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "tool",
        "analysis",
        "arxiv",
        "framework"
      ],
      "score": 0.4
    },
    {
      "title": "Inner-Instance Normalization for Time Series Forecasting",
      "url": "https://arxiv.org/abs/2510.08657",
      "description": "arXiv:2510.08657v1 Announce Type: new \nAbstract: Real-world time series are influenced by numerous factors and exhibit complex non-stationary characteristics. Non-stationarity can lead to distribution shifts, where the statistical properties of time series change over time, negatively impacting mode...",
      "published_date": "2025-10-14T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "arxiv",
        "model"
      ],
      "score": 0.4
    },
    {
      "title": "optillm - Optimizing inference proxy for LLMs",
      "url": "https://github.com/codelion/optillm",
      "description": "Optimizing inference proxy for LLMs",
      "published_date": "2024-08-22T19:46:07+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "LLM"
      ],
      "score": 0.4
    }
  ]
}