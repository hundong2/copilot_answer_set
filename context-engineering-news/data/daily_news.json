{
  "generated_at": "2025-12-02T20:06:16.067759",
  "total_items": 45,
  "items": [
    {
      "title": "Text Annotation via Inductive Coding: Comparing Human Experts to LLMs in Qualitative Data Analysis",
      "url": "https://arxiv.org/abs/2512.00046",
      "description": "arXiv:2512.00046v1 Announce Type: new \nAbstract: This paper investigates the automation of qualitative data analysis, focusing on inductive coding using large language models (LLMs). Unlike traditional approaches that rely on deductive methods with predefined labels, this research investigates the i...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "study",
        "arxiv",
        "alignment",
        "paper",
        "research",
        "analysis",
        "model",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "Emergent Convergence in Multi-Agent LLM Annotation",
      "url": "https://arxiv.org/abs/2512.00047",
      "description": "arXiv:2512.00047v1 Announce Type: new \nAbstract: Large language models (LLMs) are increasingly deployed in collaborative settings, yet little is known about how they coordinate when treated as black-box agents. We simulate 7500 multi-agent, multi-round discussions in an inductive coding task, genera...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "prompting",
        "LLM",
        "arxiv",
        "prompt",
        "alignment",
        "embedding",
        "model",
        "analysis",
        "compression",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "Tree Matching Networks for Natural Language Inference: Parameter-Efficient Semantic Understanding via Dependency Parse Trees",
      "url": "https://arxiv.org/abs/2512.00204",
      "description": "arXiv:2512.00204v1 Announce Type: new \nAbstract: In creating sentence embeddings for Natural Language Inference (NLI) tasks, using transformer-based models like BERT leads to high accuracy, but require hundreds of millions of parameters. These models take in sentences as a sequence of tokens, and le...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "arxiv",
        "attention",
        "embedding",
        "model",
        "memory",
        "transformer"
      ],
      "score": 1.0
    },
    {
      "title": "Towards Corpus-Grounded Agentic LLMs for Multilingual Grammatical Analysis",
      "url": "https://arxiv.org/abs/2512.00214",
      "description": "arXiv:2512.00214v1 Announce Type: new \nAbstract: Empirical grammar research has become increasingly data-driven, but the systematic analysis of annotated corpora still requires substantial methodological and technical effort. We explore how agentic large language models (LLMs) can streamline this pr...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "RAG",
        "LLM",
        "arxiv",
        "framework",
        "research",
        "analysis",
        "model",
        "reasoning",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "Minimal-Edit Instruction Tuning for Low-Resource Indic GEC",
      "url": "https://arxiv.org/abs/2512.00219",
      "description": "arXiv:2512.00219v1 Announce Type: new \nAbstract: Grammatical error correction for Indic languages faces limited supervision, diverse scripts, and rich morphology. We propose an augmentation-free setup that uses instruction-tuned large language models and conservative decoding. A 12B GEMMA 3 model is...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "RAG",
        "instruction",
        "vision",
        "arxiv",
        "prompt",
        "model",
        "fine-tuning",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion",
      "url": "https://arxiv.org/abs/2512.00234",
      "description": "arXiv:2512.00234v1 Announce Type: new \nAbstract: There has been significant progress in open-source text-only translation large language models (LLMs) with better language coverage and quality. However, these models can be only used in cascaded pipelines for speech translation (ST), performing autom...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "multimodal_context",
      "keywords": [
        "experiment",
        "RAG",
        "context",
        "LLM",
        "arxiv",
        "multimodal",
        "image",
        "model",
        "reasoning",
        "large language model",
        "audio"
      ],
      "score": 1.0
    },
    {
      "title": "Lost without translation -- Can transformer (language models) understand mood states?",
      "url": "https://arxiv.org/abs/2512.00274",
      "description": "arXiv:2512.00274v1 Announce Type: new \nAbstract: Background: Large Language Models show promise in psychiatry but are English-centric. Their ability to understand mood states in other languages is unclear, as different languages have their own idioms of distress. Aim: To quantify the ability of lang...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "arxiv",
        "embedding",
        "model",
        "transformer",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "EduEval: A Hierarchical Cognitive Benchmark for Evaluating Large Language Models in Chinese Education",
      "url": "https://arxiv.org/abs/2512.00290",
      "description": "arXiv:2512.00290v1 Announce Type: new \nAbstract: Large language models (LLMs) demonstrate significant potential for educational applications. However, their unscrutinized deployment poses risks to educational standards, underscoring the need for rigorous evaluation. We introduce EduEval, a comprehen...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "prompting",
        "zero-shot",
        "LLM",
        "arxiv",
        "prompt",
        "framework",
        "model",
        "reasoning",
        "few-shot",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "Comparative Analysis of 47 Context-Based Question Answer Models Across 8 Diverse Datasets",
      "url": "https://arxiv.org/abs/2512.00323",
      "description": "arXiv:2512.00323v1 Announce Type: new \nAbstract: Context-based question answering (CBQA) models provide more accurate and relevant answers by considering the contextual information. They effectively extract specific information given a context, making them functional in various applications involvin...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "context",
        "study",
        "arxiv",
        "platform",
        "analysis",
        "model",
        "retrieval",
        "fine-tuning"
      ],
      "score": 1.0
    },
    {
      "title": "Chunking Strategies for Multimodal AI Systems",
      "url": "https://arxiv.org/abs/2512.00185",
      "description": "arXiv:2512.00185v1 Announce Type: new \nAbstract: Our goal is to consolidate the landscape of multimodal chunking strategies, providing researchers and practitioners with a technical foundation and design space for developing more effective and efficient multimodal AI systems. This survey paves the w...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "multimodal_context",
      "keywords": [
        "tool",
        "context",
        "arxiv",
        "cross-modal",
        "alignment",
        "multimodal",
        "image",
        "research",
        "analysis",
        "audio"
      ],
      "score": 1.0
    },
    {
      "title": "Reasoning Under Pressure: How do Training Incentives Influence Chain-of-Thought Monitorability?",
      "url": "https://arxiv.org/abs/2512.00218",
      "description": "arXiv:2512.00218v1 Announce Type: new \nAbstract: AI systems that output their reasoning in natural language offer an opportunity for safety -- we can \\emph{monitor} their chain of thought (CoT) for undesirable reasoning, such as the pursuit of harmful objectives. However, the extent to which CoT fai...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "chain_of_thought",
      "keywords": [
        "arxiv",
        "chain-of-thought",
        "model",
        "CoT",
        "reasoning"
      ],
      "score": 1.0
    },
    {
      "title": "Trification: A Comprehensive Tree-based Strategy Planner and Structural Verification for Fact-Checking",
      "url": "https://arxiv.org/abs/2512.00267",
      "description": "arXiv:2512.00267v1 Announce Type: new \nAbstract: Technological advancement allows information to be shared in just a single click, which has enabled the rapid spread of false information. This makes automated fact-checking system necessary to ensure the safety and integrity of our online media ecosy...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "experiment",
        "RAG",
        "LLM",
        "arxiv",
        "framework",
        "model",
        "API"
      ],
      "score": 1.0
    },
    {
      "title": "ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning",
      "url": "https://arxiv.org/abs/2512.00305",
      "description": "arXiv:2512.00305v1 Announce Type: new \nAbstract: Multimodal Large Language Models (MLLMs) have emerged as powerful tools for chart comprehension. However, they heavily rely on extracted content via OCR, which leads to numerical hallucinations when chart textual annotations are sparse. While existing...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "chain_of_thought",
      "keywords": [
        "prompting",
        "RAG",
        "LLM",
        "instruction",
        "tool",
        "arxiv",
        "prompt",
        "chain-of-thought",
        "multimodal",
        "paper",
        "step-by-step",
        "model",
        "CoT",
        "reasoning",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "RL-Struct: A Lightweight Reinforcement Learning Framework for Reliable Structured Output in LLMs",
      "url": "https://arxiv.org/abs/2512.00319",
      "description": "arXiv:2512.00319v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language generation and reasoning. However, their integration into automated software ecosystems is often hindered by the \"Structure Gap\" - the inherent tension between ...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "zero-shot",
        "RAG",
        "LLM",
        "arxiv",
        "framework",
        "paper",
        "ICL",
        "model",
        "analysis",
        "reasoning",
        "fine-tuning",
        "GPT",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System",
      "url": "https://arxiv.org/abs/2512.00331",
      "description": "arXiv:2512.00331v1 Announce Type: new \nAbstract: Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeline over course materials. This design struggles in ...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "knowledge base",
        "RAG",
        "context",
        "LLM",
        "arxiv",
        "augmented",
        "model",
        "retrieval",
        "compression",
        "memory",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "Debate with Images: Detecting Deceptive Behaviors in Multimodal Large Language Models",
      "url": "https://arxiv.org/abs/2512.00349",
      "description": "arXiv:2512.00349v1 Announce Type: new \nAbstract: Are frontier AI systems becoming more capable? Certainly. Yet such progress is not an unalloyed blessing but rather a Trojan horse: behind their performance leaps lie more insidious and destructive safety risks, namely deception. Unlike hallucination,...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "multimodal_context",
      "keywords": [
        "experiment",
        "arxiv",
        "framework",
        "cross-modal",
        "chain-of-thought",
        "multimodal",
        "image",
        "research",
        "model",
        "reasoning",
        "GPT",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "Measuring What LLMs Think They Do: SHAP Faithfulness and Deployability on Financial Tabular Classification",
      "url": "https://arxiv.org/abs/2512.00163",
      "description": "arXiv:2512.00163v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have attracted significant attention for classification tasks, offering a flexible alternative to trusted classical machine learning models like LightGBM through zero-shot prompting. However, their reliability for structur...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "prompting",
        "zero-shot",
        "LLM",
        "study",
        "arxiv",
        "prompt",
        "attention",
        "model",
        "analysis",
        "few-shot",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "Orion-Bix: Bi-Axial Attention for Tabular In-Context Learning",
      "url": "https://arxiv.org/abs/2512.00181",
      "description": "arXiv:2512.00181v1 Announce Type: new \nAbstract: Tabular data drive most real-world machine learning applications, yet building general-purpose models for them remains difficult. Mixed numeric and categorical fields, weak feature structure, and limited labeled data make scaling and generalization ch...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "in_context_learning",
      "keywords": [
        "context",
        "summarization",
        "arxiv",
        "ICL",
        "attention",
        "model",
        "reasoning",
        "few-shot",
        "in-context"
      ],
      "score": 1.0
    },
    {
      "title": "Hybrid Context-Fusion Attention (CFA) U-Net and Clustering for Robust Seismic Horizon Interpretation",
      "url": "https://arxiv.org/abs/2512.00191",
      "description": "arXiv:2512.00191v1 Announce Type: new \nAbstract: Interpreting seismic horizons is a critical task for characterizing subsurface structures in hydrocarbon exploration. Recent advances in deep learning, particularly U-Net-based architectures, have significantly improved automated horizon tracking. How...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "context",
        "arxiv",
        "framework",
        "attention"
      ],
      "score": 1.0
    },
    {
      "title": "Constructing Efficient Fact-Storing MLPs for Transformers",
      "url": "https://arxiv.org/abs/2512.00207",
      "description": "arXiv:2512.00207v1 Announce Type: new \nAbstract: The success of large language models (LLMs) can be attributed in part to their ability to efficiently store factual knowledge as key-value mappings within their MLP parameters. Recent work has proposed explicit weight constructions to build such fact-...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "LLM",
        "arxiv",
        "framework",
        "paper",
        "embedding",
        "model",
        "transformer",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "Context-Engineering - \"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" â€” Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "url": "https://github.com/davidkimai/Context-Engineering",
      "description": "\"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" â€” Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "published_date": "2025-06-29T00:16:36+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "prompt",
        "context window",
        "prompt engineering",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "ThinkSound - [NeurIPS 2025] PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "url": "https://github.com/FunAudioLLM/ThinkSound",
      "description": "[NeurIPS 2025] PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "published_date": "2025-06-27T02:27:00+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "framework",
        "chain-of-thought",
        "CoT",
        "reasoning",
        "audio"
      ],
      "score": 1.0
    },
    {
      "title": "mcp-context-forge - A Model Context Protocol (MCP) Gateway & Registry. Serves as a central management point for tools, resources, and prompts that can be accessed by MCP-compatible LLM applications. Converts REST API endpoints to MCP, composes virtual MCP servers with added security and observability, and converts between protocols (stdio, SSE, Streamable HTTP).",
      "url": "https://github.com/IBM/mcp-context-forge",
      "description": "A Model Context Protocol (MCP) Gateway & Registry. Serves as a central management point for tools, resources, and prompts that can be accessed by MCP-compatible LLM applications. Converts REST API endpoints to MCP, composes virtual MCP servers with added security and observability, and converts between protocols (stdio, SSE, Streamable HTTP).",
      "published_date": "2025-05-08T08:16:59+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "tool",
        "context",
        "LLM",
        "prompt",
        "model",
        "API"
      ],
      "score": 1.0
    },
    {
      "title": "Cline-Recursive-Chain-of-Thought-System-CRCT- - A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "url": "https://github.com/RPG-fan/Cline-Recursive-Chain-of-Thought-System-CRCT-",
      "description": "A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "published_date": "2025-02-18T15:45:30+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "framework",
        "context",
        "chain-of-thought"
      ],
      "score": 1.0
    },
    {
      "title": "airweave - Context retrieval for AI agents across apps and databases",
      "url": "https://github.com/airweave-ai/airweave",
      "description": "Context retrieval for AI agents across apps and databases",
      "published_date": "2024-12-24T10:00:06+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "context",
        "retrieval"
      ],
      "score": 1.0
    },
    {
      "title": "LightRAG - [EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "url": "https://github.com/HKUDS/LightRAG",
      "description": "[EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "published_date": "2024-10-02T11:57:54+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "augmented",
        "retrieval"
      ],
      "score": 1.0
    },
    {
      "title": "KAG - KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "url": "https://github.com/OpenSPG/KAG",
      "description": "KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "published_date": "2024-09-21T13:56:44+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "knowledge base",
        "RAG",
        "LLM",
        "framework",
        "model",
        "vector",
        "reasoning",
        "retrieval"
      ],
      "score": 1.0
    },
    {
      "title": "Kiln - Easily build AI systems with Evals, RAG, Agents, fine-tuning, synthetic data, and more.",
      "url": "https://github.com/Kiln-AI/Kiln",
      "description": "Easily build AI systems with Evals, RAG, Agents, fine-tuning, synthetic data, and more.",
      "published_date": "2024-07-23T23:10:13+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "fine-tuning",
        "RAG"
      ],
      "score": 1.0
    },
    {
      "title": "graphrag - A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "url": "https://github.com/microsoft/graphrag",
      "description": "A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "published_date": "2024-03-27T17:57:52+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "augmented",
        "retrieval"
      ],
      "score": 1.0
    },
    {
      "title": "R2R - SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "url": "https://github.com/SciPhi-AI/R2R",
      "description": "SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "published_date": "2024-02-12T03:24:27+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "augmented",
        "product",
        "retrieval",
        "API"
      ],
      "score": 1.0
    },
    {
      "title": "openlit - Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. ðŸš€ðŸ’» Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "url": "https://github.com/openlit/openlit",
      "description": "Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. ðŸš€ðŸ’» Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "published_date": "2024-01-23T17:40:59+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "LLM",
        "prompt",
        "framework",
        "platform",
        "vector"
      ],
      "score": 1.0
    },
    {
      "title": "Gold-Medal-Level Olympiad Geometry Solving with Efficient Heuristic Auxiliary Constructions",
      "url": "https://arxiv.org/abs/2512.00097",
      "description": "arXiv:2512.00097v1 Announce Type: new \nAbstract: Automated theorem proving in Euclidean geometry, particularly for International Mathematical Olympiad (IMO) level problems, remains a major challenge and an important research focus in Artificial Intelligence. In this paper, we present a highly effici...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "research",
        "study",
        "paper",
        "arxiv"
      ],
      "score": 0.8
    },
    {
      "title": "fastapi_mcp - Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "url": "https://github.com/tadata-org/fastapi_mcp",
      "description": "Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "published_date": "2025-03-08T11:15:43+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "API",
        "tool",
        "model",
        "context"
      ],
      "score": 0.8
    },
    {
      "title": "cosmos-reason1 - Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "url": "https://github.com/nvidia-cosmos/cosmos-reason1",
      "description": "Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "published_date": "2025-03-02T15:23:55+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "model",
        "chain-of-thought",
        "reasoning"
      ],
      "score": 0.8
    },
    {
      "title": "Evidence-Guided Schema Normalization for Temporal Tabular Reasoning",
      "url": "https://arxiv.org/abs/2512.00329",
      "description": "arXiv:2512.00329v1 Announce Type: new \nAbstract: Temporal reasoning over evolving semi-structured tables poses a challenge to current QA systems. We propose a SQL-based approach that involves (1) generating a 3NF schema from Wikipedia infoboxes, (2) generating SQL queries, and (3) query execution. O...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "chain_of_thought",
      "keywords": [
        "model",
        "context",
        "reasoning",
        "arxiv"
      ],
      "score": 0.6
    },
    {
      "title": "Echo-N1: Affective RL Frontier",
      "url": "https://arxiv.org/abs/2512.00344",
      "description": "arXiv:2512.00344v1 Announce Type: new \nAbstract: The LLM field has spent a year perfecting RL for tasks machines already excel at, math, code, and deterministic reasoning, while completely sidestepping the domain that actually defines human intelligence: subjective, emotionally grounded, personality...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "chain_of_thought",
      "keywords": [
        "LLM",
        "arxiv",
        "framework",
        "model",
        "reasoning"
      ],
      "score": 0.6
    },
    {
      "title": "Polynomial Neural Sheaf Diffusion: A Spectral Filtering Approach on Cellular Sheaves",
      "url": "https://arxiv.org/abs/2512.00242",
      "description": "arXiv:2512.00242v1 Announce Type: new \nAbstract: Sheaf Neural Networks equip graph structures with a cellular sheaf: a geometric structure which assigns local vector spaces (stalks) and a linear learnable restriction/transport maps to nodes and edges, yielding an edge-aware inductive bias that handl...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "context_management",
      "keywords": [
        "memory",
        "vector",
        "arxiv"
      ],
      "score": 0.6
    },
    {
      "title": "Transformers v5: Simple model definitions powering the AI ecosystem",
      "url": "https://huggingface.co/blog/transformers-v5",
      "description": "...",
      "published_date": "2025-12-01T00:00:00",
      "source": "Hugging Face Blog",
      "category": "industry_news",
      "keywords": [
        "transformer",
        "model"
      ],
      "score": 0.6
    },
    {
      "title": "mcp-agent - Build effective agents using Model Context Protocol and simple workflow patterns",
      "url": "https://github.com/lastmile-ai/mcp-agent",
      "description": "Build effective agents using Model Context Protocol and simple workflow patterns",
      "published_date": "2024-12-18T01:55:10+00:00",
      "source": "GitHub",
      "category": "industry_news",
      "keywords": [
        "model",
        "context"
      ],
      "score": 0.6
    },
    {
      "title": "AlphaCodium - Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "url": "https://github.com/Codium-ai/AlphaCodium",
      "description": "Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "published_date": "2024-01-14T15:17:18+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "prompt",
        "prompt engineering",
        "paper"
      ],
      "score": 0.6
    },
    {
      "title": "A Rosetta Stone for AI Benchmarks",
      "url": "https://arxiv.org/abs/2512.00193",
      "description": "arXiv:2512.00193v1 Announce Type: new \nAbstract: Most AI benchmarks saturate within years or even months after they are introduced, making it hard to study long-run trends in AI capabilities. To address this challenge, we build a statistical framework that stitches benchmarks together, putting model...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "study",
        "arxiv",
        "framework",
        "model",
        "API"
      ],
      "score": 0.4
    },
    {
      "title": "Faster Verified Explanations for Neural Networks",
      "url": "https://arxiv.org/abs/2512.00164",
      "description": "arXiv:2512.00164v1 Announce Type: new \nAbstract: Verified explanations are a theoretically-principled way to explain the decisions taken by neural networks, which are otherwise black-box in nature. However, these techniques face significant scalability challenges, as they require multiple calls to n...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "arxiv"
      ],
      "score": 0.4
    },
    {
      "title": "TIE: A Training-Inversion-Exclusion Framework for Visually Interpretable and Uncertainty-Guided Out-of-Distribution Detection",
      "url": "https://arxiv.org/abs/2512.00229",
      "description": "arXiv:2512.00229v1 Announce Type: new \nAbstract: Deep neural networks often struggle to recognize when an input lies outside their training experience, leading to unreliable and overconfident predictions. Building dependable machine learning systems therefore requires methods that can both estimate ...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "framework",
        "model",
        "paper",
        "arxiv"
      ],
      "score": 0.4
    },
    {
      "title": "Self-Supervised Dynamical System Representations for Physiological Time-Series",
      "url": "https://arxiv.org/abs/2512.00239",
      "description": "arXiv:2512.00239v1 Announce Type: new \nAbstract: The effectiveness of self-supervised learning (SSL) for physiological time series depends on the ability of a pretraining objective to preserve information about the underlying physiological state while filtering out unrelated noise. However, existing...",
      "published_date": "2025-12-02T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "framework",
        "model",
        "arxiv"
      ],
      "score": 0.4
    },
    {
      "title": "optillm - Optimizing inference proxy for LLMs",
      "url": "https://github.com/algorithmicsuperintelligence/optillm",
      "description": "Optimizing inference proxy for LLMs",
      "published_date": "2024-08-22T19:46:07+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "LLM"
      ],
      "score": 0.4
    }
  ]
}