<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Context Engineering Daily - Context Management</title>
    <link>https://your-username.github.io/context-engineering-news#context_management</link>
    <description>Latest Context Management news in Context Engineering</description>
    <language>en-us</language>
    <item>
      <title>Efficient Attention Mechanisms for Large Language Models: A Survey</title>
      <link>https://arxiv.org/abs/2507.19595</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2507.19595</guid>
      <description>arXiv:2507.19595v1 Announce Type: new 
Abstract: Transformer-based architectures have become the prevailing backbone of large language models. However, the quadratic time and memory complexity of self-attention remains a fundamental obstacle to efficient long-context modeling. To address this limita...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; attention, research, arxiv, context, RAG | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 29 Jul 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Context Management</category>
      <category>attention</category>
      <category>research</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference</title>
      <link>https://arxiv.org/abs/2507.19608</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2507.19608</guid>
      <description>arXiv:2507.19608v1 Announce Type: new 
Abstract: Deploying Large Language Models (LLMs) on edge devices remains challenging due to their quadratically increasing computations with the sequence length. Existing studies for dynamic attention pruning are designed for hardware with massively parallel co...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; attention, fine-tuning, LLM, context window, context | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 29 Jul 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Context Management</category>
      <category>attention</category>
      <category>fine-tuning</category>
      <category>LLM</category>
    </item>
    <item>
      <title>Wavelet Logic Machines: Learning and Reasoning in the Spectral Domain Without Neural Networks</title>
      <link>https://arxiv.org/abs/2507.19514</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2507.19514</guid>
      <description>arXiv:2507.19514v1 Announce Type: new 
Abstract: We introduce a fully spectral learning framework that eliminates traditional neural layers by operating entirely in the wavelet domain. The model applies learnable nonlinear transformations, including soft-thresholding and gain-phase modulation, direc...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; attention, arxiv, framework, reasoning, memory | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 29 Jul 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Context Management</category>
      <category>attention</category>
      <category>arxiv</category>
      <category>framework</category>
    </item>
    <item>
      <title>MoBA - MoBA: Mixture of Block Attention for Long-Context LLMs</title>
      <link>https://github.com/MoonshotAI/MoBA</link>
      <guid isPermaLink="false">https://github.com/MoonshotAI/MoBA</guid>
      <description>MoBA: Mixture of Block Attention for Long-Context LLMs&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; GitHub | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; attention, LLM, context | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Mon, 17 Feb 2025 13:27:30 +0000</pubDate>
      <author>noreply@contextengineering.news (GitHub)</author>
      <category>Context Management</category>
      <category>attention</category>
      <category>LLM</category>
      <category>context</category>
    </item>
  </channel>
</rss>