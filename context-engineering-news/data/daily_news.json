{
  "generated_at": "2025-11-12T20:06:01.816949",
  "total_items": 43,
  "items": [
    {
      "title": "A Preliminary Study of RAG for Taiwanese Historical Archives",
      "url": "https://arxiv.org/abs/2511.07445",
      "description": "arXiv:2511.07445v1 Announce Type: new \nAbstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach for knowledge-intensive tasks. However, few studies have examined RAG for Taiwanese Historical Archives. In this paper, we present an initial study of a RAG pipeline applied to t...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "retrieval",
        "augmented",
        "study",
        "arxiv",
        "paper",
        "RAG"
      ],
      "score": 1.0
    },
    {
      "title": "Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey",
      "url": "https://arxiv.org/abs/2511.07448",
      "description": "arXiv:2511.07448v1 Announce Type: new \nAbstract: Scientific idea generation lies at the heart of scientific discovery and has driven human progress-whether by solving unsolved problems or proposing novel hypotheses to explain unknown phenomena. Unlike standard scientific reasoning or general creativ...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "industry_news",
      "keywords": [
        "model",
        "arxiv",
        "large language model",
        "framework",
        "prompt",
        "reasoning",
        "LLM",
        "product"
      ],
      "score": 1.0
    },
    {
      "title": "GRIP: In-Parameter Graph Reasoning through Fine-Tuning Large Language Models",
      "url": "https://arxiv.org/abs/2511.07457",
      "description": "arXiv:2511.07457v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in modeling sequential textual data and generalizing across diverse tasks. However, adapting LLMs to effectively handle structural data, such as knowledge graphs or web data, remai...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "alignment",
        "model",
        "fine-tuning",
        "arxiv",
        "large language model",
        "framework",
        "reasoning",
        "experiment",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "REFLEX: Reference-Free Evaluation of Log Summarization via Large Language Model Judgment",
      "url": "https://arxiv.org/abs/2511.07458",
      "description": "arXiv:2511.07458v1 Announce Type: new \nAbstract: Evaluating log summarization systems is challenging due to the lack of high-quality reference summaries and the limitations of existing metrics like ROUGE and BLEU, which depend on surface-level lexical overlap. We introduce REFLEX, a reference-free e...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "model",
        "zero-shot",
        "arxiv",
        "large language model",
        "summarization",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "It Takes Two: A Dual Stage Approach for Terminology-Aware Translation",
      "url": "https://arxiv.org/abs/2511.07461",
      "description": "arXiv:2511.07461v1 Announce Type: new \nAbstract: This paper introduces DuTerm, a novel two-stage architecture for terminology-constrained machine translation. Our system combines a terminology-aware NMT model, adapted via fine-tuning on large-scale synthetic data, with a prompt-based LLM for post-ed...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "context",
        "model",
        "fine-tuning",
        "arxiv",
        "paper",
        "prompt",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "Motif 2 12.7B technical report",
      "url": "https://arxiv.org/abs/2511.07464",
      "description": "arXiv:2511.07464v1 Announce Type: new \nAbstract: We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust i...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "context_management",
      "keywords": [
        "instruction",
        "model",
        "fine-tuning",
        "memory",
        "arxiv",
        "large language model",
        "attention",
        "RAG"
      ],
      "score": 1.0
    },
    {
      "title": "Focusing on Language: Revealing and Exploiting Language Attention Heads in Multilingual Large Language Models",
      "url": "https://arxiv.org/abs/2511.07498",
      "description": "arXiv:2511.07498v1 Announce Type: new \nAbstract: Large language models (LLMs) increasingly support multilingual understanding and generation. Meanwhile, efforts to interpret their internal mechanisms have emerged, offering insights to enhance multilingual performance. While multi-head self-attention...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "context",
        "study",
        "model",
        "arxiv",
        "large language model",
        "attention",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "LLM Optimization Unlocks Real-Time Pairwise Reranking",
      "url": "https://arxiv.org/abs/2511.07555",
      "description": "arXiv:2511.07555v1 Announce Type: new \nAbstract: Efficiently reranking documents retrieved from information retrieval (IR) pipelines to enhance overall quality of Retrieval-Augmented Generation (RAG) system remains an important yet challenging problem. Recent studies have highlighted the importance ...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "retrieval",
        "augmented",
        "study",
        "model",
        "prompting",
        "large language model",
        "arxiv",
        "paper",
        "prompt",
        "RAG",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "LLMs vs. Traditional Sentiment Tools in Psychology: An Evaluation on Belgian-Dutch Narratives",
      "url": "https://arxiv.org/abs/2511.07641",
      "description": "arXiv:2511.07641v1 Announce Type: new \nAbstract: Understanding emotional nuances in everyday language is crucial for computational linguistics and emotion research. While traditional lexicon-based tools like LIWC and Pattern have served as foundational instruments, Large Language Models (LLMs) promi...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "context",
        "model",
        "fine-tuning",
        "arxiv",
        "large language model",
        "framework",
        "research",
        "analysis",
        "tool",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "Revisiting NLI: Towards Cost-Effective and Human-Aligned Metrics for Evaluating LLMs in Question Answering",
      "url": "https://arxiv.org/abs/2511.07659",
      "description": "arXiv:2511.07659v1 Announce Type: new \nAbstract: Evaluating answers from state-of-the-art large language models (LLMs) is challenging: lexical metrics miss semantic nuances, whereas \"LLM-as-Judge\" scoring is computationally expensive. We re-evaluate a lightweight alternative -- off-the-shelf Natural...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "augmented",
        "alignment",
        "GPT",
        "model",
        "arxiv",
        "large language model",
        "research",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "Analysing Environmental Efficiency in AI for X-Ray Diagnosis",
      "url": "https://arxiv.org/abs/2511.07436",
      "description": "arXiv:2511.07436v1 Announce Type: new \nAbstract: The integration of AI tools into medical applications has aimed to improve the efficiency of diagnosis. The emergence of large language models (LLMs), such as ChatGPT and Claude, has expanded this integration even further. Because of LLM versatility a...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "API",
        "study",
        "knowledge base",
        "GPT",
        "model",
        "arxiv",
        "large language model",
        "paper",
        "tool",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "Agentic Educational Content Generation for African Languages on Edge Devices",
      "url": "https://arxiv.org/abs/2511.07437",
      "description": "arXiv:2511.07437v1 Announce Type: new \nAbstract: Addressing educational inequity in Sub-Saharan Africa, this research presents an autonomous agent-orchestrated framework for decentralized, culturally adaptive educational content generation on edge devices. The system leverages four specialized agent...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "context",
        "arxiv",
        "platform",
        "framework",
        "research",
        "RAG",
        "experiment"
      ],
      "score": 1.0
    },
    {
      "title": "Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning",
      "url": "https://arxiv.org/abs/2511.07483",
      "description": "arXiv:2511.07483v1 Announce Type: new \nAbstract: Recent advancements in large language models (LLMs) have shifted the post-training paradigm from traditional instruction tuning and human preference alignment toward reinforcement learning (RL) focused on reasoning capabilities. However, numerous tech...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "industry_news",
      "keywords": [
        "alignment",
        "release",
        "instruction",
        "model",
        "arxiv",
        "large language model",
        "reasoning",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "Procedural Knowledge Improves Agentic LLM Workflows",
      "url": "https://arxiv.org/abs/2511.07568",
      "description": "arXiv:2511.07568v1 Announce Type: new \nAbstract: Large language models (LLMs) often struggle when performing agentic tasks without substantial tool support, prom-pt engineering, or fine tuning. Despite research showing that domain-dependent, procedural knowledge can dramatically increase planning ef...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "model",
        "RAG",
        "large language model",
        "arxiv",
        "research",
        "tool",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models",
      "url": "https://arxiv.org/abs/2511.07581",
      "description": "arXiv:2511.07581v1 Announce Type: new \nAbstract: Effective information retrieval requires reasoning over partial evidence and refining strategies as information emerges. Yet current approaches fall short: neural retrievers lack reasoning capabilities, large language models (LLMs) provide semantic de...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "retrieval",
        "model",
        "fine-tuning",
        "arxiv",
        "large language model",
        "framework",
        "RAG",
        "LLM",
        "vision",
        "reasoning"
      ],
      "score": 1.0
    },
    {
      "title": "Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces",
      "url": "https://arxiv.org/abs/2511.07587",
      "description": "arXiv:2511.07587v1 Announce Type: new \nAbstract: Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with externa...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "context",
        "retrieval",
        "model",
        "memory",
        "context window",
        "arxiv",
        "large language model",
        "embedding",
        "framework",
        "RAG",
        "LLM",
        "reasoning"
      ],
      "score": 1.0
    },
    {
      "title": "AI-Driven Contribution Evaluation and Conflict Resolution: A Framework & Design for Group Workload Investigation",
      "url": "https://arxiv.org/abs/2511.07667",
      "description": "arXiv:2511.07667v1 Announce Type: new \nAbstract: The equitable assessment of individual contribution in teams remains a persistent challenge, where conflict and disparity in workload can result in unfair performance evaluation, often requiring manual intervention - a costly and challenging process. ...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "context",
        "model",
        "arxiv",
        "large language model",
        "framework",
        "analysis",
        "tool",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "Making LLMs Reliable When It Matters Most: A Five-Layer Architecture for High-Stakes Decisions",
      "url": "https://arxiv.org/abs/2511.07669",
      "description": "arXiv:2511.07669v1 Announce Type: new \nAbstract: Current large language models (LLMs) excel in verifiable domains where outputs can be checked before action but prove less reliable for high-stakes strategic decisions with uncertain outcomes. This gap, driven by mutually reinforcing cognitive biases ...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "context",
        "model",
        "prompting",
        "large language model",
        "arxiv",
        "framework",
        "prompt",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "AIA Forecaster: Technical Report",
      "url": "https://arxiv.org/abs/2511.07678",
      "description": "arXiv:2511.07678v1 Announce Type: new \nAbstract: This technical report describes the AIA Forecaster, a Large Language Model (LLM)-based system for judgmental forecasting using unstructured data. The AIA Forecaster approach combines three core elements: agentic search over high-quality news sources, ...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "model",
        "arxiv",
        "large language model",
        "research",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents",
      "url": "https://arxiv.org/abs/2511.07685",
      "description": "arXiv:2511.07685v1 Announce Type: new \nAbstract: Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the gener...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "context",
        "release",
        "model",
        "arxiv",
        "large language model",
        "framework",
        "research",
        "prompt",
        "RAG",
        "LLM",
        "reasoning"
      ],
      "score": 1.0
    },
    {
      "title": "Optimizing Classification of Infrequent Labels by Reducing Variability in Label Distribution",
      "url": "https://arxiv.org/abs/2511.07459",
      "description": "arXiv:2511.07459v1 Announce Type: new \nAbstract: This paper presents a novel solution, LEVER, designed to address the challenges posed by underperforming infrequent categories in Extreme Classification (XC) tasks. Infrequent categories, often characterized by sparse samples, suffer from high label i...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "paper",
        "RAG",
        "research",
        "arxiv"
      ],
      "score": 1.0
    },
    {
      "title": "Towards Personalized Quantum Federated Learning for Anomaly Detection",
      "url": "https://arxiv.org/abs/2511.07471",
      "description": "arXiv:2511.07471v1 Announce Type: new \nAbstract: Anomaly detection has a significant impact on applications such as video surveillance, medical diagnostics, and industrial monitoring, where anomalies frequently depend on context and anomaly-labeled data are limited. Quantum federated learning (QFL) ...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "context",
        "model",
        "arxiv",
        "framework",
        "RAG",
        "experiment"
      ],
      "score": 1.0
    },
    {
      "title": "Comparing Reconstruction Attacks on Pretrained Versus Full Fine-tuned Large Language Model Embeddings on Homo Sapiens Splice Sites Genomic Data",
      "url": "https://arxiv.org/abs/2511.07481",
      "description": "arXiv:2511.07481v1 Announce Type: new \nAbstract: This study investigates embedding reconstruction attacks in large language models (LLMs) applied to genomic sequences, with a specific focus on how fine-tuning affects vulnerability to these attacks. Building upon Pan et al.'s seminal work demonstrati...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "API",
        "study",
        "GPT",
        "model",
        "fine-tuning",
        "arxiv",
        "large language model",
        "embedding",
        "research",
        "analysis",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "Alignment-Constrained Dynamic Pruning for LLMs: Identifying and Preserving Alignment-Critical Circuits",
      "url": "https://arxiv.org/abs/2511.07482",
      "description": "arXiv:2511.07482v1 Announce Type: new \nAbstract: Large Language Models require substantial computational resources for inference, posing deployment challenges. While dynamic pruning offers superior efficiency over static methods through adaptive circuit selection, it exacerbates alignment degradatio...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "alignment",
        "model",
        "arxiv",
        "large language model",
        "experiment",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "Context-Engineering - \"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" â€” Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "url": "https://github.com/davidkimai/Context-Engineering",
      "description": "\"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" â€” Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "published_date": "2025-06-29T00:16:36+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "context",
        "prompt engineering",
        "context window",
        "prompt"
      ],
      "score": 1.0
    },
    {
      "title": "ThinkSound - [NeurIPS 2025] PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "url": "https://github.com/FunAudioLLM/ThinkSound",
      "description": "[NeurIPS 2025] PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "published_date": "2025-06-27T02:27:00+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "CoT",
        "framework",
        "chain-of-thought",
        "audio",
        "reasoning"
      ],
      "score": 1.0
    },
    {
      "title": "mcp-context-forge - A Model Context Protocol (MCP) Gateway & Registry. Serves as a central management point for tools, resources, and prompts that can be accessed by MCP-compatible LLM applications. Converts REST API endpoints to MCP, composes virtual MCP servers with added security and observability, and converts between protocols (stdio, SSE, Streamable HTTP).",
      "url": "https://github.com/IBM/mcp-context-forge",
      "description": "A Model Context Protocol (MCP) Gateway & Registry. Serves as a central management point for tools, resources, and prompts that can be accessed by MCP-compatible LLM applications. Converts REST API endpoints to MCP, composes virtual MCP servers with added security and observability, and converts between protocols (stdio, SSE, Streamable HTTP).",
      "published_date": "2025-05-08T08:16:59+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "context",
        "API",
        "model",
        "prompt",
        "tool",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "Cline-Recursive-Chain-of-Thought-System-CRCT- - A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "url": "https://github.com/RPG-fan/Cline-Recursive-Chain-of-Thought-System-CRCT-",
      "description": "A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "published_date": "2025-02-18T15:45:30+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "context",
        "framework",
        "chain-of-thought"
      ],
      "score": 1.0
    },
    {
      "title": "airweave - Context retrieval for AI agents across apps and databases",
      "url": "https://github.com/airweave-ai/airweave",
      "description": "Context retrieval for AI agents across apps and databases",
      "published_date": "2024-12-24T10:00:06+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "context",
        "retrieval"
      ],
      "score": 1.0
    },
    {
      "title": "LightRAG - [EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "url": "https://github.com/HKUDS/LightRAG",
      "description": "[EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "published_date": "2024-10-02T11:57:54+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "augmented",
        "retrieval",
        "RAG"
      ],
      "score": 1.0
    },
    {
      "title": "KAG - KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "url": "https://github.com/OpenSPG/KAG",
      "description": "KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "published_date": "2024-09-21T13:56:44+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "retrieval",
        "knowledge base",
        "vector",
        "model",
        "framework",
        "RAG",
        "LLM",
        "reasoning"
      ],
      "score": 1.0
    },
    {
      "title": "Kiln - The easiest tool for fine-tuning LLM models, synthetic data generation, and collaborating on datasets.",
      "url": "https://github.com/Kiln-AI/Kiln",
      "description": "The easiest tool for fine-tuning LLM models, synthetic data generation, and collaborating on datasets.",
      "published_date": "2024-07-23T23:10:13+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "tool",
        "model",
        "fine-tuning",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "graphrag - A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "url": "https://github.com/microsoft/graphrag",
      "description": "A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "published_date": "2024-03-27T17:57:52+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "augmented",
        "retrieval",
        "RAG"
      ],
      "score": 1.0
    },
    {
      "title": "R2R - SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "url": "https://github.com/SciPhi-AI/R2R",
      "description": "SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "published_date": "2024-02-12T03:24:27+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "API",
        "retrieval",
        "augmented",
        "RAG",
        "product"
      ],
      "score": 1.0
    },
    {
      "title": "openlit - Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. ðŸš€ðŸ’» Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "url": "https://github.com/openlit/openlit",
      "description": "Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. ðŸš€ðŸ’» Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "published_date": "2024-01-23T17:40:59+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "vector",
        "platform",
        "framework",
        "prompt",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "Counterfactual Forecasting of Human Behavior using Generative AI and Causal Graphs",
      "url": "https://arxiv.org/abs/2511.07484",
      "description": "arXiv:2511.07484v1 Announce Type: new \nAbstract: This study presents a novel framework for counterfactual user behavior forecasting that combines structural causal models with transformer-based generative artificial intelligence. To model fictitious situations, the method creates causal graphs that ...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "study",
        "model",
        "arxiv",
        "framework",
        "transformer",
        "product"
      ],
      "score": 0.8
    },
    {
      "title": "fastapi_mcp - Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "url": "https://github.com/tadata-org/fastapi_mcp",
      "description": "Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "published_date": "2025-03-08T11:15:43+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "context",
        "tool",
        "API",
        "model"
      ],
      "score": 0.8
    },
    {
      "title": "cosmos-reason1 - Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "url": "https://github.com/nvidia-cosmos/cosmos-reason1",
      "description": "Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "published_date": "2025-03-02T15:23:55+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "reasoning",
        "model",
        "chain-of-thought"
      ],
      "score": 0.8
    },
    {
      "title": "Provably Efficient Sample Complexity for Robust CMDP",
      "url": "https://arxiv.org/abs/2511.07486",
      "description": "arXiv:2511.07486v1 Announce Type: new \nAbstract: We study the problem of learning policies that maximize cumulative reward while satisfying safety constraints, even when the real environment differs from a simulator or nominal model. We focus on robust constrained Markov decision processes (RCMDPs),...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "study",
        "augmented",
        "model",
        "arxiv",
        "paper"
      ],
      "score": 0.6
    },
    {
      "title": "mcp-agent - Build effective agents using Model Context Protocol and simple workflow patterns",
      "url": "https://github.com/lastmile-ai/mcp-agent",
      "description": "Build effective agents using Model Context Protocol and simple workflow patterns",
      "published_date": "2024-12-18T01:55:10+00:00",
      "source": "GitHub",
      "category": "industry_news",
      "keywords": [
        "context",
        "model"
      ],
      "score": 0.6
    },
    {
      "title": "AlphaCodium - Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "url": "https://github.com/Codium-ai/AlphaCodium",
      "description": "Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "published_date": "2024-01-14T15:17:18+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "prompt engineering",
        "paper",
        "prompt"
      ],
      "score": 0.6
    },
    {
      "title": "When Are Learning Biases Equivalent? A Unifying Framework for Fairness, Robustness, and Distribution Shift",
      "url": "https://arxiv.org/abs/2511.07485",
      "description": "arXiv:2511.07485v1 Announce Type: new \nAbstract: Machine learning systems exhibit diverse failure modes: unfairness toward protected groups, brittleness to spurious correlations, poor performance on minority sub-populations, which are typically studied in isolation by distinct research communities. ...",
      "published_date": "2025-11-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "model",
        "framework",
        "research",
        "arxiv"
      ],
      "score": 0.4
    },
    {
      "title": "optillm - Optimizing inference proxy for LLMs",
      "url": "https://github.com/algorithmicsuperintelligence/optillm",
      "description": "Optimizing inference proxy for LLMs",
      "published_date": "2024-08-22T19:46:07+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "LLM"
      ],
      "score": 0.4
    }
  ]
}