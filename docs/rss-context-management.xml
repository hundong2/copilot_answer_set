<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Context Engineering Daily - Context Management</title>
    <link>https://your-username.github.io/context-engineering-news#context_management</link>
    <description>Latest Context Management news in Context Engineering</description>
    <language>en-us</language>
    <item>
      <title>LADY: Linear Attention for Autonomous Driving Efficiency without Transformers</title>
      <link>https://arxiv.org/abs/2512.15038</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.15038</guid>
      <description>arXiv:2512.15038v2 Announce Type: new 
Abstract: End-to-end paradigms have demonstrated great potential for autonomous driving. Additionally, most existing methods are built upon Transformer architectures. However, transformers incur a quadratic attention cost, limiting their ability to model long s...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; experiment, cross-modal, transformer, attention, context | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 19 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Context Management</category>
      <category>experiment</category>
      <category>cross-modal</category>
      <category>transformer</category>
    </item>
    <item>
      <title>Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation</title>
      <link>https://arxiv.org/abs/2512.16189</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.16189</guid>
      <description>arXiv:2512.16189v1 Announce Type: new 
Abstract: In healthcare, it is essential for any LLM-generated output to be reliable and accurate, particularly in cases involving decision-making and patient safety. However, the outputs are often unreliable in such critical areas due to the risk of hallucinat...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; LLM, summarization, model, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 60%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 19 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Context Management</category>
      <category>LLM</category>
      <category>summarization</category>
      <category>model</category>
    </item>
    <item>
      <title>SHARe-KAN: Holographic Vector Quantization for Memory-Bound Inference</title>
      <link>https://arxiv.org/abs/2512.15742</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.15742</guid>
      <description>arXiv:2512.15742v1 Announce Type: new 
Abstract: Kolmogorov-Arnold Networks (KANs) face a fundamental memory wall: their learned basis functions create parameter counts that impose extreme bandwidth demands, hindering deployment in memory-constrained environments. We show that Vision KANs exhibit a ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; vector, vision, arxiv, framework, memory | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 60%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 19 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Context Management</category>
      <category>vector</category>
      <category>vision</category>
      <category>arxiv</category>
    </item>
  </channel>
</rss>