<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Context Engineering Daily - Research Papers</title>
    <link>https://your-username.github.io/context-engineering-news#research_papers</link>
    <description>Latest Research Papers news in Context Engineering</description>
    <language>en-us</language>
    <item>
      <title>Computational Linguistics Meets Libyan Dialect: A Study on Dialect Identification</title>
      <link>https://arxiv.org/abs/2512.04257</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.04257</guid>
      <description>arXiv:2512.04257v1 Announce Type: new 
Abstract: This study investigates logistic regression, linear support vector machine, multinomial Naive Bayes, and Bernoulli Naive Bayes for classifying Libyan dialect utterances gathered from Twitter. The dataset used is the QADI corpus, which consists of 540,...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; research, study, arxiv, analysis, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 05 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>research</category>
      <category>study</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle</title>
      <link>https://arxiv.org/abs/2512.04324</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.04324</guid>
      <description>arXiv:2512.04324v1 Announce Type: new 
Abstract: Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; RAG, arxiv, LLM, analysis, reasoning | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 05 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>RAG</category>
      <category>arxiv</category>
      <category>LLM</category>
    </item>
    <item>
      <title>MSME: A Multi-Stage Multi-Expert Framework for Zero-Shot Stance Detection</title>
      <link>https://arxiv.org/abs/2512.04492</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.04492</guid>
      <description>arXiv:2512.04492v1 Announce Type: new 
Abstract: LLM-based approaches have recently achieved impressive results in zero-shot stance detection. However, they still struggle in complex real-world scenarios, where stance understanding requires dynamic background knowledge, target definitions involve co...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; RAG, zero-shot, framework, LLM, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 05 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>RAG</category>
      <category>zero-shot</category>
      <category>framework</category>
    </item>
    <item>
      <title>Exploring Syntropic Frameworks in AI Alignment: A Philosophical Investigation</title>
      <link>https://arxiv.org/abs/2512.03048</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.03048</guid>
      <description>arXiv:2512.03048v1 Announce Type: new 
Abstract: I argue that AI alignment should be reconceived as architecting syntropic, reasons-responsive agents through process-based, multi-agent, developmental mechanisms rather than encoding fixed human value content. The paper makes three philosophical contr...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; research, paper, framework, arxiv, experiment | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 05 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>research</category>
      <category>paper</category>
      <category>framework</category>
    </item>
    <item>
      <title>When Do Symbolic Solvers Enhance Reasoning in Large Language Models?</title>
      <link>https://arxiv.org/abs/2512.03272</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.03272</guid>
      <description>arXiv:2512.03272v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) achieve strong performance on complex reasoning tasks by generating long Chains of Thought (CoTs). However, this paradigm might incur substantial token overhead, especially when models &amp;quot;overthink&amp;quot; by producing lengthy rea...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; CoT, RAG, paper, LLM, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 05 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>CoT</category>
      <category>RAG</category>
      <category>paper</category>
    </item>
    <item>
      <title>Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia</title>
      <link>https://arxiv.org/abs/2512.03318</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.03318</guid>
      <description>arXiv:2512.03318v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; paper, zero-shot, LLM, arxiv, context | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 05 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>paper</category>
      <category>zero-shot</category>
      <category>LLM</category>
    </item>
    <item>
      <title>PARC: An Autonomous Self-Reflective Coding Agent for Robust Execution of Long-Horizon Tasks</title>
      <link>https://arxiv.org/abs/2512.03549</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.03549</guid>
      <description>arXiv:2512.03549v1 Announce Type: new 
Abstract: We introduce PARC, a coding agent for the autonomous and robust execution of long-horizon computational tasks. PARC is built on a hierarchical multi-agent architecture incorporating task planning, execution, and a mechanism that evaluates its own acti...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; arxiv, context, analysis, experiment, instruction | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 05 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>arxiv</category>
      <category>context</category>
      <category>analysis</category>
    </item>
    <item>
      <title>Sarcasm Detection on Reddit Using Classical Machine Learning and Feature Engineering</title>
      <link>https://arxiv.org/abs/2512.04396</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.04396</guid>
      <description>arXiv:2512.04396v1 Announce Type: new 
Abstract: Sarcasm is common in online discussions, yet difficult for machines to identify because the intended meaning often contradicts the literal wording. In this work, I study sarcasm detection using only classical machine learning methods and explicit feat...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, study, arxiv, context | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 80%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 05 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>model</category>
      <category>study</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>Beyond the Black Box: A Cognitive Architecture for Explainable and Aligned AI</title>
      <link>https://arxiv.org/abs/2512.03072</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.03072</guid>
      <description>arXiv:2512.03072v1 Announce Type: new 
Abstract: Current AI paradigms, as &amp;quot;architects of experience,&amp;quot; face fundamental challenges in explainability and value alignment. This paper introduces &amp;quot;Weight-Calculatism,&amp;quot; a novel cognitive architecture grounded in first principles, and demonstrates its poten...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; paper, arxiv, model, reasoning, alignment | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 80%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 05 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>paper</category>
      <category>arxiv</category>
      <category>model</category>
    </item>
    <item>
      <title>Multimodal Reinforcement Learning with Agentic Verifier for AI Agents</title>
      <link>https://arxiv.org/abs/2512.03438</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.03438</guid>
      <description>arXiv:2512.03438v1 Announce Type: new 
Abstract: Agentic reasoning models trained with multimodal reinforcement learning (MMRL) have become increasingly capable, yet they are almost universally optimized using sparse, outcome-based rewards computed based on the final answers. Richer rewards computed...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; RAG, paper, arxiv, model, reasoning | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 80%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 05 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>RAG</category>
      <category>paper</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths</title>
      <link>https://arxiv.org/abs/2512.03571</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.03571</guid>
      <description>arXiv:2512.03571v1 Announce Type: new 
Abstract: We introduce a new approach to agent programming, the development of LLM-based agents. Current approaches to agent programming often entangle two aspects of agent design: the core workflow logic and the inference-time strategy (e.g., tree search). We ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; framework, arxiv, LLM, model, experiment | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 80%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 05 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>framework</category>
      <category>arxiv</category>
      <category>LLM</category>
    </item>
    <item>
      <title>Mitigating the Curse of Detail: Scaling Arguments for Feature Learning and Sample Complexity</title>
      <link>https://arxiv.org/abs/2512.04165</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.04165</guid>
      <description>arXiv:2512.04165v1 Announce Type: new 
Abstract: Two pressing topics in the theory of deep learning are the interpretation of feature learning mechanisms and the determination of implicit bias of networks in the rich regime. Current theories of rich feature learning effects revolve around networks w...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; attention, arxiv, analysis | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 80%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 05 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>attention</category>
      <category>arxiv</category>
      <category>analysis</category>
    </item>
    <item>
      <title>BEP: A Binary Error Propagation Algorithm for Binary Neural Networks Training</title>
      <link>https://arxiv.org/abs/2512.04189</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.04189</guid>
      <description>arXiv:2512.04189v1 Announce Type: new 
Abstract: Binary Neural Networks (BNNs), which constrain both weights and activations to binary values, offer substantial reductions in computational complexity, memory footprint, and energy consumption. These advantages make them particularly well suited for d...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; paper, release, arxiv, memory, vector | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 80%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 05 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>paper</category>
      <category>release</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>Studying Various Activation Functions and Non-IID Data for Machine Learning Model Robustness</title>
      <link>https://arxiv.org/abs/2512.04264</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.04264</guid>
      <description>arXiv:2512.04264v1 Announce Type: new 
Abstract: Adversarial training is an effective method to improve the machine learning (ML) model robustness. Most existing studies typically consider the Rectified linear unit (ReLU) activation function and centralized training environments. In this paper, we s...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; paper, study, arxiv, model, experiment | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 80%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 05 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>paper</category>
      <category>study</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>MechDetect: Detecting Data-Dependent Errors</title>
      <link>https://arxiv.org/abs/2512.04138</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.04138</guid>
      <description>arXiv:2512.04138v1 Announce Type: new 
Abstract: Data quality monitoring is a core challenge in modern information processing systems. While many approaches to detect data errors or shifts have been proposed, few studies investigate the mechanisms governing error generation. We argue that knowing ho...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; experiment, study, arxiv, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 60%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 05 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>experiment</category>
      <category>study</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>Decoding Large Language Diffusion Models with Foreseeing Movement</title>
      <link>https://arxiv.org/abs/2512.04135</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.04135</guid>
      <description>arXiv:2512.04135v1 Announce Type: new 
Abstract: Large Language Diffusion Models (LLDMs) benefit from a flexible decoding mechanism that enables parallelized inference and controllable generations over autoregressive models. Yet such flexibility introduces a critical challenge: inference performance...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; experiment, arxiv, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 40%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 05 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>experiment</category>
      <category>arxiv</category>
      <category>model</category>
    </item>
  </channel>
</rss>