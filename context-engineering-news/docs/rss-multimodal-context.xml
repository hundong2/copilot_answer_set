<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Context Engineering Daily - Multimodal Context</title>
    <link>https://your-username.github.io/context-engineering-news#multimodal_context</link>
    <description>Latest Multimodal Context news in Context Engineering</description>
    <language>en-us</language>
    <item>
      <title>Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors</title>
      <link>https://arxiv.org/abs/2507.22367</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2507.22367</guid>
      <description>arXiv:2507.22367v1 Announce Type: new 
Abstract: Accurate and reliable personality assessment plays a vital role in many fields, such as emotional intelligence, mental health diagnostics, and personalized education. Unlike fleeting emotions, personality traits are stable, often subconsciously leaked...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; audio, prompt, experiment, multimodal, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 31 Jul 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Multimodal Context</category>
      <category>audio</category>
      <category>prompt</category>
      <category>experiment</category>
    </item>
    <item>
      <title>CIMR: Contextualized Iterative Multimodal Reasoning for Robust Instruction Following in LVLMs</title>
      <link>https://arxiv.org/abs/2507.22074</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2507.22074</guid>
      <description>arXiv:2507.22074v1 Announce Type: new 
Abstract: The rapid advancement of Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) has enhanced our ability to process and generate human language and visual information. However, these models often struggle with complex, multi-step multi-...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; framework, instruction, API, multimodal, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 31 Jul 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Multimodal Context</category>
      <category>framework</category>
      <category>instruction</category>
      <category>API</category>
    </item>
    <item>
      <title>Test-time Prompt Refinement for Text-to-Image Models</title>
      <link>https://arxiv.org/abs/2507.22076</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2507.22076</guid>
      <description>arXiv:2507.22076v1 Announce Type: new 
Abstract: Text-to-image (T2I) generation models have made significant strides but still struggle with prompt sensitivity: even minor changes in prompt wording can yield inconsistent or inaccurate outputs. To address this challenge, we introduce a closed-loop, t...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; prompt, alignment, multimodal, arxiv, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 31 Jul 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Multimodal Context</category>
      <category>prompt</category>
      <category>alignment</category>
      <category>multimodal</category>
    </item>
  </channel>
</rss>