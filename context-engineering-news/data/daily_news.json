{
  "generated_at": "2025-07-29T20:06:07.721777",
  "total_items": 48,
  "items": [
    {
      "title": "Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media",
      "url": "https://arxiv.org/abs/2507.19511",
      "description": "arXiv:2507.19511v1 Announce Type: new \nAbstract: The rising prevalence of mental health disorders necessitates the development of robust, automated tools for early detection and monitoring. Recent advances in Natural Language Processing (NLP), particularly transformer-based architectures, have demon...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "study",
        "embedding",
        "experiment",
        "analysis",
        "memory",
        "transformer",
        "augmented",
        "tool",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Setting The Table with Intent: Intent-aware Schema Generation and Editing for Literature Review Tables",
      "url": "https://arxiv.org/abs/2507.19521",
      "description": "arXiv:2507.19521v1 Announce Type: new \nAbstract: The increasing volume of academic literature makes it essential for researchers to organize, compare, and contrast collections of documents. Large language models (LLMs) can support this process by generating schemas defining shared aspects along whic...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "research",
        "LLM",
        "study",
        "prompt",
        "large language model",
        "paper",
        "arxiv",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Mind the Language Gap in Digital Humanities: LLM-Aided Translation of SKOS Thesauri",
      "url": "https://arxiv.org/abs/2507.19537",
      "description": "arXiv:2507.19537v1 Announce Type: new \nAbstract: We introduce WOKIE, an open-source, modular, and ready-to-use pipeline for the automated translation of SKOS thesauri. This work addresses a critical need in the Digital Humanities (DH), where language diversity can limit access, reuse, and semantic i...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "research",
        "LLM",
        "large language model",
        "arxiv",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning",
      "url": "https://arxiv.org/abs/2507.19586",
      "description": "arXiv:2507.19586v1 Announce Type: new \nAbstract: Large language models (LLMs) possess extensive world knowledge, including geospatial knowledge, which has been successfully applied to various geospatial tasks such as mobility prediction and social indicator prediction. However, LLMs often generate i...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "RAG",
        "framework",
        "large language model",
        "experiment",
        "reasoning",
        "arxiv",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Efficient Attention Mechanisms for Large Language Models: A Survey",
      "url": "https://arxiv.org/abs/2507.19595",
      "description": "arXiv:2507.19595v1 Announce Type: new \nAbstract: Transformer-based architectures have become the prevailing backbone of large language models. However, the quadratic time and memory complexity of self-attention remains a fundamental obstacle to efficient long-context modeling. To address this limita...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "context_management",
      "keywords": [
        "attention",
        "research",
        "arxiv",
        "context",
        "RAG",
        "large language model",
        "memory",
        "transformer",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?",
      "url": "https://arxiv.org/abs/2507.19598",
      "description": "arXiv:2507.19598v1 Announce Type: new \nAbstract: Recent advancements in Large Language Models (LLMs) have significantly enhanced their code generation capabilities. However, their robustness against adversarial misuse, particularly through multi-turn malicious coding prompts, remains underexplored. ...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "fine-tuning",
        "LLM",
        "prompt",
        "large language model",
        "arxiv",
        "vision",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic Track",
      "url": "https://arxiv.org/abs/2507.19616",
      "description": "arXiv:2507.19616v1 Announce Type: new \nAbstract: This paper presents HITSZ's submission for the IWSLT 2025 Indic track, focusing on speech-to-text translation (ST) for English-to-Indic and Indic-to-English language pairs. To enhance translation quality in this low-resource scenario, we propose an en...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "chain-of-thought",
        "RAG",
        "experiment",
        "large language model",
        "paper",
        "CoT",
        "arxiv",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks",
      "url": "https://arxiv.org/abs/2507.19634",
      "description": "arXiv:2507.19634v1 Announce Type: new \nAbstract: Recent advances in large language models have catalyzed the development of multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to general-purpose in...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "multimodal_context",
      "keywords": [
        "research",
        "LLM",
        "multimodal",
        "context",
        "RAG",
        "framework",
        "large language model",
        "release",
        "instruction",
        "arxiv",
        "vision",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams",
      "url": "https://arxiv.org/abs/2507.19666",
      "description": "arXiv:2507.19666v1 Announce Type: new \nAbstract: The intersection of AI and legal systems presents a growing need for tools that support legal education, particularly in under-resourced languages such as Romanian. In this work, we aim to evaluate the capabilities of Large Language Models (LLMs) and ...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "fine-tuning",
        "LLM",
        "multimodal",
        "chain-of-thought",
        "prompt",
        "RAG",
        "arxiv",
        "large language model",
        "experiment",
        "tool",
        "reasoning",
        "prompting",
        "retrieval",
        "augmented",
        "vision",
        "image",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks",
      "url": "https://arxiv.org/abs/2507.19699",
      "description": "arXiv:2507.19699v1 Announce Type: new \nAbstract: Although LLMs have attained significant success in high-resource languages, their capacity in low-resource linguistic environments like Kannada and Arabic is not yet fully understood. This work benchmarking the performance of multilingual and monoling...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "compression",
        "LLM",
        "arxiv",
        "RAG",
        "large language model",
        "model",
        "transformer",
        "GPT"
      ],
      "score": 1.0
    },
    {
      "title": "Agent WARPP: Workflow Adherence via Runtime Parallel Personalization",
      "url": "https://arxiv.org/abs/2507.19543",
      "description": "arXiv:2507.19543v1 Announce Type: new \nAbstract: Large language models (LLMs) are increasingly applied in task-oriented dialogue (TOD) systems but often struggle with long, conditional workflows that involve external tool calls and depend on user-specific information. We present Workflow Adherence v...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "LLM",
        "RAG",
        "framework",
        "large language model",
        "reasoning",
        "arxiv",
        "tool",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Hypergames: Modeling Misaligned Perceptions and Nested Beliefs for Multi-agent Systems",
      "url": "https://arxiv.org/abs/2507.19593",
      "description": "arXiv:2507.19593v1 Announce Type: new \nAbstract: Classical game-theoretic models typically assume rational agents, complete information, and common knowledge of payoffs - assumptions that are often violated in real-world MAS characterized by uncertainty, misaligned perceptions, and nested beliefs. T...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "research",
        "alignment",
        "context",
        "framework",
        "analysis",
        "reasoning",
        "arxiv",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference",
      "url": "https://arxiv.org/abs/2507.19608",
      "description": "arXiv:2507.19608v1 Announce Type: new \nAbstract: Deploying Large Language Models (LLMs) on edge devices remains challenging due to their quadratically increasing computations with the sequence length. Existing studies for dynamic attention pruning are designed for hardware with massively parallel co...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "context_management",
      "keywords": [
        "attention",
        "fine-tuning",
        "LLM",
        "context window",
        "context",
        "framework",
        "large language model",
        "memory",
        "arxiv",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges",
      "url": "https://arxiv.org/abs/2507.19672",
      "description": "arXiv:2507.19672v1 Announce Type: new \nAbstract: Due to the remarkable capabilities and growing impact of large language models (LLMs), they have been deeply integrated into many aspects of society. Thus, ensuring their alignment with human values and intentions has emerged as a critical challenge. ...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "fine-tuning",
        "research",
        "alignment",
        "LLM",
        "framework",
        "large language model",
        "analysis",
        "instruction",
        "arxiv",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "The wall confronting large language models",
      "url": "https://arxiv.org/abs/2507.19703",
      "description": "arXiv:2507.19703v1 Announce Type: new \nAbstract: We show that the scaling laws which determine the performance of large language models (LLMs) severely limit their ability to improve the uncertainty of their predictions. As a result, raising their reliability to meet the standards of scientific inqu...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "research",
        "LLM",
        "API",
        "large language model",
        "paper",
        "arxiv",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "HypKG: Hypergraph-based Knowledge Graph Contextualization for Precision Healthcare",
      "url": "https://arxiv.org/abs/2507.19726",
      "description": "arXiv:2507.19726v1 Announce Type: new \nAbstract: Knowledge graphs (KGs) are important products of the semantic web, which are widely used in various application domains. Healthcare is one of such domains where KGs are intensively used, due to the high requirement for knowledge accuracy and interconn...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "context",
        "RAG",
        "framework",
        "experiment",
        "paper",
        "transformer",
        "product",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)",
      "url": "https://arxiv.org/abs/2507.19749",
      "description": "arXiv:2507.19749v1 Announce Type: new \nAbstract: Answer Set Programming (ASP) is a powerful paradigm for non-monotonic reasoning. Recently, large language models (LLMs) have demonstrated promising capabilities in logical reasoning. Despite this potential, current evaluations of LLM capabilities in A...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "study",
        "LLM",
        "large language model",
        "reasoning",
        "arxiv",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Beyond 9-to-5: A Generative Model for Augmenting Mobility Data of Underrepresented Shift Workers",
      "url": "https://arxiv.org/abs/2507.19510",
      "description": "arXiv:2507.19510v1 Announce Type: new \nAbstract: This paper addresses a critical gap in urban mobility modeling by focusing on shift workers, a population segment comprising 15-20% of the workforce in industrialized societies yet systematically underrepresented in traditional transportation surveys ...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "alignment",
        "arxiv",
        "study",
        "RAG",
        "embedding",
        "analysis",
        "paper",
        "transformer",
        "tool",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Wavelet Logic Machines: Learning and Reasoning in the Spectral Domain Without Neural Networks",
      "url": "https://arxiv.org/abs/2507.19514",
      "description": "arXiv:2507.19514v1 Announce Type: new \nAbstract: We introduce a fully spectral learning framework that eliminates traditional neural layers by operating entirely in the wavelet domain. The model applies learnable nonlinear transformations, including soft-thresholding and gain-phase modulation, direc...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "context_management",
      "keywords": [
        "attention",
        "arxiv",
        "framework",
        "reasoning",
        "memory",
        "transformer",
        "vision",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "A Comparative Analysis of Traditional and Deep Learning Time Series Architectures for Influenza A Infectious Disease Forecasting",
      "url": "https://arxiv.org/abs/2507.19515",
      "description": "arXiv:2507.19515v1 Announce Type: new \nAbstract: Influenza A is responsible for 290,000 to 650,000 respiratory deaths a year, though this estimate is an improvement from years past due to improvements in sanitation, healthcare practices, and vaccination programs. In this study, we perform a comparat...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "study",
        "RAG",
        "analysis",
        "transformer",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "BikeVAE-GNN: A Variational Autoencoder-Augmented Hybrid Graph Neural Network for Sparse Bicycle Volume Estimation",
      "url": "https://arxiv.org/abs/2507.19517",
      "description": "arXiv:2507.19517v1 Announce Type: new \nAbstract: Accurate link-level bicycle volume estimation is essential for informed urban and transport planning but it is challenged by extremely sparse count data in urban bicycling networks worldwide. We propose BikeVAE-GNN, a novel dual-task framework augment...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "attention",
        "research",
        "RAG",
        "framework",
        "experiment",
        "arxiv",
        "ICL",
        "augmented",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Target Circuit Matching in Large-Scale Netlists using GNN-Based Region Prediction",
      "url": "https://arxiv.org/abs/2507.19518",
      "description": "arXiv:2507.19518v1 Announce Type: new \nAbstract: Subgraph matching plays an important role in electronic design automation (EDA) and circuit verification. Traditional rule-based methods have limitations in generalizing to arbitrary target circuits. Furthermore, node-to-node matching approaches tend ...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "embedding",
        "experiment",
        "paper",
        "arxiv",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Physics-informed transfer learning for SHM via feature selection",
      "url": "https://arxiv.org/abs/2507.19519",
      "description": "arXiv:2507.19519v1 Announce Type: new \nAbstract: Data used for training structural health monitoring (SHM) systems are expensive and often impractical to obtain, particularly labelled data. Population-based SHM presents a potential solution to this issue by considering the available data across a po...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "RAG",
        "paper",
        "arxiv",
        "experiment"
      ],
      "score": 1.0
    },
    {
      "title": "Exoplanet Detection Using Machine Learning Models Trained on Synthetic Light Curves",
      "url": "https://arxiv.org/abs/2507.19520",
      "description": "arXiv:2507.19520v1 Announce Type: new \nAbstract: With manual searching processes, the rate at which scientists and astronomers discover exoplanets is slow because of inefficiencies that require an extensive time of laborious inspections. In fact, as of now there have been about only 5,000 confirmed ...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "study",
        "context",
        "paper",
        "arxiv",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Language Models for Controllable DNA Sequence Design",
      "url": "https://arxiv.org/abs/2507.19523",
      "description": "arXiv:2507.19523v1 Announce Type: new \nAbstract: We consider controllable DNA sequence design, where sequences are generated by conditioning on specific biological properties. While language models (LMs) such as GPT and BERT have achieved remarkable success in natural language generation, their appl...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "RAG",
        "experiment",
        "model",
        "release",
        "cross-modal",
        "transformer",
        "GPT"
      ],
      "score": 1.0
    },
    {
      "title": "Context-Engineering - \"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" — Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "url": "https://github.com/davidkimai/Context-Engineering",
      "description": "\"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" — Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "published_date": "2025-06-29T00:16:36+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "prompt engineering",
        "context window",
        "prompt",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "ThinkSound - PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "url": "https://github.com/FunAudioLLM/ThinkSound",
      "description": "PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "published_date": "2025-06-27T02:27:00+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "chain-of-thought",
        "framework",
        "audio",
        "reasoning",
        "CoT"
      ],
      "score": 1.0
    },
    {
      "title": "Cline-Recursive-Chain-of-Thought-System-CRCT- - A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "url": "https://github.com/RPG-fan/Cline-Recursive-Chain-of-Thought-System-CRCT-",
      "description": "A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "published_date": "2025-02-18T15:45:30+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "framework",
        "chain-of-thought",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "MoBA - MoBA: Mixture of Block Attention for Long-Context LLMs",
      "url": "https://github.com/MoonshotAI/MoBA",
      "description": "MoBA: Mixture of Block Attention for Long-Context LLMs",
      "published_date": "2025-02-17T13:27:30+00:00",
      "source": "GitHub",
      "category": "context_management",
      "keywords": [
        "attention",
        "LLM",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "LightRAG - \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "url": "https://github.com/HKUDS/LightRAG",
      "description": "\"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "published_date": "2024-10-02T11:57:54+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "retrieval",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "KAG - KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "url": "https://github.com/OpenSPG/KAG",
      "description": "KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "published_date": "2024-09-21T13:56:44+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "LLM",
        "RAG",
        "framework",
        "reasoning",
        "knowledge base",
        "vector",
        "retrieval",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Kiln - The easiest tool for fine-tuning LLM models, synthetic data generation, and collaborating on datasets.",
      "url": "https://github.com/Kiln-AI/Kiln",
      "description": "The easiest tool for fine-tuning LLM models, synthetic data generation, and collaborating on datasets.",
      "published_date": "2024-07-23T23:10:13+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "fine-tuning",
        "tool",
        "LLM",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "graphrag - A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "url": "https://github.com/microsoft/graphrag",
      "description": "A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "published_date": "2024-03-27T17:57:52+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "retrieval",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "R2R - SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "url": "https://github.com/SciPhi-AI/R2R",
      "description": "SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "published_date": "2024-02-12T03:24:27+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "API",
        "retrieval",
        "augmented",
        "product"
      ],
      "score": 1.0
    },
    {
      "title": "openlit - Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. 🚀💻 Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "url": "https://github.com/openlit/openlit",
      "description": "Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. 🚀💻 Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "published_date": "2024-01-23T17:40:59+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "LLM",
        "prompt",
        "framework",
        "platform",
        "vector"
      ],
      "score": 1.0
    },
    {
      "title": "AutoRAG - AutoRAG: An Open-Source Framework for Retrieval-Augmented Generation (RAG) Evaluation & Optimization with AutoML-Style Automation",
      "url": "https://github.com/Marker-Inc-Korea/AutoRAG",
      "description": "AutoRAG: An Open-Source Framework for Retrieval-Augmented Generation (RAG) Evaluation & Optimization with AutoML-Style Automation",
      "published_date": "2024-01-10T12:25:00+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "retrieval",
        "augmented",
        "framework"
      ],
      "score": 1.0
    },
    {
      "title": "Integrating Activity Predictions in Knowledge Graphs",
      "url": "https://arxiv.org/abs/2507.19733",
      "description": "arXiv:2507.19733v1 Announce Type: new \nAbstract: We argue that ontology-structured knowledge graphs can play a crucial role in generating predictions about future events. By leveraging the semantic framework provided by Basic Formal Ontology (BFO) and Common Core Ontologies (CCO), we demonstrate how...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "RAG",
        "framework",
        "analysis",
        "arxiv",
        "model"
      ],
      "score": 0.8
    },
    {
      "title": "Enhancing Spatiotemporal Networks with xLSTM: A Scalar LSTM Approach for Cellular Traffic Forecasting",
      "url": "https://arxiv.org/abs/2507.19513",
      "description": "arXiv:2507.19513v1 Announce Type: new \nAbstract: Accurate spatiotemporal traffic forecasting is vital for intelligent resource management in 5G and beyond. However, conventional AI approaches often fail to capture the intricate spatial and temporal patterns that exist, due to e.g., the mobility of u...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "RAG",
        "arxiv",
        "experiment",
        "model"
      ],
      "score": 0.8
    },
    {
      "title": "fastapi_mcp - Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "url": "https://github.com/tadata-org/fastapi_mcp",
      "description": "Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "published_date": "2025-03-08T11:15:43+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "API",
        "model",
        "tool",
        "context"
      ],
      "score": 0.8
    },
    {
      "title": "cosmos-reason1 - Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "url": "https://github.com/nvidia-cosmos/cosmos-reason1",
      "description": "Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "published_date": "2025-03-02T15:23:55+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "model",
        "chain-of-thought",
        "reasoning"
      ],
      "score": 0.8
    },
    {
      "title": "MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation",
      "url": "https://arxiv.org/abs/2507.19489",
      "description": "arXiv:2507.19489v1 Announce Type: new \nAbstract: The integration of Artificial Intelligence (AI) into clinical workflows requires robust collaborative platforms that are able to bridge the gap between technical innovation and practical healthcare applications. This paper introduces MAIA (Medical Art...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "research",
        "platform",
        "paper",
        "arxiv",
        "tool",
        "model"
      ],
      "score": 0.6
    },
    {
      "title": "Applications and Manipulations of Physics-Informed Neural Networks in Solving Differential Equations",
      "url": "https://arxiv.org/abs/2507.19522",
      "description": "arXiv:2507.19522v1 Announce Type: new \nAbstract: Mathematical models in neural networks are powerful tools for solving complex differential equations and optimizing their parameters; that is, solving the forward and inverse problems, respectively. A forward problem predicts the output of a network f...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "research",
        "library",
        "paper",
        "arxiv",
        "tool",
        "model"
      ],
      "score": 0.6
    },
    {
      "title": "excel-mcp-server - A Model Context Protocol server for Excel file manipulation",
      "url": "https://github.com/haris-musa/excel-mcp-server",
      "description": "A Model Context Protocol server for Excel file manipulation",
      "published_date": "2025-02-12T06:39:48+00:00",
      "source": "GitHub",
      "category": "industry_news",
      "keywords": [
        "model",
        "context"
      ],
      "score": 0.6
    },
    {
      "title": "mcp-agent - Build effective agents using Model Context Protocol and simple workflow patterns",
      "url": "https://github.com/lastmile-ai/mcp-agent",
      "description": "Build effective agents using Model Context Protocol and simple workflow patterns",
      "published_date": "2024-12-18T01:55:10+00:00",
      "source": "GitHub",
      "category": "industry_news",
      "keywords": [
        "model",
        "context"
      ],
      "score": 0.6
    },
    {
      "title": "AlphaCodium - Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "url": "https://github.com/Codium-ai/AlphaCodium",
      "description": "Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "published_date": "2024-01-14T15:17:18+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "prompt engineering",
        "paper",
        "prompt"
      ],
      "score": 0.6
    },
    {
      "title": "Minding Motivation: The Effect of Intrinsic Motivation on Agent Behaviors",
      "url": "https://arxiv.org/abs/2507.19725",
      "description": "arXiv:2507.19725v1 Announce Type: new \nAbstract: Games are challenging for Reinforcement Learning~(RL) agents due to their reward-sparsity, as rewards are only obtainable after long sequences of deliberate actions. Intrinsic Motivation~(IM) methods -- which introduce exploration rewards -- are an ef...",
      "published_date": "2025-07-29T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "study",
        "model"
      ],
      "score": 0.4
    },
    {
      "title": "optillm - Optimizing inference proxy for LLMs",
      "url": "https://github.com/codelion/optillm",
      "description": "Optimizing inference proxy for LLMs",
      "published_date": "2024-08-22T19:46:07+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "LLM"
      ],
      "score": 0.4
    },
    {
      "title": "Introducing Trackio: A Lightweight Experiment Tracking Library from Hugging Face",
      "url": "https://huggingface.co/blog/trackio",
      "description": "...",
      "published_date": "2025-07-29T00:00:00",
      "source": "Hugging Face Blog",
      "category": "tools_frameworks",
      "keywords": [
        "experiment",
        "library"
      ],
      "score": 0.2
    }
  ]
}