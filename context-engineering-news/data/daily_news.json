{
  "generated_at": "2025-09-25T20:05:57.547665",
  "total_items": 45,
  "items": [
    {
      "title": "Automated Item Neutralization for Non-Cognitive Scales: A Large Language Model Approach to Reducing Social-Desirability Bias",
      "url": "https://arxiv.org/abs/2509.19314",
      "description": "arXiv:2509.19314v1 Announce Type: new \nAbstract: This study evaluates item neutralization assisted by the large language model (LLM) to reduce social desirability bias in personality assessment. GPT-o3 was used to rewrite the International Personality Item Pool Big Five Measure (IPIP-BFM-50), and 20...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "study",
        "model",
        "large language model",
        "LLM",
        "GPT"
      ],
      "score": 1.0
    },
    {
      "title": "FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering",
      "url": "https://arxiv.org/abs/2509.19319",
      "description": "arXiv:2509.19319v1 Announce Type: new \nAbstract: The recent shift toward the Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR) standard opens a new frontier for clinical AI, demanding LLM agents to navigate complex, resource-based data models instead of conventional structured...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "tool",
        "ICL",
        "arxiv",
        "framework",
        "model",
        "reasoning",
        "retrieval",
        "release",
        "LLM",
        "experiment",
        "research",
        "API"
      ],
      "score": 1.0
    },
    {
      "title": "Readme_AI: Dynamic Context Construction for Large Language Models",
      "url": "https://arxiv.org/abs/2509.19322",
      "description": "arXiv:2509.19322v1 Announce Type: new \nAbstract: Despite being trained on significant amounts of data, Large Language Models (LLMs) can provide inaccurate or unreliable information in the context of a user's specific query. Given query-specific context significantly improves the usefulness of its re...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "tool",
        "arxiv",
        "model",
        "reasoning",
        "library",
        "large language model",
        "paper",
        "LLM",
        "example",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "Magnitude Matters: a Superior Class of Similarity Metrics for Holistic Semantic Understanding",
      "url": "https://arxiv.org/abs/2509.19323",
      "description": "arXiv:2509.19323v1 Announce Type: new \nAbstract: Vector comparison in high dimensions is a fundamental task in NLP, yet it is dominated by two baselines: the raw dot product, which is unbounded and sensitive to vector norms, and the cosine similarity, which discards magnitude information entirely. T...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "product",
        "vector",
        "model",
        "embedding",
        "paper",
        "alignment"
      ],
      "score": 1.0
    },
    {
      "title": "How Much of Your Data Can Suck? Thresholds for Domain Performance and Emergent Misalignment in LLMs",
      "url": "https://arxiv.org/abs/2509.19325",
      "description": "arXiv:2509.19325v1 Announce Type: new \nAbstract: This paper investigates the impact of incorrect data on the performance and safety of large language models (LLMs), specifically gpt-4o, during supervised fine-tuning (SFT). Although LLMs become increasingly vital across broad domains like finance, co...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "model",
        "RAG",
        "fine-tuning",
        "large language model",
        "paper",
        "LLM",
        "research",
        "alignment",
        "GPT"
      ],
      "score": 1.0
    },
    {
      "title": "Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers",
      "url": "https://arxiv.org/abs/2509.19326",
      "description": "arXiv:2509.19326v1 Announce Type: new \nAbstract: The surge in scientific submissions has placed increasing strain on the traditional peer-review process, prompting the exploration of large language models (LLMs) for automated review generation. While LLMs demonstrate competence in producing structur...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "tool",
        "ICL",
        "arxiv",
        "prompting",
        "framework",
        "model",
        "reasoning",
        "analysis",
        "large language model",
        "paper",
        "LLM",
        "prompt",
        "example",
        "context",
        "GPT"
      ],
      "score": 1.0
    },
    {
      "title": "A systematic review of trial-matching pipelines using large language models",
      "url": "https://arxiv.org/abs/2509.19327",
      "description": "arXiv:2509.19327v1 Announce Type: new \nAbstract: Matching patients to clinical trial options is critical for identifying novel treatments, especially in oncology. However, manual matching is labor-intensive and error-prone, leading to recruitment delays. Pipelines incorporating large language models...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "zero-shot",
        "ICL",
        "arxiv",
        "study",
        "prompting",
        "model",
        "retrieval",
        "attention",
        "fine-tuning",
        "large language model",
        "LLM",
        "prompt",
        "GPT"
      ],
      "score": 1.0
    },
    {
      "title": "How Model Size, Temperature, and Prompt Style Affect LLM-Human Assessment Score Alignment",
      "url": "https://arxiv.org/abs/2509.19329",
      "description": "arXiv:2509.19329v1 Announce Type: new \nAbstract: We examined how model size, temperature, and prompt style affect Large Language Models' (LLMs) alignment within itself, between models, and with human in assessing clinical reasoning skills. Model size emerged as a key factor in LLM-human score alignm...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "study",
        "model",
        "reasoning",
        "large language model",
        "LLM",
        "prompt",
        "alignment"
      ],
      "score": 1.0
    },
    {
      "title": "Quantifying Compositionality of Classic and State-of-the-Art Embeddings",
      "url": "https://arxiv.org/abs/2509.19332",
      "description": "arXiv:2509.19332v1 Announce Type: new \nAbstract: For language models to generalize correctly to novel expressions, it is critical that they exploit access compositional meanings when this is justified. Even if we don't know what a \"pelp\" is, we can use our knowledge of numbers to understand that \"te...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "arxiv",
        "model",
        "analysis",
        "embedding",
        "retrieval",
        "transformer",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "Pluralistic Off-policy Evaluation and Alignment",
      "url": "https://arxiv.org/abs/2509.19333",
      "description": "arXiv:2509.19333v1 Announce Type: new \nAbstract: Personalized preference alignment for LLMs with diverse human preferences requires evaluation and alignment methods that capture pluralism. Most existing preference alignment datasets are logged under policies that differ substantially from the evalua...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "arxiv",
        "framework",
        "model",
        "RAG",
        "LLM",
        "alignment"
      ],
      "score": 1.0
    },
    {
      "title": "The Indispensable Role of User Simulation in the Pursuit of AGI",
      "url": "https://arxiv.org/abs/2509.19456",
      "description": "arXiv:2509.19456v1 Announce Type: new \nAbstract: Progress toward Artificial General Intelligence (AGI) faces significant bottlenecks, particularly in rigorously evaluating complex interactive systems and acquiring the vast interaction data needed for training adaptive agents. This paper posits that ...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "tool",
        "ICL",
        "arxiv",
        "model",
        "large language model",
        "paper",
        "research"
      ],
      "score": 1.0
    },
    {
      "title": "Estimating the Self-Consistency of LLMs",
      "url": "https://arxiv.org/abs/2509.19489",
      "description": "arXiv:2509.19489v1 Announce Type: new \nAbstract: Systems often repeat the same prompt to large language models (LLMs) and aggregate responses to improve reliability. This short note analyzes an estimator of the self-consistency of LLMs and the tradeoffs it induces under a fixed compute budget $B=mn$...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "model",
        "analysis",
        "large language model",
        "LLM",
        "prompt"
      ],
      "score": 1.0
    },
    {
      "title": "Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning",
      "url": "https://arxiv.org/abs/2509.19517",
      "description": "arXiv:2509.19517v1 Announce Type: new \nAbstract: The scaling of Large Language Models (LLMs) has exposed a critical gap between their performance on static benchmarks and their fragility in dynamic, information-rich environments. While models excel at isolated tasks, the computational limits that go...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "study",
        "model",
        "reasoning",
        "RAG",
        "context",
        "attention",
        "large language model",
        "LLM",
        "instruction"
      ],
      "score": 1.0
    },
    {
      "title": "SteinerSQL: Graph-Guided Mathematical Reasoning for Text-to-SQL Generation",
      "url": "https://arxiv.org/abs/2509.19623",
      "description": "arXiv:2509.19623v1 Announce Type: new \nAbstract: Large Language Models (LLMs) struggle with complex Text-to-SQL queries that demand both sophisticated mathematical reasoning and intricate schema navigation. Existing methods often tackle these challenges in isolation, creating a fractured reasoning p...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "chain_of_thought",
      "keywords": [
        "arxiv",
        "framework",
        "model",
        "reasoning",
        "large language model",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "UserRL: Training Interactive User-Centric Agent via Reinforcement Learning",
      "url": "https://arxiv.org/abs/2509.19736",
      "description": "arXiv:2509.19736v1 Announce Type: new \nAbstract: Reinforcement learning (RL) has shown promise in training agentic models that move beyond static benchmarks to engage in dynamic, multi-turn interactions. Yet, the ultimate value of such agents lies in their ability to assist users, a setting where di...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "framework",
        "model",
        "experiment",
        "research",
        "API",
        "GPT"
      ],
      "score": 1.0
    },
    {
      "title": "Wavelet Fourier Diffuser: Frequency-Aware Diffusion Model for Reinforcement Learning",
      "url": "https://arxiv.org/abs/2509.19305",
      "description": "arXiv:2509.19305v1 Announce Type: new \nAbstract: Diffusion probability models have shown significant promise in offline reinforcement learning by directly modeling trajectory sequences. However, existing approaches primarily focus on time-domain features while overlooking frequency-domain features, ...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "framework",
        "model",
        "attention",
        "paper",
        "experiment"
      ],
      "score": 1.0
    },
    {
      "title": "Anti-Money Laundering Systems Using Deep Learning",
      "url": "https://arxiv.org/abs/2509.19359",
      "description": "arXiv:2509.19359v1 Announce Type: new \nAbstract: In this paper, we focused on using deep learning methods for detecting money laundering in financial transaction networks, in order to demonstrate that it can be used as a complement or instead of the more commonly used rule-based systems and conventi...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "model",
        "analysis",
        "paper",
        "API",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "DeepACTIF: Efficient Feature Attribution via Activation Traces in Neural Sequence Models",
      "url": "https://arxiv.org/abs/2509.19362",
      "description": "arXiv:2509.19362v1 Announce Type: new \nAbstract: Feature attribution is essential for interpreting deep learning models, particularly in time-series domains such as healthcare, biometrics, and human-AI interaction. However, standard attribution methods, such as Integrated Gradients or SHAP, are comp...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "model",
        "analysis",
        "RAG",
        "memory",
        "experiment"
      ],
      "score": 1.0
    },
    {
      "title": "Analyzing the Impact of Credit Card Fraud on Economic Fluctuations of American Households Using an Adaptive Neuro-Fuzzy Inference System",
      "url": "https://arxiv.org/abs/2509.19363",
      "description": "arXiv:2509.19363v1 Announce Type: new \nAbstract: Credit card fraud is assuming growing proportions as a major threat to the financial position of American household, leading to unpredictable changes in household economic behavior. To solve this problem, in this paper, a new hybrid analysis method is...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "framework",
        "model",
        "analysis",
        "attention",
        "library",
        "paper",
        "experiment"
      ],
      "score": 1.0
    },
    {
      "title": "Unsupervised Outlier Detection in Audit Analytics: A Case Study Using USA Spending Data",
      "url": "https://arxiv.org/abs/2509.19366",
      "description": "arXiv:2509.19366v1 Announce Type: new \nAbstract: This study investigates the effectiveness of unsupervised outlier detection methods in audit analytics, utilizing USA spending data from the U.S. Department of Health and Human Services (DHHS) as a case example. We employ and compare multiple outlier ...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "study",
        "model",
        "analysis",
        "RAG",
        "research",
        "example"
      ],
      "score": 1.0
    },
    {
      "title": "Uncertainty Quantification of Large Language Models using Approximate Bayesian Computation",
      "url": "https://arxiv.org/abs/2509.19375",
      "description": "arXiv:2509.19375v1 Announce Type: new \nAbstract: Despite their widespread applications, Large Language Models (LLMs) often struggle to express uncertainty, posing a challenge for reliable deployment in high stakes and safety critical domains like clinical diagnostics. Existing standard baseline meth...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "in_context_learning",
      "keywords": [
        "arxiv",
        "ICL",
        "model",
        "large language model",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "Learning from Observation: A Survey of Recent Advances",
      "url": "https://arxiv.org/abs/2509.19379",
      "description": "arXiv:2509.19379v1 Announce Type: new \nAbstract: Imitation Learning (IL) algorithms offer an efficient way to train an agent by mimicking an expert's behavior without requiring a reward function. IL algorithms often necessitate access to state and action information from expert demonstrations. Altho...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "framework",
        "model",
        "attention",
        "paper",
        "research",
        "demonstration"
      ],
      "score": 1.0
    },
    {
      "title": "TensLoRA: Tensor Alternatives for Low-Rank Adaptation",
      "url": "https://arxiv.org/abs/2509.19391",
      "description": "arXiv:2509.19391v1 Announce Type: new \nAbstract: Low-Rank Adaptation (LoRA) is widely used to efficiently adapt Transformers by adding trainable low-rank matrices to attention projections. While effective, these matrices are considered independent for each attention projection (Query, Key, and Value...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "context_management",
      "keywords": [
        "arxiv",
        "framework",
        "model",
        "vision",
        "attention",
        "transformer",
        "experiment",
        "compression"
      ],
      "score": 1.0
    },
    {
      "title": "Context-Engineering - \"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" â€” Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "url": "https://github.com/davidkimai/Context-Engineering",
      "description": "\"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" â€” Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "published_date": "2025-06-29T00:16:36+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "context window",
        "prompt engineering",
        "context",
        "prompt"
      ],
      "score": 1.0
    },
    {
      "title": "ThinkSound - [NeurIPS 2025] PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "url": "https://github.com/FunAudioLLM/ThinkSound",
      "description": "[NeurIPS 2025] PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "published_date": "2025-06-27T02:27:00+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "framework",
        "CoT",
        "reasoning",
        "chain-of-thought",
        "audio"
      ],
      "score": 1.0
    },
    {
      "title": "mcp-context-forge - A Model Context Protocol (MCP) Gateway & Registry. Serves as a central management point for tools, resources, and prompts that can be accessed by MCP-compatible LLM applications. Converts REST API endpoints to MCP, composes virtual MCP servers with added security and observability, and converts between protocols (stdio, SSE, Streamable HTTP).",
      "url": "https://github.com/IBM/mcp-context-forge",
      "description": "A Model Context Protocol (MCP) Gateway & Registry. Serves as a central management point for tools, resources, and prompts that can be accessed by MCP-compatible LLM applications. Converts REST API endpoints to MCP, composes virtual MCP servers with added security and observability, and converts between protocols (stdio, SSE, Streamable HTTP).",
      "published_date": "2025-05-08T08:16:59+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "tool",
        "model",
        "LLM",
        "prompt",
        "API",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "Cline-Recursive-Chain-of-Thought-System-CRCT- - A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "url": "https://github.com/RPG-fan/Cline-Recursive-Chain-of-Thought-System-CRCT-",
      "description": "A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "published_date": "2025-02-18T15:45:30+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "chain-of-thought",
        "framework",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "LightRAG - [EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "url": "https://github.com/HKUDS/LightRAG",
      "description": "[EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "published_date": "2024-10-02T11:57:54+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "augmented",
        "retrieval",
        "RAG"
      ],
      "score": 1.0
    },
    {
      "title": "KAG - KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "url": "https://github.com/OpenSPG/KAG",
      "description": "KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "published_date": "2024-09-21T13:56:44+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "knowledge base",
        "framework",
        "vector",
        "model",
        "reasoning",
        "retrieval",
        "RAG",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "Kiln - The easiest tool for fine-tuning LLM models, synthetic data generation, and collaborating on datasets.",
      "url": "https://github.com/Kiln-AI/Kiln",
      "description": "The easiest tool for fine-tuning LLM models, synthetic data generation, and collaborating on datasets.",
      "published_date": "2024-07-23T23:10:13+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "fine-tuning",
        "model",
        "tool",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "graphrag - A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "url": "https://github.com/microsoft/graphrag",
      "description": "A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "published_date": "2024-03-27T17:57:52+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "augmented",
        "retrieval",
        "RAG"
      ],
      "score": 1.0
    },
    {
      "title": "R2R - SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "url": "https://github.com/SciPhi-AI/R2R",
      "description": "SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "published_date": "2024-02-12T03:24:27+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "product",
        "augmented",
        "retrieval",
        "RAG",
        "API"
      ],
      "score": 1.0
    },
    {
      "title": "openlit - Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. ðŸš€ðŸ’» Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "url": "https://github.com/openlit/openlit",
      "description": "Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. ðŸš€ðŸ’» Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "published_date": "2024-01-23T17:40:59+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "framework",
        "vector",
        "LLM",
        "prompt",
        "platform"
      ],
      "score": 1.0
    },
    {
      "title": "AutoRAG - AutoRAG: An Open-Source Framework for Retrieval-Augmented Generation (RAG) Evaluation & Optimization with AutoML-Style Automation",
      "url": "https://github.com/Marker-Inc-Korea/AutoRAG",
      "description": "AutoRAG: An Open-Source Framework for Retrieval-Augmented Generation (RAG) Evaluation & Optimization with AutoML-Style Automation",
      "published_date": "2024-01-10T12:25:00+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "augmented",
        "framework",
        "retrieval",
        "RAG"
      ],
      "score": 1.0
    },
    {
      "title": "Score the Steps, Not Just the Goal: VLM-Based Subgoal Evaluation for Robotic Manipulation",
      "url": "https://arxiv.org/abs/2509.19524",
      "description": "arXiv:2509.19524v1 Announce Type: new \nAbstract: Robot learning papers typically report a single binary success rate (SR), which obscures where a policy succeeds or fails along a multi-step manipulation task. We argue that subgoal-level reporting should become routine: for each trajectory, a vector ...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "multimodal_context",
      "keywords": [
        "arxiv",
        "framework",
        "vector",
        "model",
        "vision",
        "paper",
        "image",
        "API"
      ],
      "score": 0.8
    },
    {
      "title": "fastapi_mcp - Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "url": "https://github.com/tadata-org/fastapi_mcp",
      "description": "Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "published_date": "2025-03-08T11:15:43+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "tool",
        "model",
        "API",
        "context"
      ],
      "score": 0.8
    },
    {
      "title": "cosmos-reason1 - Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "url": "https://github.com/nvidia-cosmos/cosmos-reason1",
      "description": "Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "published_date": "2025-03-02T15:23:55+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "model",
        "reasoning",
        "chain-of-thought"
      ],
      "score": 0.8
    },
    {
      "title": "Nano Bio-Agents (NBA): Small Language Model Agents for Genomics",
      "url": "https://arxiv.org/abs/2509.19566",
      "description": "arXiv:2509.19566v1 Announce Type: new \nAbstract: We investigate the application of Small Language Models (<10 billion parameters) for genomics question answering via agentic framework to address hallucination issues and computational cost challenges. The Nano Bio-Agent (NBA) framework we implemented...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "tool",
        "arxiv",
        "framework",
        "model",
        "API"
      ],
      "score": 0.6
    },
    {
      "title": "Representation-based Broad Hallucination Detectors Fail to Generalize Out of Distribution",
      "url": "https://arxiv.org/abs/2509.19372",
      "description": "arXiv:2509.19372v1 Announce Type: new \nAbstract: We critically assess the efficacy of the current SOTA in hallucination detection and find that its performance on the RAGTruth dataset is largely driven by a spurious correlation with data. Controlling for this effect, state-of-the-art performs no bet...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "arxiv",
        "RAG"
      ],
      "score": 0.6
    },
    {
      "title": "Solving Freshness in RAG: A Simple Recency Prior and the Limits of Heuristic Trend Detection",
      "url": "https://arxiv.org/abs/2509.19376",
      "description": "arXiv:2509.19376v1 Announce Type: new \nAbstract: We address temporal failures in RAG systems using two methods on cybersecurity data. A simple recency prior achieved an accuracy of 1.00 on freshness tasks. In contrast, a clustering heuristic for topic evolution failed (0.08 F1-score), showing trend ...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "arxiv",
        "RAG"
      ],
      "score": 0.6
    },
    {
      "title": "excel-mcp-server - A Model Context Protocol server for Excel file manipulation",
      "url": "https://github.com/haris-musa/excel-mcp-server",
      "description": "A Model Context Protocol server for Excel file manipulation",
      "published_date": "2025-02-12T06:39:48+00:00",
      "source": "GitHub",
      "category": "industry_news",
      "keywords": [
        "model",
        "context"
      ],
      "score": 0.6
    },
    {
      "title": "mcp-agent - Build effective agents using Model Context Protocol and simple workflow patterns",
      "url": "https://github.com/lastmile-ai/mcp-agent",
      "description": "Build effective agents using Model Context Protocol and simple workflow patterns",
      "published_date": "2024-12-18T01:55:10+00:00",
      "source": "GitHub",
      "category": "industry_news",
      "keywords": [
        "model",
        "context"
      ],
      "score": 0.6
    },
    {
      "title": "AlphaCodium - Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "url": "https://github.com/Codium-ai/AlphaCodium",
      "description": "Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "published_date": "2024-01-14T15:17:18+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "paper",
        "prompt engineering",
        "prompt"
      ],
      "score": 0.6
    },
    {
      "title": "Evaluation-Aware Reinforcement Learning",
      "url": "https://arxiv.org/abs/2509.19464",
      "description": "arXiv:2509.19464v1 Announce Type: new \nAbstract: Policy evaluation is often a prerequisite for deploying safety- and performance-critical systems. Existing evaluation approaches frequently suffer from high variance due to limited data and long-horizon tasks, or high bias due to unequal support or in...",
      "published_date": "2025-09-25T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "analysis",
        "model",
        "framework"
      ],
      "score": 0.4
    },
    {
      "title": "optillm - Optimizing inference proxy for LLMs",
      "url": "https://github.com/codelion/optillm",
      "description": "Optimizing inference proxy for LLMs",
      "published_date": "2024-08-22T19:46:07+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "LLM"
      ],
      "score": 0.4
    }
  ]
}