{
  "generated_at": "2025-10-23T20:05:55.293325",
  "total_items": 50,
  "items": [
    {
      "title": "Contextual Augmentation for Entity Linking using Large Language Models",
      "url": "https://arxiv.org/abs/2510.18888",
      "description": "arXiv:2510.18888v1 Announce Type: new \nAbstract: Entity Linking involves detecting and linking entity mentions in natural language texts to a knowledge graph. Traditional methods use a two-step process with separate models for entity recognition and disambiguation, which can be computationally inten...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "large language model",
        "arxiv",
        "context",
        "framework",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Small Language Models Offer Significant Potential for Science Community",
      "url": "https://arxiv.org/abs/2510.18890",
      "description": "arXiv:2510.18890v1 Announce Type: new \nAbstract: Recent advancements in natural language processing, particularly with large language models (LLMs), are transforming how scientists engage with the literature. While the adoption of LLMs is increasing, concerns remain regarding potential information b...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "LLM",
        "large language model",
        "research",
        "tool",
        "arxiv",
        "retrieval",
        "API",
        "image",
        "analysis",
        "framework",
        "model",
        "GPT"
      ],
      "score": 1.0
    },
    {
      "title": "When Models Can't Follow: Testing Instruction Adherence Across 256 LLMs",
      "url": "https://arxiv.org/abs/2510.18892",
      "description": "arXiv:2510.18892v1 Announce Type: new \nAbstract: Despite widespread deployment of Large Language Models, systematic evaluation of instruction-following capabilities remains challenging. While comprehensive benchmarks exist, focused assessments that quickly diagnose specific instruction adherence pat...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "large language model",
        "study",
        "prompt",
        "research",
        "tool",
        "instruction",
        "arxiv",
        "paper",
        "analysis",
        "framework",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Transformer-Based Low-Resource Language Translation: A Study on Standard Bengali to Sylheti",
      "url": "https://arxiv.org/abs/2510.18898",
      "description": "arXiv:2510.18898v1 Announce Type: new \nAbstract: Machine Translation (MT) has advanced from rule-based and statistical methods to neural approaches based on the Transformer architecture. While these methods have achieved impressive results for high-resource languages, low-resource varieties such as ...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "large language model",
        "transformer",
        "study",
        "experiment",
        "zero-shot",
        "arxiv",
        "fine-tuning",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "DuoLens: A Framework for Robust Detection of Machine-Generated Multilingual Text and Code",
      "url": "https://arxiv.org/abs/2510.18904",
      "description": "arXiv:2510.18904v1 Announce Type: new \nAbstract: The prevalence of Large Language Models (LLMs) for generating multilingual text and source code has only increased the imperative for machine-generated content detectors to be accurate and efficient across domains. Current detectors, predominantly uti...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "industry_news",
      "keywords": [
        "LLM",
        "large language model",
        "zero-shot",
        "arxiv",
        "release",
        "fine-tuning",
        "framework",
        "model",
        "GPT"
      ],
      "score": 1.0
    },
    {
      "title": "Improving Topic Modeling of Social Media Short Texts with Rephrasing: A Case Study of COVID-19 Related Tweets",
      "url": "https://arxiv.org/abs/2510.18908",
      "description": "arXiv:2510.18908v1 Announce Type: new \nAbstract: Social media platforms such as Twitter (now X) provide rich data for analyzing public discourse, especially during crises such as the COVID-19 pandemic. However, the brevity, informality, and noise of social media short texts often hinder the effectiv...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "RAG",
        "large language model",
        "study",
        "arxiv",
        "analysis",
        "platform",
        "framework",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Learning from the Best, Differently: A Diversity-Driven Rethinking on Data Selection",
      "url": "https://arxiv.org/abs/2510.18909",
      "description": "arXiv:2510.18909v1 Announce Type: new \nAbstract: High-quality pre-training data is crutial for large language models, where quality captures factual reliability and semantic value, and diversity ensures broad coverage and distributional heterogeneity. Existing approaches typically rely on single or ...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "RAG",
        "large language model",
        "arxiv",
        "analysis",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Context-aware Fairness Evaluation and Mitigation in LLMs",
      "url": "https://arxiv.org/abs/2510.18914",
      "description": "arXiv:2510.18914v1 Announce Type: new \nAbstract: Large language models often display undesirable behaviors embedded in their internal representations, undermining fairness, inconsistency drift, amplification of harmful content, and the propagation of unwanted patterns during extended dialogue and co...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "context_management",
      "keywords": [
        "LLM",
        "memory",
        "large language model",
        "arxiv",
        "context",
        "framework",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Misinformation Detection using Large Language Models with Explainability",
      "url": "https://arxiv.org/abs/2510.18918",
      "description": "arXiv:2510.18918v1 Announce Type: new \nAbstract: The rapid spread of misinformation on online platforms undermines trust among individuals and hinders informed decision making. This paper shows an explainable and computationally efficient pipeline to detect misinformation using transformer-based pre...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "large language model",
        "transformer",
        "arxiv",
        "paper",
        "API",
        "fine-tuning",
        "framework",
        "platform",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Test-time Verification via Optimal Transport: Coverage, ROC, & Sub-optimality",
      "url": "https://arxiv.org/abs/2510.18982",
      "description": "arXiv:2510.18982v1 Announce Type: new \nAbstract: While test-time scaling with verification has shown promise in improving the performance of large language models (LLMs), the role of the verifier and its imperfections remain underexplored. The effect of verification manifests through interactions of...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "LLM",
        "RAG",
        "large language model",
        "arxiv",
        "framework",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Timely Clinical Diagnosis through Active Test Selection",
      "url": "https://arxiv.org/abs/2510.18988",
      "description": "arXiv:2510.18988v1 Announce Type: new \nAbstract: There is growing interest in using machine learning (ML) to support clinical diag- nosis, but most approaches rely on static, fully observed datasets and fail to reflect the sequential, resource-aware reasoning clinicians use in practice. Diagnosis re...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "large language model",
        "reasoning",
        "experiment",
        "arxiv",
        "framework",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Rectifying Shortcut Behaviors in Preference-based Reward Learning",
      "url": "https://arxiv.org/abs/2510.19050",
      "description": "arXiv:2510.19050v1 Announce Type: new \nAbstract: In reinforcement learning from human feedback, preference-based reward models play a central role in aligning large language models to human-aligned behavior. However, recent studies show that these models are prone to reward hacking and often fail to...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "large language model",
        "experiment",
        "arxiv",
        "alignment",
        "paper",
        "framework",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "The MUSE Benchmark: Probing Music Perception and Auditory Relational Reasoning in Audio LLMS",
      "url": "https://arxiv.org/abs/2510.19055",
      "description": "arXiv:2510.19055v1 Announce Type: new \nAbstract: Multimodal Large Language Models (MLLMs) have demonstrated capabilities in audio understanding, but current evaluations may obscure fundamental weaknesses in relational reasoning. We introduce the Music Understanding and Structural Evaluation (MUSE) B...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "chain_of_thought",
      "keywords": [
        "LLM",
        "model",
        "large language model",
        "reasoning",
        "prompt",
        "audio",
        "CoT",
        "prompting",
        "tool",
        "arxiv",
        "multimodal",
        "chain-of-thought"
      ],
      "score": 1.0
    },
    {
      "title": "A Multi-faceted Analysis of Cognitive Abilities: Evaluating Prompt Methods with Large Language Models on the CONSORT Checklist",
      "url": "https://arxiv.org/abs/2510.19139",
      "description": "arXiv:2510.19139v1 Announce Type: new \nAbstract: Despite the rapid expansion of Large Language Models (LLMs) in healthcare, the ability of these systems to assess clinical trial reporting according to CONSORT standards remains unclear, particularly with respect to their cognitive and reasoning strat...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "large language model",
        "reasoning",
        "study",
        "prompt",
        "arxiv",
        "API",
        "analysis",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "The Zero-Step Thinking: An Empirical Study of Mode Selection as Harder Early Exit in Reasoning Models",
      "url": "https://arxiv.org/abs/2510.19176",
      "description": "arXiv:2510.19176v1 Announce Type: new \nAbstract: Reasoning models have demonstrated exceptional performance in tasks such as mathematics and logical reasoning, primarily due to their ability to engage in step-by-step thinking during the reasoning process. However, this often leads to overthinking, r...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "chain_of_thought",
      "keywords": [
        "model",
        "RAG",
        "reasoning",
        "study",
        "prompt",
        "CoT",
        "arxiv",
        "paper",
        "chain-of-thought",
        "step-by-step"
      ],
      "score": 1.0
    },
    {
      "title": "ChatGPT Unveils Its Limits: Principles of Law Deliver Checkmate",
      "url": "https://arxiv.org/abs/2510.19261",
      "description": "arXiv:2510.19261v1 Announce Type: new \nAbstract: This study examines the performance of ChatGPT with an experiment in the legal domain. We compare the outcome with it a baseline using regular expressions (Regex), rather than focusing solely on the assessment against human performance. The study reve...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "reasoning",
        "study",
        "experiment",
        "arxiv",
        "GPT"
      ],
      "score": 1.0
    },
    {
      "title": "Learning to Make Friends: Coaching LLM Agents toward Emergent Social Ties",
      "url": "https://arxiv.org/abs/2510.19299",
      "description": "arXiv:2510.19299v1 Announce Type: new \nAbstract: Can large language model (LLM) agents reproduce the complex social dynamics that characterize human online behavior -- shaped by homophily, reciprocity, and social validation -- and what memory and learning mechanisms enable such dynamics to emerge? W...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "model",
        "memory",
        "large language model",
        "study",
        "experiment",
        "arxiv",
        "context",
        "framework",
        "in-context"
      ],
      "score": 1.0
    },
    {
      "title": "3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and Latency",
      "url": "https://arxiv.org/abs/2510.18905",
      "description": "arXiv:2510.18905v1 Announce Type: new \nAbstract: AI inference scaling is often tuned through 1D heuristics (a fixed reasoning passes) or 2D bivariate trade-offs (e.g., performance vs. compute), which fail to consider cost and latency constraints. We introduce a 3D optimization framework that jointly...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "chain_of_thought",
      "keywords": [
        "large language model",
        "reasoning",
        "arxiv",
        "context",
        "framework",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Benchmarking On-Device Machine Learning on Apple Silicon with MLX",
      "url": "https://arxiv.org/abs/2510.18921",
      "description": "arXiv:2510.18921v1 Announce Type: new \nAbstract: The recent widespread adoption of Large Language Models (LLMs) and machine learning in general has sparked research interest in exploring the possibilities of deploying these models on smaller devices such as laptops and mobile phones. This creates a ...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "RAG",
        "large language model",
        "transformer",
        "research",
        "study",
        "experiment",
        "arxiv",
        "paper",
        "framework",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Noise-corrected GRPO: From Noisy Rewards to Unbiased Gradients",
      "url": "https://arxiv.org/abs/2510.18924",
      "description": "arXiv:2510.18924v1 Announce Type: new \nAbstract: Reinforcement learning from human feedback (RLHF) or verifiable rewards (RLVR), the standard paradigm for aligning LLMs or building recent SOTA reasoning models, is highly sensitive to noise from inconsistent or erroneous rewards. Yet, the interaction...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "reasoning",
        "arxiv",
        "RLHF",
        "analysis",
        "framework",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping",
      "url": "https://arxiv.org/abs/2510.18927",
      "description": "arXiv:2510.18927v1 Announce Type: new \nAbstract: Reinforcement learning (RL) has recently become the core paradigm for aligning and strengthening large language models (LLMs). Yet, applying RL in off-policy settings--where stale data from past policies are used for training--improves sample efficien...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "large language model",
        "arxiv",
        "analysis",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Context-Engineering - \"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" — Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "url": "https://github.com/davidkimai/Context-Engineering",
      "description": "\"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" — Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "published_date": "2025-06-29T00:16:36+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "prompt",
        "context window",
        "context",
        "prompt engineering"
      ],
      "score": 1.0
    },
    {
      "title": "ThinkSound - [NeurIPS 2025] PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "url": "https://github.com/FunAudioLLM/ThinkSound",
      "description": "[NeurIPS 2025] PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "published_date": "2025-06-27T02:27:00+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "reasoning",
        "audio",
        "CoT",
        "framework",
        "chain-of-thought"
      ],
      "score": 1.0
    },
    {
      "title": "mcp-context-forge - A Model Context Protocol (MCP) Gateway & Registry. Serves as a central management point for tools, resources, and prompts that can be accessed by MCP-compatible LLM applications. Converts REST API endpoints to MCP, composes virtual MCP servers with added security and observability, and converts between protocols (stdio, SSE, Streamable HTTP).",
      "url": "https://github.com/IBM/mcp-context-forge",
      "description": "A Model Context Protocol (MCP) Gateway & Registry. Serves as a central management point for tools, resources, and prompts that can be accessed by MCP-compatible LLM applications. Converts REST API endpoints to MCP, composes virtual MCP servers with added security and observability, and converts between protocols (stdio, SSE, Streamable HTTP).",
      "published_date": "2025-05-08T08:16:59+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "LLM",
        "prompt",
        "tool",
        "context",
        "API",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Cline-Recursive-Chain-of-Thought-System-CRCT- - A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "url": "https://github.com/RPG-fan/Cline-Recursive-Chain-of-Thought-System-CRCT-",
      "description": "A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "published_date": "2025-02-18T15:45:30+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "context",
        "framework",
        "chain-of-thought"
      ],
      "score": 1.0
    },
    {
      "title": "LightRAG - [EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "url": "https://github.com/HKUDS/LightRAG",
      "description": "[EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "published_date": "2024-10-02T11:57:54+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "retrieval",
        "augmented",
        "RAG"
      ],
      "score": 1.0
    },
    {
      "title": "KAG - KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "url": "https://github.com/OpenSPG/KAG",
      "description": "KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "published_date": "2024-09-21T13:56:44+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "LLM",
        "RAG",
        "reasoning",
        "knowledge base",
        "vector",
        "retrieval",
        "framework",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Kiln - The easiest tool for fine-tuning LLM models, synthetic data generation, and collaborating on datasets.",
      "url": "https://github.com/Kiln-AI/Kiln",
      "description": "The easiest tool for fine-tuning LLM models, synthetic data generation, and collaborating on datasets.",
      "published_date": "2024-07-23T23:10:13+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "LLM",
        "model",
        "tool",
        "fine-tuning"
      ],
      "score": 1.0
    },
    {
      "title": "graphrag - A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "url": "https://github.com/microsoft/graphrag",
      "description": "A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "published_date": "2024-03-27T17:57:52+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "retrieval",
        "augmented",
        "RAG"
      ],
      "score": 1.0
    },
    {
      "title": "R2R - SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "url": "https://github.com/SciPhi-AI/R2R",
      "description": "SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "published_date": "2024-02-12T03:24:27+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "product",
        "retrieval",
        "API",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "openlit - Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. 🚀💻 Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "url": "https://github.com/openlit/openlit",
      "description": "Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. 🚀💻 Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "published_date": "2024-01-23T17:40:59+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "LLM",
        "prompt",
        "vector",
        "platform",
        "framework"
      ],
      "score": 1.0
    },
    {
      "title": "AutoRAG - AutoRAG: An Open-Source Framework for Retrieval-Augmented Generation (RAG) Evaluation & Optimization with AutoML-Style Automation",
      "url": "https://github.com/Marker-Inc-Korea/AutoRAG",
      "description": "AutoRAG: An Open-Source Framework for Retrieval-Augmented Generation (RAG) Evaluation & Optimization with AutoML-Style Automation",
      "published_date": "2024-01-10T12:25:00+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "retrieval",
        "augmented",
        "framework",
        "RAG"
      ],
      "score": 1.0
    },
    {
      "title": "ADPO: Anchored Direct Preference Optimization",
      "url": "https://arxiv.org/abs/2510.18913",
      "description": "arXiv:2510.18913v1 Announce Type: new \nAbstract: Anchored Direct Preference Optimization (ADPO) is a unified framework that generalizes Direct Preference Optimization (DPO) with soft preferences, reference-policy anchoring, and groupwise extensions. While standard DPO assumes hard binary labels and ...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "arxiv",
        "context",
        "framework",
        "model"
      ],
      "score": 0.8
    },
    {
      "title": "Application of Reduced-Order Models for Temporal Multiscale Representations in the Prediction of Dynamical Systems",
      "url": "https://arxiv.org/abs/2510.18925",
      "description": "arXiv:2510.18925v1 Announce Type: new \nAbstract: Modeling and predicting the dynamics of complex multiscale systems remains a significant challenge due to their inherent nonlinearities and sensitivity to initial conditions, as well as limitations of traditional machine learning methods that fail to ...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "RAG",
        "study",
        "arxiv",
        "framework",
        "model"
      ],
      "score": 0.8
    },
    {
      "title": "Position: Many generalization measures for deep learning are fragile",
      "url": "https://arxiv.org/abs/2510.18934",
      "description": "arXiv:2510.18934v1 Announce Type: new \nAbstract: A wide variety of generalization measures have been applied to deep neural networks (DNNs). Although obtaining tight bounds remains challenging, such measures are often assumed to reproduce qualitative generalization trends. In this position paper, we...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "example",
        "paper",
        "RAG",
        "arxiv"
      ],
      "score": 0.8
    },
    {
      "title": "NeuroAda: Activating Each Neuron's Potential for Parameter-Efficient Fine-Tuning",
      "url": "https://arxiv.org/abs/2510.18940",
      "description": "arXiv:2510.18940v1 Announce Type: new \nAbstract: Existing parameter-efficient fine-tuning (PEFT) methods primarily fall into two categories: addition-based and selective in-situ adaptation. The former, such as LoRA, introduce additional modules to adapt the model to downstream tasks, offering strong...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "industry_news",
      "keywords": [
        "memory",
        "arxiv",
        "release",
        "fine-tuning",
        "model"
      ],
      "score": 0.8
    },
    {
      "title": "fastapi_mcp - Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "url": "https://github.com/tadata-org/fastapi_mcp",
      "description": "Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "published_date": "2025-03-08T11:15:43+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "context",
        "API",
        "model",
        "tool"
      ],
      "score": 0.8
    },
    {
      "title": "cosmos-reason1 - Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "url": "https://github.com/nvidia-cosmos/cosmos-reason1",
      "description": "Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "published_date": "2025-03-02T15:23:55+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "model",
        "chain-of-thought",
        "reasoning"
      ],
      "score": 0.8
    },
    {
      "title": "MMAO-Bench: MultiModal All in One Benchmark Reveals Compositional Law between Uni-modal and Omni-modal in OmniModels",
      "url": "https://arxiv.org/abs/2510.18915",
      "description": "arXiv:2510.18915v1 Announce Type: new \nAbstract: Multimodal Large Languages models have been progressing from uni-modal understanding toward unifying visual, audio and language modalities, collectively termed omni models. However, the correlation between uni-modal and omni-modal remains unclear, whi...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "multimodal_context",
      "keywords": [
        "cross-modal",
        "reasoning",
        "audio",
        "experiment",
        "arxiv",
        "multimodal",
        "model"
      ],
      "score": 0.6
    },
    {
      "title": "WebGraphEval: Multi-Turn Trajectory Evaluation for Web Agents using Graph Representation",
      "url": "https://arxiv.org/abs/2510.19205",
      "description": "arXiv:2510.19205v1 Announce Type: new \nAbstract: Current evaluation of web agents largely reduces to binary success metrics or conformity to a single reference trajectory, ignoring the structural diversity present in benchmark datasets. We present WebGraphEval, a framework that abstracts trajectorie...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "framework",
        "RAG",
        "model",
        "arxiv"
      ],
      "score": 0.6
    },
    {
      "title": "excel-mcp-server - A Model Context Protocol server for Excel file manipulation",
      "url": "https://github.com/haris-musa/excel-mcp-server",
      "description": "A Model Context Protocol server for Excel file manipulation",
      "published_date": "2025-02-12T06:39:48+00:00",
      "source": "GitHub",
      "category": "industry_news",
      "keywords": [
        "context",
        "model"
      ],
      "score": 0.6
    },
    {
      "title": "mcp-agent - Build effective agents using Model Context Protocol and simple workflow patterns",
      "url": "https://github.com/lastmile-ai/mcp-agent",
      "description": "Build effective agents using Model Context Protocol and simple workflow patterns",
      "published_date": "2024-12-18T01:55:10+00:00",
      "source": "GitHub",
      "category": "industry_news",
      "keywords": [
        "context",
        "model"
      ],
      "score": 0.6
    },
    {
      "title": "AlphaCodium - Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "url": "https://github.com/Codium-ai/AlphaCodium",
      "description": "Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "published_date": "2024-01-14T15:17:18+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "prompt",
        "paper",
        "prompt engineering"
      ],
      "score": 0.6
    },
    {
      "title": "An Argumentative Explanation Framework for Generalized Reason Model with Inconsistent Precedents",
      "url": "https://arxiv.org/abs/2510.19263",
      "description": "arXiv:2510.19263v1 Announce Type: new \nAbstract: Precedential constraint is one foundation of case-based reasoning in AI and Law. It generally assumes that the underlying set of precedents must be consistent. To relax this assumption, a generalized notion of the reason model has been introduced. Whi...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "reasoning",
        "arxiv",
        "paper",
        "framework",
        "model"
      ],
      "score": 0.4
    },
    {
      "title": "Large Connectome Model: An fMRI Foundation Model of Brain Connectomes Empowered by Brain-Environment Interaction in Multitask Learning Landscape",
      "url": "https://arxiv.org/abs/2510.18910",
      "description": "arXiv:2510.18910v1 Announce Type: new \nAbstract: A reliable foundation model of functional neuroimages is critical to promote clinical applications where the performance of current AI models is significantly impeded by a limited sample size. To that end, tremendous efforts have been made to pretrain...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "multimodal_context",
      "keywords": [
        "arxiv",
        "API",
        "image",
        "vision",
        "model"
      ],
      "score": 0.4
    },
    {
      "title": "Towards Universal Solvers: Using PGD Attack in Active Learning to Increase Generalizability of Neural Operators as Knowledge Distillation from Numerical PDE Solvers",
      "url": "https://arxiv.org/abs/2510.18989",
      "description": "arXiv:2510.18989v1 Announce Type: new \nAbstract: Nonlinear PDE solvers require fine space-time discretizations and local linearizations, leading to high memory cost and slow runtimes. Neural operators such as FNOs and DeepONets offer fast single-shot inference by learning function-to-function mappin...",
      "published_date": "2025-10-23T04:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "framework",
        "memory",
        "arxiv"
      ],
      "score": 0.4
    },
    {
      "title": "Sentence Transformers is joining Hugging Face!",
      "url": "https://huggingface.co/blog/sentence-transformers-joins-hf",
      "description": "...",
      "published_date": "2025-10-22T00:00:00",
      "source": "Hugging Face Blog",
      "category": "prompt_engineering",
      "keywords": [
        "transformer"
      ],
      "score": 0.4
    },
    {
      "title": "optillm - Optimizing inference proxy for LLMs",
      "url": "https://github.com/codelion/optillm",
      "description": "Optimizing inference proxy for LLMs",
      "published_date": "2024-08-22T19:46:07+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "LLM"
      ],
      "score": 0.4
    },
    {
      "title": "Supercharge your OCR Pipelines with Open Models",
      "url": "https://huggingface.co/blog/ocr-open-models",
      "description": "...",
      "published_date": "2025-10-21T00:00:00",
      "source": "Hugging Face Blog",
      "category": "industry_news",
      "keywords": [
        "model"
      ],
      "score": 0.2
    },
    {
      "title": "Unlock the power of images with AI Sheets",
      "url": "https://huggingface.co/blog/aisheets-unlock-images",
      "description": "...",
      "published_date": "2025-10-21T00:00:00",
      "source": "Hugging Face Blog",
      "category": "multimodal_context",
      "keywords": [
        "image"
      ],
      "score": 0.2
    }
  ]
}