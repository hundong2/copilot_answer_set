<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Context Engineering Daily - Multimodal Context</title>
    <link>https://your-username.github.io/context-engineering-news#multimodal_context</link>
    <description>Latest Multimodal Context news in Context Engineering</description>
    <language>en-us</language>
    <item>
      <title>EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture</title>
      <link>https://arxiv.org/abs/2510.16198</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.16198</guid>
      <description>arXiv:2510.16198v1 Announce Type: new 
Abstract: Despite recent advances in AI, multimodal culturally diverse datasets are still limited, particularly for regions in the Middle East and Africa. In this paper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian culture. By designing ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; context, vision, arxiv, paper, zero-shot | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 21 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Multimodal Context</category>
      <category>context</category>
      <category>vision</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search</title>
      <link>https://arxiv.org/abs/2510.15948</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.15948</guid>
      <description>arXiv:2510.15948v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable progress in multimodal perception and generation, yet their safety alignment remains a critical challenge.Existing defenses and vulnerable to multimodal jailbreaks, as visual inputs introdu...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; arxiv, vision, cross-modal, prompt, experiment | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 21 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Multimodal Context</category>
      <category>arxiv</category>
      <category>vision</category>
      <category>cross-modal</category>
    </item>
    <item>
      <title>Unlock the power of images with AI Sheets</title>
      <link>https://huggingface.co/blog/aisheets-unlock-images</link>
      <guid isPermaLink="false">https://huggingface.co/blog/aisheets-unlock-images</guid>
      <description>...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; Hugging Face Blog | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; image | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 20%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 21 Oct 2025 00:00:00 </pubDate>
      <author>noreply@contextengineering.news (Hugging Face Blog)</author>
      <category>Multimodal Context</category>
      <category>image</category>
    </item>
  </channel>
</rss>