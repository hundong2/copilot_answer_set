<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Context Engineering Daily - Research Papers</title>
    <link>https://your-username.github.io/context-engineering-news#research_papers</link>
    <description>Latest Research Papers news in Context Engineering</description>
    <language>en-us</language>
    <item>
      <title>Response-Based Knowledge Distillation for Multilingual Jailbreak Prevention Unwittingly Compromises Safety</title>
      <link>https://arxiv.org/abs/2602.11157</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.11157</guid>
      <description>arXiv:2602.11157v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed worldwide, yet their safety alignment remains predominantly English-centric. This allows for vulnerabilities in non-English contexts, especially with low-resource languages. We introduce a novel a...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; reasoning, LLM, research, experiment, context | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 13 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>reasoning</category>
      <category>LLM</category>
      <category>research</category>
    </item>
    <item>
      <title>Retrieval Heads are Dynamic</title>
      <link>https://arxiv.org/abs/2602.11162</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.11162</guid>
      <description>arXiv:2602.11162v1 Announce Type: new 
Abstract: Recent studies have identified &amp;quot;retrieval heads&amp;quot; in Large Language Models (LLMs) responsible for extracting information from input contexts. However, prior works largely rely on static statistics aggregated across datasets, identifying heads that perf...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; LLM, retrieval, context, arxiv, paper | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 13 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>LLM</category>
      <category>retrieval</category>
      <category>context</category>
    </item>
    <item>
      <title>Nested Named Entity Recognition in Plasma Physics Research Articles</title>
      <link>https://arxiv.org/abs/2602.11163</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.11163</guid>
      <description>arXiv:2602.11163v1 Announce Type: new 
Abstract: Named Entity Recognition (NER) is an important task in natural language processing that aims to identify and extract key entities from unstructured text. We present a novel application of NER in plasma physics research articles and address the challen...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; research, context, transformer, arxiv, ICL | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 13 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>research</category>
      <category>context</category>
      <category>transformer</category>
    </item>
    <item>
      <title>Assessing LLM Reliability on Temporally Recent Open-Domain Questions</title>
      <link>https://arxiv.org/abs/2602.11165</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.11165</guid>
      <description>arXiv:2602.11165v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly deployed for open-domain question answering, yet their alignment with human perspectives on temporally recent information remains underexplored. We introduce RECOM (Reddit Evaluation for Correspondence of ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; GPT, framework, product, LLM, analysis | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 13 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>GPT</category>
      <category>framework</category>
      <category>product</category>
    </item>
    <item>
      <title>Small Updates, Big Doubts: Does Parameter-Efficient Fine-tuning Enhance Hallucination Detection ?</title>
      <link>https://arxiv.org/abs/2602.11166</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.11166</guid>
      <description>arXiv:2602.11166v1 Announce Type: new 
Abstract: Parameter-efficient fine-tuning (PEFT) methods are widely used to adapt large language models (LLMs) to downstream tasks and are often assumed to improve factual correctness. However, how the parameter-efficient fine-tuning methods affect hallucinatio...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; LLM, experiment, fine-tuning, arxiv, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 13 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>LLM</category>
      <category>experiment</category>
      <category>fine-tuning</category>
    </item>
    <item>
      <title>Visualizing and Benchmarking LLM Factual Hallucination Tendencies via Internal State Analysis and Clustering</title>
      <link>https://arxiv.org/abs/2602.11167</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.11167</guid>
      <description>arXiv:2602.11167v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often hallucinate, generating nonsensical or false information that can be especially harmful in sensitive fields such as medicine or law. To study this phenomenon systematically, we introduce FalseCite, a curated dataset ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; GPT, vector, research, LLM, analysis | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 13 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>GPT</category>
      <category>vector</category>
      <category>research</category>
    </item>
    <item>
      <title>Enhancing SDG-Text Classification with Combinatorial Fusion Analysis and Generative AI</title>
      <link>https://arxiv.org/abs/2602.11168</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.11168</guid>
      <description>arXiv:2602.11168v1 Announce Type: new 
Abstract: (Natural Language Processing) NLP techniques such as text classification and topic discovery are very useful in many application areas including information retrieval, knowledge discovery, policy formulation, and decision-making. However, it remains a...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; retrieval, context, paper, analysis, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 13 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>retrieval</category>
      <category>context</category>
      <category>paper</category>
    </item>
    <item>
      <title>Disentangling Direction and Magnitude in Transformer Representations: A Double Dissociation Through L2-Matched Perturbation Analysis</title>
      <link>https://arxiv.org/abs/2602.11169</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.11169</guid>
      <description>arXiv:2602.11169v1 Announce Type: new 
Abstract: Transformer hidden states encode information as high-dimensional vectors, yet whether direction (orientation in representational space) and magnitude (vector norm) serve distinct functional roles remains unclear. Studying Pythia-family models, we disc...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; vector, research, transformer, analysis, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 13 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>vector</category>
      <category>research</category>
      <category>transformer</category>
    </item>
    <item>
      <title>Latent Generative Solvers for Generalizable Long-Term Physics Simulation</title>
      <link>https://arxiv.org/abs/2602.11229</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.11229</guid>
      <description>arXiv:2602.11229v1 Announce Type: new 
Abstract: We study long-horizon surrogate simulation across heterogeneous PDE systems. We introduce Latent Generative Solvers (LGS), a two-stage framework that (i) maps diverse PDE states into a shared latent physics space with a pretrained VAE, and (ii) learns...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; context, transformer, arxiv, framework, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 13 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>context</category>
      <category>transformer</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>Dissecting Subjectivity and the &amp;quot;Ground Truth&amp;quot; Illusion in Data Annotation</title>
      <link>https://arxiv.org/abs/2602.11318</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.11318</guid>
      <description>arXiv:2602.11318v1 Announce Type: new 
Abstract: In machine learning, &amp;quot;ground truth&amp;quot; refers to the assumed correct labels used to train and evaluate models. However, the foundational &amp;quot;ground truth&amp;quot; paradigm rests on a positivistic fallacy that treats human disagreement as technical noise rather than...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; research, paper, analysis, arxiv, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 13 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>research</category>
      <category>paper</category>
      <category>analysis</category>
    </item>
    <item>
      <title>Bi-Level Prompt Optimization for Multimodal LLM-as-a-Judge</title>
      <link>https://arxiv.org/abs/2602.11340</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.11340</guid>
      <description>arXiv:2602.11340v1 Announce Type: new 
Abstract: Large language models (LLMs) have become widely adopted as automated judges for evaluating AI-generated content. Despite their success, aligning LLM-based evaluations with human judgments remains challenging. While supervised fine-tuning on human-labe...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; study, LLM, experiment, context, example | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 13 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>study</category>
      <category>LLM</category>
      <category>experiment</category>
    </item>
    <item>
      <title>ReplicatorBench: Benchmarking LLM Agents for Replicability in Social and Behavioral Sciences</title>
      <link>https://arxiv.org/abs/2602.11354</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.11354</guid>
      <description>arXiv:2602.11354v1 Announce Type: new 
Abstract: The literature has witnessed an emerging interest in AI agents for automated assessment of scientific papers. Existing benchmarks focus primarily on the computational aspect of this task, testing agents&amp;#x27; ability to reproduce or replicate research outc...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; framework, product, research, LLM, experiment | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 13 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>framework</category>
      <category>product</category>
      <category>research</category>
    </item>
    <item>
      <title>Automated Optimization Modeling via a Localizable Error-Driven Perspective</title>
      <link>https://arxiv.org/abs/2602.11164</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.11164</guid>
      <description>arXiv:2602.11164v1 Announce Type: new 
Abstract: Automated optimization modeling via Large Language Models (LLMs) has emerged as a promising approach to assist complex human decision-making. While post-training has become a pivotal technique to enhance LLMs&amp;#x27; capabilities in this domain, its effectiv...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; reasoning, LLM, experiment, arxiv, RAG | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 13 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>reasoning</category>
      <category>LLM</category>
      <category>experiment</category>
    </item>
    <item>
      <title>Spectra: Rethinking Optimizers for LLMs Under Spectral Anisotropy</title>
      <link>https://arxiv.org/abs/2602.11185</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.11185</guid>
      <description>arXiv:2602.11185v1 Announce Type: new 
Abstract: Gradient signals in LLM training are highly anisotropic: recurrent linguistic structure concentrates energy into a small set of dominant spectral directions, while context specific information resides in a long tail. We show that this spike tail separ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; API, memory, LLM, context, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 13 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>API</category>
      <category>memory</category>
      <category>LLM</category>
    </item>
    <item>
      <title>GAC-KAN: An Ultra-Lightweight GNSS Interference Classifier for GenAI-Powered Consumer Edge Devices</title>
      <link>https://arxiv.org/abs/2602.11186</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.11186</guid>
      <description>arXiv:2602.11186v1 Announce Type: new 
Abstract: The integration of Generative AI (GenAI) into Consumer Electronics (CE)--from AI-powered assistants in wearables to generative planning in autonomous Uncrewed Aerial Vehicles (UAVs)--has revolutionized user experiences. However, these GenAI applicatio...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; framework, vision, experiment, paper, transformer | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 13 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>framework</category>
      <category>vision</category>
      <category>experiment</category>
    </item>
    <item>
      <title>Time-TK: A Multi-Offset Temporal Interaction Framework Combining Transformer and Kolmogorov-Arnold Networks for Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2602.11190</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.11190</guid>
      <description>arXiv:2602.11190v1 Announce Type: new 
Abstract: Time series forecasting is crucial for the World Wide Web and represents a core technical challenge in ensuring the stable and efficient operation of modern web services, such as intelligent transportation and website throughput. However, we have foun...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; experiment, transformer, arxiv, framework, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 13 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>experiment</category>
      <category>transformer</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>Predicting the post-wildfire mudflow onset using machine learning models on multi-parameter experimental data</title>
      <link>https://arxiv.org/abs/2602.11194</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.11194</guid>
      <description>arXiv:2602.11194v1 Announce Type: new 
Abstract: Post-wildfire mudflows are increasingly hazardous due to the prevalence of wildfires, including those on the wildland-urban interface. Upon burning, soil on the surface or immediately beneath becomes hydrophobic, a phenomenon that occurs predominantly...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; vector, experiment, analysis, arxiv, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 13 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>vector</category>
      <category>experiment</category>
      <category>analysis</category>
    </item>
    <item>
      <title>Explaining AI Without Code: A User Study on Explainable AI</title>
      <link>https://arxiv.org/abs/2602.11159</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.11159</guid>
      <description>arXiv:2602.11159v1 Announce Type: new 
Abstract: The increasing use of Machine Learning (ML) in sensitive domains such as healthcare, finance, and public policy has raised concerns about the transparency of automated decisions. Explainable AI (XAI) addresses this by clarifying how models generate pr...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; arxiv, model, study, platform | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 40%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 13 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>arxiv</category>
      <category>model</category>
      <category>study</category>
    </item>
    <item>
      <title>On Decision-Valued Maps and Representational Dependence</title>
      <link>https://arxiv.org/abs/2602.11295</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.11295</guid>
      <description>arXiv:2602.11295v1 Announce Type: new 
Abstract: A computational engine applied to different representations of the same data can produce different discrete outcomes, with some representations preserving the result and others changing it entirely. A decision-valued map records which representations ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; arxiv, paper | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 40%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 13 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>arxiv</category>
      <category>paper</category>
    </item>
  </channel>
</rss>