<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Context Engineering Daily - Context Management</title>
    <link>https://your-username.github.io/context-engineering-news#context_management</link>
    <description>Latest Context Management news in Context Engineering</description>
    <language>en-us</language>
    <item>
      <title>DYCP: Dynamic Context Pruning for Long-Form Dialogue with LLMs</title>
      <link>https://arxiv.org/abs/2601.07994</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.07994</guid>
      <description>arXiv:2601.07994v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often exhibit increased response latency and degraded answer quality as dialogue length grows, making effective context management essential. However, existing methods rely on extra LLM calls to build memory or perform off...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; LLM, context, large language model, retrieval, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 14 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Context Management</category>
      <category>LLM</category>
      <category>context</category>
      <category>large language model</category>
    </item>
    <item>
      <title>Hierarchical Sparse Plus Low Rank Compression of LLM</title>
      <link>https://arxiv.org/abs/2601.07839</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.07839</guid>
      <description>arXiv:2601.07839v1 Announce Type: new 
Abstract: Modern large language models (LLMs) place extraordinary pressure on memory and compute budgets, making principled compression indispensable for both deployment and continued training. We present Hierarchical Sparse Plus Low-Rank (HSS) compression, a t...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; example, attention, LLM, experiment, large language model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 14 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Context Management</category>
      <category>example</category>
      <category>attention</category>
      <category>LLM</category>
    </item>
    <item>
      <title>Sliced-Wasserstein Distribution Alignment Loss Improves the Ultra-Low-Bit Quantization of Large Language Models</title>
      <link>https://arxiv.org/abs/2601.07878</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.07878</guid>
      <description>arXiv:2601.07878v1 Announce Type: new 
Abstract: The benefits of most large language models come with steep and often hidden economic and environmental costs due to their resource usage inefficiency during deployment. Model quantization improves energy and memory efficiency through representing mode...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; framework, alignment, large language model, compression, augmented | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 14 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Context Management</category>
      <category>framework</category>
      <category>alignment</category>
      <category>large language model</category>
    </item>
  </channel>
</rss>