{
  "generated_at": "2025-12-12T20:05:55.299573",
  "total_items": 48,
  "items": [
    {
      "title": "What Kind of Reasoning (if any) is an LLM actually doing? On the Stochastic Nature and Abductive Appearance of Large Language Models",
      "url": "https://arxiv.org/abs/2512.10080",
      "description": "arXiv:2512.10080v1 Announce Type: new \nAbstract: This article looks at how reasoning works in current Large Language Models (LLMs) that function using the token-completion method. It examines their stochastic nature and their similarity to human abductive reasoning. The argument is that these LLMs c...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "in_context_learning",
      "keywords": [
        "LLM",
        "reasoning",
        "analysis",
        "ICL",
        "example",
        "arxiv",
        "model",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "Generate-Then-Validate: A Novel Question Generation Approach Using Small Language Models",
      "url": "https://arxiv.org/abs/2512.10110",
      "description": "arXiv:2512.10110v1 Announce Type: new \nAbstract: We explore the use of small language models (SLMs) for automatic question generation as a complement to the prevalent use of their large counterparts in learning analytics research. We present a novel question generation pipeline that leverages both t...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "RAG",
        "LLM",
        "reasoning",
        "arxiv",
        "model",
        "research",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "Workflow is All You Need: Escaping the \"Statistical Smoothing Trap\" via High-Entropy Information Foraging and Adversarial Pacing",
      "url": "https://arxiv.org/abs/2512.10121",
      "description": "arXiv:2512.10121v1 Announce Type: new \nAbstract: Central to long-form text generation in vertical domains is the \"impossible trinity\" confronting current large language models (LLMs): the simultaneous achievement of low hallucination, deep logical coherence, and personalized expression. This study e...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "large language model",
        "framework",
        "experiment",
        "study",
        "RAG",
        "prompting",
        "LLM",
        "zero-shot",
        "retrieval",
        "context",
        "API",
        "arxiv",
        "model",
        "GPT",
        "knowledge base",
        "prompt"
      ],
      "score": 1.0
    },
    {
      "title": "PARAN: Persona-Augmented Review ANswering system on Food Delivery Review Dataset",
      "url": "https://arxiv.org/abs/2512.10148",
      "description": "arXiv:2512.10148v1 Announce Type: new \nAbstract: Personalized review response generation presents a significant challenge in domains where user information is limited, such as food delivery platforms. While large language models (LLMs) offer powerful text generation capabilities, they often produce ...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "fine-tuning",
        "framework",
        "RAG",
        "prompting",
        "LLM",
        "augmented",
        "platform",
        "context",
        "arxiv",
        "prompt",
        "model",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "Unforgotten Safety: Preserving Safety Alignment of Large Language Models with Continual Learning",
      "url": "https://arxiv.org/abs/2512.10150",
      "description": "arXiv:2512.10150v1 Announce Type: new \nAbstract: The safety alignment of large language models (LLMs) is becoming increasingly important with their democratization. In this paper, we study the safety degradation that comes with adapting LLMs to new tasks. We attribute this safety compromise to catas...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "large language model",
        "study",
        "alignment",
        "paper",
        "LLM",
        "arxiv",
        "model",
        "fine-tuning",
        "memory"
      ],
      "score": 1.0
    },
    {
      "title": "AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding",
      "url": "https://arxiv.org/abs/2512.10195",
      "description": "arXiv:2512.10195v1 Announce Type: new \nAbstract: Evaluating large language models (LLMs) has recently emerged as a critical issue for safe and trustworthy application of LLMs in the medical domain. Although a variety of static medical question-answering (QA) benchmarks have been proposed, many aspec...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "framework",
        "LLM",
        "arxiv",
        "model",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
      "url": "https://arxiv.org/abs/2512.10398",
      "description": "arXiv:2512.10398v1 Announce Type: new \nAbstract: Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents pr...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "SDK",
        "tool",
        "reasoning",
        "platform",
        "context",
        "API",
        "arxiv",
        "product",
        "research",
        "memory"
      ],
      "score": 1.0
    },
    {
      "title": "Sliding Window Attention Adaptation",
      "url": "https://arxiv.org/abs/2512.10411",
      "description": "arXiv:2512.10411v1 Announce Type: new \nAbstract: The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "chain_of_thought",
      "keywords": [
        "fine-tuning",
        "experiment",
        "LLM",
        "chain-of-thought",
        "transformer",
        "CoT",
        "context",
        "arxiv",
        "model",
        "large language model",
        "attention"
      ],
      "score": 1.0
    },
    {
      "title": "Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers",
      "url": "https://arxiv.org/abs/2512.10422",
      "description": "arXiv:2512.10422v1 Announce Type: new \nAbstract: Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG m...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "framework",
        "experiment",
        "RAG",
        "LLM",
        "reasoning",
        "augmented",
        "retrieval",
        "arxiv",
        "model",
        "large language model",
        "attention"
      ],
      "score": 1.0
    },
    {
      "title": "Suzume-chan: Your Personal Navigator as an Embodied Information Hub",
      "url": "https://arxiv.org/abs/2512.09932",
      "description": "arXiv:2512.09932v1 Announce Type: new \nAbstract: Access to expert knowledge often requires real-time human communication. Digital tools improve access to information but rarely create the sense of connection needed for deep understanding. This study addresses this issue using Social Presence Theory,...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "tool",
        "study",
        "RAG",
        "augmented",
        "retrieval",
        "arxiv",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Exploring Health Misinformation Detection with Multi-Agent Debate",
      "url": "https://arxiv.org/abs/2512.09935",
      "description": "arXiv:2512.09935v1 Announce Type: new \nAbstract: Fact-checking health-related claims has become increasingly critical as misinformation proliferates online. Effective verification requires both the retrieval of high-quality evidence and rigorous reasoning processes. In this paper, we propose a two-s...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "framework",
        "experiment",
        "paper",
        "LLM",
        "reasoning",
        "ICL",
        "retrieval",
        "arxiv",
        "model",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "Echo-CoPilot: A Multi-View, Multi-Task Agent for Echocardiography Interpretation and Reporting",
      "url": "https://arxiv.org/abs/2512.09944",
      "description": "arXiv:2512.09944v1 Announce Type: new \nAbstract: Echocardiography is central to contemporary cardiovascular care, but full-study interpretation remains a cognitively demanding, multi-view task that is still performed manually. While recent foundation models for echocardiography can achieve strong pe...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "release",
        "tool",
        "study",
        "paper",
        "RAG",
        "vision",
        "context",
        "arxiv",
        "model",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "Exploring LLMs for Scientific Information Extraction Using The SciEx Framework",
      "url": "https://arxiv.org/abs/2512.10004",
      "description": "arXiv:2512.10004v1 Announce Type: new \nAbstract: Large language models (LLMs) are increasingly touted as powerful tools for automating scientific information extraction. However, existing methods and tools often struggle with the realities of scientific literature: long-context documents, multi-moda...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "framework",
        "tool",
        "prompting",
        "LLM",
        "reasoning",
        "retrieval",
        "context",
        "API",
        "arxiv",
        "prompt",
        "model",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "DynaMate: An Autonomous Agent for Protein-Ligand Molecular Dynamics Simulations",
      "url": "https://arxiv.org/abs/2512.10034",
      "description": "arXiv:2512.10034v1 Announce Type: new \nAbstract: Force field-based molecular dynamics (MD) simulations are indispensable for probing the structure, dynamics, and functions of biomolecular systems, including proteins and protein-ligand complexes. Despite their broad utility in drug discovery and prot...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "framework",
        "experiment",
        "tool",
        "paper",
        "LLM",
        "reasoning",
        "arxiv",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Parallel Decoder Transformer: Model-Internal Parallel Decoding with Speculative Invariance via Note Conditioning",
      "url": "https://arxiv.org/abs/2512.10054",
      "description": "arXiv:2512.10054v1 Announce Type: new \nAbstract: Autoregressive decoding in Large Language Models (LLMs) is inherently sequential, creating a latency bottleneck that scales linearly with output length. While ``Decomposition-and-Fill'' methods like Skeleton-of-Thought attempt to parallelize generatio...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "large language model",
        "RAG",
        "LLM",
        "transformer",
        "arxiv",
        "model",
        "fine-tuning"
      ],
      "score": 1.0
    },
    {
      "title": "Mind the Gap! Pathways Towards Unifying AI Safety and Ethics Research",
      "url": "https://arxiv.org/abs/2512.10058",
      "description": "arXiv:2512.10058v1 Announce Type: new \nAbstract: While much research in artificial intelligence (AI) has focused on scaling capabilities, the accelerating pace of development makes countervailing work on producing harmless, \"aligned\" systems increasingly urgent. Yet research on alignment has diverge...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "study",
        "paper",
        "alignment",
        "analysis",
        "arxiv",
        "product",
        "research"
      ],
      "score": 1.0
    },
    {
      "title": "BAMBO: Construct Ability and Efficiency LLM Pareto Set via Bayesian Adaptive Multi-objective Block-wise Optimization",
      "url": "https://arxiv.org/abs/2512.09972",
      "description": "arXiv:2512.09972v1 Announce Type: new \nAbstract: Constructing a Pareto set is pivotal for navigating the capability-efficiency trade-offs in Large Language Models (LLMs); however, existing merging techniques remain inadequate for this task. Coarse-grained, model-level methods yield only a sparse set...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "framework",
        "experiment",
        "RAG",
        "LLM",
        "arxiv",
        "model",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "Intelligently Weighting Multiple Reference Models for Direct Preference Optimization of LLMs",
      "url": "https://arxiv.org/abs/2512.10040",
      "description": "arXiv:2512.10040v1 Announce Type: new \nAbstract: Fine-tuning is integral for aligning large language models (LLMs) with human preferences. Multiple-Reference Preference Optimization (MRPO) builds on Direct Preference Optimization (DPO) by fine-tuning LLMs on preference datasets while regularizing th...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "large language model",
        "experiment",
        "RAG",
        "LLM",
        "arxiv",
        "RLHF",
        "model",
        "fine-tuning"
      ],
      "score": 1.0
    },
    {
      "title": "Local LLM Ensembles for Zero-shot Portuguese Named Entity Recognition",
      "url": "https://arxiv.org/abs/2512.10043",
      "description": "arXiv:2512.10043v1 Announce Type: new \nAbstract: Large Language Models (LLMs) excel in many Natural Language Processing (NLP) tasks through in-context learning but often under-perform in Named Entity Recognition (NER), especially for lower-resource languages like Portuguese. While open-weight LLMs e...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "in-context",
        "fine-tuning",
        "RAG",
        "LLM",
        "zero-shot",
        "context",
        "arxiv",
        "model",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "Detailed balance in large language model-driven agents",
      "url": "https://arxiv.org/abs/2512.10047",
      "description": "arXiv:2512.10047v1 Announce Type: new \nAbstract: Large language model (LLM)-driven agents are emerging as a powerful new paradigm for solving complex problems. Despite the empirical success of these practices, a theoretical framework to understand and unify their macroscopic dynamics remains lacking...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "framework",
        "experiment",
        "study",
        "LLM",
        "arxiv",
        "template",
        "prompt",
        "model",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "DB2-TransF: All You Need Is Learnable Daubechies Wavelets for Time Series Forecasting",
      "url": "https://arxiv.org/abs/2512.10051",
      "description": "arXiv:2512.10051v1 Announce Type: new \nAbstract: Time series forecasting requires models that can efficiently capture complex temporal dependencies, especially in large-scale and high-dimensional settings. While Transformer-based architectures excel at modeling long-range dependencies, their quadrat...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "context_management",
      "keywords": [
        "framework",
        "experiment",
        "transformer",
        "arxiv",
        "model",
        "attention",
        "memory"
      ],
      "score": 1.0
    },
    {
      "title": "Context-Engineering - \"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" â€” Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "url": "https://github.com/davidkimai/Context-Engineering",
      "description": "\"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" â€” Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "published_date": "2025-06-29T00:16:36+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "context window",
        "context",
        "prompt engineering",
        "prompt"
      ],
      "score": 1.0
    },
    {
      "title": "ThinkSound - [NeurIPS 2025] PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "url": "https://github.com/FunAudioLLM/ThinkSound",
      "description": "[NeurIPS 2025] PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "published_date": "2025-06-27T02:27:00+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "audio",
        "framework",
        "reasoning",
        "chain-of-thought",
        "CoT"
      ],
      "score": 1.0
    },
    {
      "title": "Cline-Recursive-Chain-of-Thought-System-CRCT- - A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "url": "https://github.com/RPG-fan/Cline-Recursive-Chain-of-Thought-System-CRCT-",
      "description": "A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "published_date": "2025-02-18T15:45:30+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "framework",
        "context",
        "chain-of-thought"
      ],
      "score": 1.0
    },
    {
      "title": "airweave - Context retrieval for AI agents across apps and databases",
      "url": "https://github.com/airweave-ai/airweave",
      "description": "Context retrieval for AI agents across apps and databases",
      "published_date": "2024-12-24T10:00:06+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "retrieval",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "LightRAG - [EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "url": "https://github.com/HKUDS/LightRAG",
      "description": "[EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "published_date": "2024-10-02T11:57:54+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "retrieval",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "KAG - KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "url": "https://github.com/OpenSPG/KAG",
      "description": "KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "published_date": "2024-09-21T13:56:44+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "framework",
        "RAG",
        "LLM",
        "reasoning",
        "retrieval",
        "vector",
        "model",
        "knowledge base"
      ],
      "score": 1.0
    },
    {
      "title": "Kiln - Easily build AI systems with Evals, RAG, Agents, fine-tuning, synthetic data, and more.",
      "url": "https://github.com/Kiln-AI/Kiln",
      "description": "Easily build AI systems with Evals, RAG, Agents, fine-tuning, synthetic data, and more.",
      "published_date": "2024-07-23T23:10:13+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "fine-tuning"
      ],
      "score": 1.0
    },
    {
      "title": "graphrag - A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "url": "https://github.com/microsoft/graphrag",
      "description": "A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "published_date": "2024-03-27T17:57:52+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "retrieval",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "R2R - SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "url": "https://github.com/SciPhi-AI/R2R",
      "description": "SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "published_date": "2024-02-12T03:24:27+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "augmented",
        "retrieval",
        "API",
        "product"
      ],
      "score": 1.0
    },
    {
      "title": "openlit - Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. ðŸš€ðŸ’» Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "url": "https://github.com/openlit/openlit",
      "description": "Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. ðŸš€ðŸ’» Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "published_date": "2024-01-23T17:40:59+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "framework",
        "LLM",
        "platform",
        "vector",
        "prompt"
      ],
      "score": 1.0
    },
    {
      "title": "ExaCraft: Dynamic Learning Context Adaptation for Personalized Educational Examples",
      "url": "https://arxiv.org/abs/2512.09931",
      "description": "arXiv:2512.09931v1 Announce Type: new \nAbstract: Learning is most effective when it's connected to relevant, relatable examples that resonate with learners on a personal level. However, existing educational AI tools don't focus on generating examples or adapting to learners' changing understanding, ...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "in_context_learning",
      "keywords": [
        "tool",
        "analysis",
        "example",
        "context",
        "API",
        "arxiv",
        "demonstration"
      ],
      "score": 0.8
    },
    {
      "title": "SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration",
      "url": "https://arxiv.org/abs/2512.10046",
      "description": "arXiv:2512.10046v1 Announce Type: new \nAbstract: Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scen...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "multimodal_context",
      "keywords": [
        "multimodal",
        "experiment",
        "reasoning",
        "instruction",
        "vision",
        "platform",
        "arxiv",
        "model"
      ],
      "score": 0.8
    },
    {
      "title": "HGC-Herd: Efficient Heterogeneous Graph Condensation via Representative Node Herding",
      "url": "https://arxiv.org/abs/2512.09947",
      "description": "arXiv:2512.09947v1 Announce Type: new \nAbstract: Heterogeneous graph neural networks (HGNNs) have demonstrated strong capability in modeling complex semantics across multi-type nodes and relations. However, their scalability to large-scale graphs remains challenging due to structural redundancy and ...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "framework",
        "experiment",
        "context",
        "arxiv",
        "model",
        "memory"
      ],
      "score": 0.8
    },
    {
      "title": "fastapi_mcp - Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "url": "https://github.com/tadata-org/fastapi_mcp",
      "description": "Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "published_date": "2025-03-08T11:15:43+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "model",
        "API",
        "context",
        "tool"
      ],
      "score": 0.8
    },
    {
      "title": "cosmos-reason1 - Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "url": "https://github.com/nvidia-cosmos/cosmos-reason1",
      "description": "Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "published_date": "2025-03-02T15:23:55+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "model",
        "reasoning",
        "chain-of-thought"
      ],
      "score": 0.8
    },
    {
      "title": "Fuzzy Hierarchical Multiplex",
      "url": "https://arxiv.org/abs/2512.09976",
      "description": "arXiv:2512.09976v1 Announce Type: new \nAbstract: A new fuzzy optimization framework that extends FCM causality is proposed. This model utilizes the dynamics to map data into metrics and create a framework that examines logical implication and hierarchy of concepts using a multiplex. Moreover, this i...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "framework",
        "paper",
        "analysis",
        "arxiv",
        "model"
      ],
      "score": 0.6
    },
    {
      "title": "Cluster-Dags as Powerful Background Knowledge For Causal Discovery",
      "url": "https://arxiv.org/abs/2512.10032",
      "description": "arXiv:2512.10032v1 Announce Type: new \nAbstract: Finding cause-effect relationships is of key importance in science. Causal discovery aims to recover a graph from data that succinctly describes these cause-effect relationships. However, current methods face several challenges, especially when dealin...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "framework",
        "arxiv"
      ],
      "score": 0.6
    },
    {
      "title": "SEMDICE: Off-policy State Entropy Maximization via Stationary Distribution Correction Estimation",
      "url": "https://arxiv.org/abs/2512.10042",
      "description": "arXiv:2512.10042v1 Announce Type: new \nAbstract: In the unsupervised pre-training for reinforcement learning, the agent aims to learn a prior policy for downstream tasks without relying on task-specific reward functions. We focus on state entropy maximization (SEM), where the goal is to learn a poli...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "arxiv",
        "paper"
      ],
      "score": 0.6
    },
    {
      "title": "excel-mcp-server - A Model Context Protocol server for Excel file manipulation",
      "url": "https://github.com/haris-musa/excel-mcp-server",
      "description": "A Model Context Protocol server for Excel file manipulation",
      "published_date": "2025-02-12T06:39:48+00:00",
      "source": "GitHub",
      "category": "industry_news",
      "keywords": [
        "model",
        "context"
      ],
      "score": 0.6
    },
    {
      "title": "mcp-agent - Build effective agents using Model Context Protocol and simple workflow patterns",
      "url": "https://github.com/lastmile-ai/mcp-agent",
      "description": "Build effective agents using Model Context Protocol and simple workflow patterns",
      "published_date": "2024-12-18T01:55:10+00:00",
      "source": "GitHub",
      "category": "industry_news",
      "keywords": [
        "model",
        "context"
      ],
      "score": 0.6
    },
    {
      "title": "AlphaCodium - Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "url": "https://github.com/Codium-ai/AlphaCodium",
      "description": "Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "published_date": "2024-01-14T15:17:18+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "prompt engineering",
        "prompt",
        "paper"
      ],
      "score": 0.6
    },
    {
      "title": "Multilingual VLM Training: Adapting an English-Trained VLM to French",
      "url": "https://arxiv.org/abs/2512.10336",
      "description": "arXiv:2512.10336v1 Announce Type: new \nAbstract: Artificial intelligence has made great progress in recent years, particularly in the development of Vision--Language Models (VLMs) that understand both visual and textual data. However, these advancements remain largely limited to English, reducing th...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "multimodal_context",
      "keywords": [
        "multimodal",
        "paper",
        "vision",
        "arxiv",
        "model"
      ],
      "score": 0.4
    },
    {
      "title": "Latent Action World Models for Control with Unlabeled Trajectories",
      "url": "https://arxiv.org/abs/2512.10016",
      "description": "arXiv:2512.10016v1 Announce Type: new \nAbstract: Inspired by how humans combine direct interaction with action-free experience (e.g., videos), we study world models that learn from heterogeneous data. Standard world models typically rely on action-conditioned trajectories, which limits effectiveness...",
      "published_date": "2025-12-12T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "model",
        "arxiv",
        "study"
      ],
      "score": 0.4
    },
    {
      "title": "optillm - Optimizing inference proxy for LLMs",
      "url": "https://github.com/algorithmicsuperintelligence/optillm",
      "description": "Optimizing inference proxy for LLMs",
      "published_date": "2024-08-22T19:46:07+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "LLM"
      ],
      "score": 0.4
    },
    {
      "title": "New in llama.cpp: Model Management",
      "url": "https://huggingface.co/blog/ggml-org/model-management-in-llamacpp",
      "description": "...",
      "published_date": "2025-12-11T15:47:44",
      "source": "Hugging Face Blog",
      "category": "industry_news",
      "keywords": [
        "model"
      ],
      "score": 0.2
    },
    {
      "title": "Codex is Open Sourcing AI models",
      "url": "https://huggingface.co/blog/hf-skills-training-codex",
      "description": "...",
      "published_date": "2025-12-11T00:00:00",
      "source": "Hugging Face Blog",
      "category": "industry_news",
      "keywords": [
        "model"
      ],
      "score": 0.2
    },
    {
      "title": "Apriel-1.6-15b-Thinker: Cost-efficient Frontier Multimodal Performance",
      "url": "https://huggingface.co/blog/ServiceNow-AI/apriel-1p6-15b-thinker",
      "description": "...",
      "published_date": "2025-12-09T20:06:56",
      "source": "Hugging Face Blog",
      "category": "multimodal_context",
      "keywords": [
        "multimodal"
      ],
      "score": 0.2
    }
  ]
}