<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Context Engineering Daily - In-Context Learning</title>
    <link>https://your-username.github.io/context-engineering-news#in_context_learning</link>
    <description>Latest In-Context Learning news in Context Engineering</description>
    <language>en-us</language>
    <item>
      <title>Liars&amp;#x27; Bench: Evaluating Lie Detectors for Language Models</title>
      <link>https://arxiv.org/abs/2511.16035</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2511.16035</guid>
      <description>arXiv:2511.16035v1 Announce Type: new 
Abstract: Prior work has introduced techniques for detecting when large language models (LLMs) lie, that is, generating statements they believe are false. However, these techniques are typically validated in narrow settings that do not capture the diverse lies ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, large language model, example, arxiv, LLM | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 21 Nov 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>In Context Learning</category>
      <category>model</category>
      <category>large language model</category>
      <category>example</category>
    </item>
    <item>
      <title>Learning Tractable Distributions Of Language Model Continuations</title>
      <link>https://arxiv.org/abs/2511.16054</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2511.16054</guid>
      <description>arXiv:2511.16054v1 Announce Type: new 
Abstract: Controlled language generation conditions text on sequence-level constraints (for example, syntax, style, or safety). These constraints may depend on future tokens, which makes directly conditioning an autoregressive language model (LM) generally intr...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, vision, context, example, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 60%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 21 Nov 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>In Context Learning</category>
      <category>model</category>
      <category>vision</category>
      <category>context</category>
    </item>
    <item>
      <title>Uncertainty-Aware Measurement of Scenario Suite Representativeness for Autonomous Systems</title>
      <link>https://arxiv.org/abs/2511.14853</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2511.14853</guid>
      <description>arXiv:2511.14853v1 Announce Type: new 
Abstract: Assuring the trustworthiness and safety of AI systems, e.g., autonomous vehicles (AV), depends critically on the data-related safety properties, e.g., representativeness, completeness, etc., of the datasets used for their training and testing. Among t...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; arxiv, ICL, paper, example | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 40%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 21 Nov 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>In Context Learning</category>
      <category>arxiv</category>
      <category>ICL</category>
      <category>paper</category>
    </item>
  </channel>
</rss>