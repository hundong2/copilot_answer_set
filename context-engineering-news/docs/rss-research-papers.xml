<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Context Engineering Daily - Research Papers</title>
    <link>https://your-username.github.io/context-engineering-news#research_papers</link>
    <description>Latest Research Papers news in Context Engineering</description>
    <language>en-us</language>
    <item>
      <title>asr_eval: Algorithms and tools for multi-reference and streaming speech recognition evaluation</title>
      <link>https://arxiv.org/abs/2601.20992</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.20992</guid>
      <description>arXiv:2601.20992v1 Announce Type: new 
Abstract: We propose several improvements to the speech recognition evaluation. First, we propose a string alignment algorithm that supports both multi-reference labeling, arbitrary-length insertions and better word alignment. This is especially useful for non-...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; study, model, tool, ICL, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Sat, 31 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>study</category>
      <category>model</category>
      <category>tool</category>
    </item>
    <item>
      <title>UrduBench: An Urdu Reasoning Benchmark using Contextually Ensembled Translations with Human-in-the-Loop</title>
      <link>https://arxiv.org/abs/2601.21000</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.21000</guid>
      <description>arXiv:2601.21000v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have led to strong reasoning capabilities; however, evaluating such models in low-resource languages remains challenging due to the lack of standardized benchmarks. In particular, Urdu reasoning evaluati...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; prompt, LLM, instruction, paper, analysis | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Sat, 31 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>prompt</category>
      <category>LLM</category>
      <category>instruction</category>
    </item>
    <item>
      <title>Position-invariant Fine-tuning of Speech Enhancement Models with Self-supervised Speech Representations</title>
      <link>https://arxiv.org/abs/2601.21084</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.21084</guid>
      <description>arXiv:2601.21084v1 Announce Type: new 
Abstract: Integrating front-end speech enhancement (SE) models with self-supervised learning (SSL)-based speech models is effective for downstream tasks in noisy conditions. SE models are commonly fine-tuned using SSL representations with mean squared error (MS...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, embedding, arxiv, experiment, fine-tuning | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Sat, 31 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>model</category>
      <category>embedding</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>ChunkWise LoRA: Adaptive Sequence Partitioning for Memory-Efficient Low-Rank Adaptation and Accelerated LLM Inference</title>
      <link>https://arxiv.org/abs/2601.21109</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.21109</guid>
      <description>arXiv:2601.21109v1 Announce Type: new 
Abstract: Recent advances in low-rank adaptation (LoRA) have enabled efficient fine-tuning of large language models (LLMs) with minimal additional parameters. However, existing LoRA methods apply static rank configurations uniformly across all input tokens, ign...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; LLM, model, large language model, transformer, framework | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Sat, 31 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>LLM</category>
      <category>model</category>
      <category>large language model</category>
    </item>
    <item>
      <title>Multi-task Code LLMs: Data Mix or Model Merge?</title>
      <link>https://arxiv.org/abs/2601.21115</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.21115</guid>
      <description>arXiv:2601.21115v1 Announce Type: new 
Abstract: Recent research advocates deploying smaller, specialized code LLMs in agentic frameworks alongside frontier models, sparking interest in efficient strategies for multi-task learning that balance performance, constraints, and costs. We compare two appr...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; LLM, model, analysis, framework, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Sat, 31 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>LLM</category>
      <category>model</category>
      <category>analysis</category>
    </item>
    <item>
      <title>From Linear Input to Hierarchical Structure: Function Words as Statistical Cues for Language Learning</title>
      <link>https://arxiv.org/abs/2601.21191</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.21191</guid>
      <description>arXiv:2601.21191v1 Announce Type: new 
Abstract: What statistical conditions support learning hierarchical structure from linear input? In this paper, we address this question by focusing on the statistical distribution of function words. Function words have long been argued to play a crucial role i...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, analysis, paper, arxiv, alignment | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Sat, 31 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>model</category>
      <category>analysis</category>
      <category>paper</category>
    </item>
    <item>
      <title>EnsembleLink: Accurate Record Linkage Without Training Data</title>
      <link>https://arxiv.org/abs/2601.21138</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.21138</guid>
      <description>arXiv:2601.21138v1 Announce Type: new 
Abstract: Record linkage, the process of matching records that refer to the same entity across datasets, is essential to empirical social science but remains methodologically underdeveloped. Researchers treat it as a preprocessing step, applying ad hoc rules wi...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, RAG, API, arxiv, research | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 80%&amp;lt;/small&amp;gt;</description>
      <pubDate>Sat, 31 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>model</category>
      <category>RAG</category>
      <category>API</category>
    </item>
  </channel>
</rss>