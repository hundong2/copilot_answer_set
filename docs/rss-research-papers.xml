<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Context Engineering Daily - Research Papers</title>
    <link>https://your-username.github.io/context-engineering-news#research_papers</link>
    <description>Latest Research Papers news in Context Engineering</description>
    <language>en-us</language>
    <item>
      <title>Language Ranker: A Lightweight Ranking framework for LLM Decoding</title>
      <link>https://arxiv.org/abs/2510.21883</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.21883</guid>
      <description>arXiv:2510.21883v1 Announce Type: new 
Abstract: Conventional research on large language models (LLMs) has primarily focused on refining output distributions, while paying less attention to the decoding process that transforms these distributions into final responses. Recent advances, such as scalin...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; LLM, model, attention, experiment, paper | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 28 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>LLM</category>
      <category>model</category>
      <category>attention</category>
    </item>
    <item>
      <title>Framework for Machine Evaluation of Reasoning Completeness in Large Language Models For Classification Tasks</title>
      <link>https://arxiv.org/abs/2510.21884</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.21884</guid>
      <description>arXiv:2510.21884v1 Announce Type: new 
Abstract: The growing adoption of machine learning (ML) in sensitive domains has heightened the demand for transparent and interpretable artificial intelligence. Large Language Models (LLMs) are increasingly capable of producing natural language explanations, y...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; LLM, model, paper, large language model, RAG | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 28 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>LLM</category>
      <category>model</category>
      <category>paper</category>
    </item>
    <item>
      <title>Understanding Network Behaviors through Natural Language Question-Answering</title>
      <link>https://arxiv.org/abs/2510.21894</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.21894</guid>
      <description>arXiv:2510.21894v1 Announce Type: new 
Abstract: Modern large-scale networks introduce significant complexity in understanding network behaviors, increasing the risk of misconfiguration. Prior work proposed to understand network behaviors by mining network configurations, typically relying on domain...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; LLM, model, experiment, research, large language model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 28 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>LLM</category>
      <category>model</category>
      <category>experiment</category>
    </item>
    <item>
      <title>Deep Literature Survey Automation with an Iterative Workflow</title>
      <link>https://arxiv.org/abs/2510.21900</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.21900</guid>
      <description>arXiv:2510.21900v1 Announce Type: new 
Abstract: Automatic literature survey generation has attracted increasing attention, yet most existing systems follow a one-shot paradigm, where a large set of papers is retrieved at once and a static outline is generated before drafting. This design often lead...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; attention, experiment, retrieval, paper, research | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 28 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>attention</category>
      <category>experiment</category>
      <category>retrieval</category>
    </item>
    <item>
      <title>A Multi-Component AI Framework for Computational Psychology: From Robust Predictive Modeling to Deployed Generative Dialogue</title>
      <link>https://arxiv.org/abs/2510.21720</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.21720</guid>
      <description>arXiv:2510.21720v1 Announce Type: new 
Abstract: The confluence of Artificial Intelligence and Computational Psychology presents an opportunity to model, understand, and interact with complex human psychological states through computational means. This paper presents a comprehensive, multi-faceted f...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; LLM, model, transformer, paper, research | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 28 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>LLM</category>
      <category>model</category>
      <category>transformer</category>
    </item>
    <item>
      <title>PREFINE: Personalized Story Generation via Simulated User Critics and User-Specific Rubric Generation</title>
      <link>https://arxiv.org/abs/2510.21721</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.21721</guid>
      <description>arXiv:2510.21721v1 Announce Type: new 
Abstract: While recent advances in Large Language Models (LLMs) have improved the quality of creative text generation, significant challenges remain in producing personalized stories that reflect individual user preferences. Conventional approaches rely on expl...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; LLM, model, fine-tuning, large language model, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 28 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>LLM</category>
      <category>model</category>
      <category>fine-tuning</category>
    </item>
    <item>
      <title>SIGN: Schema-Induced Games for Naming</title>
      <link>https://arxiv.org/abs/2510.21855</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.21855</guid>
      <description>arXiv:2510.21855v1 Announce Type: new 
Abstract: Real-world AI systems are tackling increasingly complex problems, often through interactions among large language model (LLM) agents. When these agents develop inconsistent conventions, coordination can break down. Applications such as collaborative c...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; large language model, LLM, arxiv, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 28 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>large language model</category>
      <category>LLM</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>Capability Ceilings in Autoregressive Language Models: Empirical Evidence from Knowledge-Intensive Tasks</title>
      <link>https://arxiv.org/abs/2510.21866</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.21866</guid>
      <description>arXiv:2510.21866v1 Announce Type: new 
Abstract: We document empirical capability ceilings in decoder-only autoregressive language models across knowledge-intensive tasks. Systematic evaluation of OPT and Pythia model families (70M-30B parameters, spanning 240 times scaling) reveals that knowledge r...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, attention, experiment, retrieval, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 28 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>model</category>
      <category>attention</category>
      <category>experiment</category>
    </item>
    <item>
      <title>Performance Trade-offs of Optimizing Small Language Models for E-Commerce</title>
      <link>https://arxiv.org/abs/2510.21970</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.21970</guid>
      <description>arXiv:2510.21970v1 Announce Type: new 
Abstract: Large Language Models (LLMs) offer state-of-the-art performance in natural language understanding and generation tasks. However, the deployment of leading commercial models for specialized tasks, such as e-commerce, is often hindered by high computati...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; LLM, model, paper, large language model, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 28 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>LLM</category>
      <category>model</category>
      <category>paper</category>
    </item>
    <item>
      <title>Foundation of Intelligence: Review of Math Word Problems from Human Cognition Perspective</title>
      <link>https://arxiv.org/abs/2510.21999</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.21999</guid>
      <description>arXiv:2510.21999v1 Announce Type: new 
Abstract: Math word problem (MWP) serves as a fundamental research topic in artificial intelligence (AI) dating back to 1960s. This research aims to advance the reasoning abilities of AI by mirroring the human-like cognitive intelligence. The mainstream technol...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; LLM, API, model, paper, release | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 28 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>LLM</category>
      <category>API</category>
      <category>model</category>
    </item>
    <item>
      <title>MOBO-OSD: Batch Multi-Objective Bayesian Optimization via Orthogonal Search Directions</title>
      <link>https://arxiv.org/abs/2510.20872</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.20872</guid>
      <description>arXiv:2510.20872v1 Announce Type: new 
Abstract: Bayesian Optimization (BO) is a powerful tool for optimizing expensive black-box objective functions. While extensive research has been conducted on the single-objective optimization problem, the multi-objective optimization problem remains challengin...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; tool, experiment, paper, research, RAG | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 28 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>tool</category>
      <category>experiment</category>
      <category>paper</category>
    </item>
    <item>
      <title>Meta-Learning for Cross-Task Generalization in Protein Mutation Property Prediction</title>
      <link>https://arxiv.org/abs/2510.20943</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.20943</guid>
      <description>arXiv:2510.20943v1 Announce Type: new 
Abstract: Protein mutations can have profound effects on biological function, making accurate prediction of property changes critical for drug discovery, protein engineering, and precision medicine. Current approaches rely on fine-tuning protein-specific transf...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; API, model, fine-tuning, transformer, experiment | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 28 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>API</category>
      <category>model</category>
      <category>fine-tuning</category>
    </item>
    <item>
      <title>LLM-Integrated Bayesian State Space Models for Multimodal Time-Series Forecasting</title>
      <link>https://arxiv.org/abs/2510.20952</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.20952</guid>
      <description>arXiv:2510.20952v1 Announce Type: new 
Abstract: Forecasting in the real world requires integrating structured time-series data with unstructured textual information, but existing methods are architecturally limited by fixed input/output horizons and are unable to model or quantify uncertainty. We a...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; LLM, model, experiment, large language model, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 28 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>LLM</category>
      <category>model</category>
      <category>experiment</category>
    </item>
    <item>
      <title>Computational Hardness of Reinforcement Learning with Partial $q^{\pi}$-Realizability</title>
      <link>https://arxiv.org/abs/2510.21888</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.21888</guid>
      <description>arXiv:2510.21888v1 Announce Type: new 
Abstract: This paper investigates the computational complexity of reinforcement learning in a novel linear function approximation regime, termed partial $q^{\pi}$-realizability. In this framework, the objective is to learn an $\epsilon$-optimal policy with resp...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, paper, arxiv, vector, framework | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 80%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 28 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>model</category>
      <category>paper</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>Global Dynamics of Heavy-Tailed SGDs in Nonconvex Loss Landscape: Characterization and Control</title>
      <link>https://arxiv.org/abs/2510.20905</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.20905</guid>
      <description>arXiv:2510.20905v1 Announce Type: new 
Abstract: Stochastic gradient descent (SGD) and its variants enable modern artificial intelligence. However, theoretical understanding lags far behind their empirical success. It is widely believed that SGD has a curious ability to avoid sharp local minima in t...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; experiment, arxiv, analysis, paper | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 80%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 28 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>experiment</category>
      <category>arxiv</category>
      <category>analysis</category>
    </item>
    <item>
      <title>Learning from Interval Targets</title>
      <link>https://arxiv.org/abs/2510.20925</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.20925</guid>
      <description>arXiv:2510.20925v1 Announce Type: new 
Abstract: We study the problem of regression with interval targets, where only upper and lower bounds on target values are available in the form of intervals. This problem arises when the exact target label is expensive or impossible to obtain, due to inherent ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; experiment, arxiv, study | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 60%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 28 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>experiment</category>
      <category>arxiv</category>
      <category>study</category>
    </item>
    <item>
      <title>Explaining and Mitigating Crosslingual Tokenizer Inequities</title>
      <link>https://arxiv.org/abs/2510.21909</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.21909</guid>
      <description>arXiv:2510.21909v1 Announce Type: new 
Abstract: The number of tokens it takes to encode parallel text in different languages is known to vary. These disparities are called token premiums. Having high token premiums leads to less throughput during training and increases costs at inference. In this p...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; arxiv, compression, paper | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 40%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 28 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>arxiv</category>
      <category>compression</category>
      <category>paper</category>
    </item>
    <item>
      <title>Exploration through Generation: Applying GFlowNets to Structured Search</title>
      <link>https://arxiv.org/abs/2510.21886</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.21886</guid>
      <description>arXiv:2510.21886v1 Announce Type: new 
Abstract: This work applies Generative Flow Networks (GFlowNets) to three graph optimization problems: the Traveling Salesperson Problem, Minimum Spanning Tree, and Shortest Path. GFlowNets are generative models that learn to sample solutions proportionally to ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; arxiv, experiment, framework, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 40%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 28 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>arxiv</category>
      <category>experiment</category>
      <category>framework</category>
    </item>
    <item>
      <title>Multimodal Negative Learning</title>
      <link>https://arxiv.org/abs/2510.20877</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2510.20877</guid>
      <description>arXiv:2510.20877v1 Announce Type: new 
Abstract: Multimodal learning systems often encounter challenges related to modality imbalance, where a dominant modality may overshadow others, thereby hindering the learning of weak modalities. Conventional approaches often force weak modalities to align with...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; arxiv, multimodal, experiment, framework | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 40%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 28 Oct 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>arxiv</category>
      <category>multimodal</category>
      <category>experiment</category>
    </item>
  </channel>
</rss>