<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Context Engineering Daily - Multimodal Context</title>
    <link>https://your-username.github.io/context-engineering-news#multimodal_context</link>
    <description>Latest Multimodal Context news in Context Engineering</description>
    <language>en-us</language>
    <item>
      <title>Imagine in Space: Exploring the Frontier of Spatial Intelligence and Reasoning Efficiency in Vision Language Models</title>
      <link>https://arxiv.org/abs/2511.13782</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2511.13782</guid>
      <description>arXiv:2511.13782v1 Announce Type: new 
Abstract: Large language models (LLMs) and vision language models (VLMs), such as DeepSeek R1,OpenAI o3, and Gemini 2.5 Pro, have demonstrated remarkable reasoning capabilities across logical inference, problem solving, and decision making. However, spatial rea...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; large language model, API, arxiv, image, reasoning | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 19 Nov 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Multimodal Context</category>
      <category>large language model</category>
      <category>API</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>Jailbreaking Large Vision Language Models in Intelligent Transportation Systems</title>
      <link>https://arxiv.org/abs/2511.13892</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2511.13892</guid>
      <description>arXiv:2511.13892v1 Announce Type: new 
Abstract: Large Vision Language Models (LVLMs) demonstrate strong capabilities in multimodal reasoning and many real-world applications, such as visual question answering. However, LVLMs are highly vulnerable to jailbreaking attacks. This paper systematically a...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; paper, GPT, arxiv, reasoning, image | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 19 Nov 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Multimodal Context</category>
      <category>paper</category>
      <category>GPT</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>Scene Graph-Guided Generative AI Framework for Synthesizing and Evaluating Industrial Hazard Scenarios</title>
      <link>https://arxiv.org/abs/2511.13970</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2511.13970</guid>
      <description>arXiv:2511.13970v1 Announce Type: new 
Abstract: Training vision models to detect workplace hazards accurately requires realistic images of unsafe conditions that could lead to accidents. However, acquiring such datasets is difficult because capturing accident-triggering scenarios as they occur is n...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; GPT, arxiv, reasoning, image, study | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 19 Nov 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Multimodal Context</category>
      <category>GPT</category>
      <category>arxiv</category>
      <category>reasoning</category>
    </item>
    <item>
      <title>Adaptive Redundancy Regulation for Balanced Multimodal Information Refinement</title>
      <link>https://arxiv.org/abs/2511.13755</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2511.13755</guid>
      <description>arXiv:2511.13755v1 Announce Type: new 
Abstract: Multimodal learning aims to improve performance by leveraging data from multiple sources. During joint multimodal training, due to modality bias, the advantaged modality often dominates backpropagation, leading to imbalanced optimization. Existing met...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; RAG, experiment, arxiv, cross-modal, multimodal | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 80%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 19 Nov 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Multimodal Context</category>
      <category>RAG</category>
      <category>experiment</category>
      <category>arxiv</category>
    </item>
  </channel>
</rss>