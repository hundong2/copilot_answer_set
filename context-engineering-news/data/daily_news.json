{
  "generated_at": "2025-12-18T20:05:48.901149",
  "total_items": 47,
  "items": [
    {
      "title": "Incentives or Ontology? A Structural Rebuttal to OpenAI's Hallucination Thesis",
      "url": "https://arxiv.org/abs/2512.14801",
      "description": "arXiv:2512.14801v1 Announce Type: new \nAbstract: OpenAI has recently argued that hallucinations in large language models result primarily from misaligned evaluation incentives that reward confident guessing rather than epistemic humility. On this view, hallucination is a contingent behavioral artifa...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "transformer",
        "experiment",
        "paper",
        "model",
        "fine-tuning",
        "prompting",
        "large language model",
        "prompt",
        "arxiv",
        "embedding"
      ],
      "score": 1.0
    },
    {
      "title": "T5Gemma 2: Seeing, Reading, and Understanding Longer",
      "url": "https://arxiv.org/abs/2512.14856",
      "description": "arXiv:2512.14856v1 Announce Type: new \nAbstract: We introduce T5Gemma 2, the next generation of the T5Gemma family of lightweight open encoder-decoder models, featuring strong multilingual, multimodal and long-context capabilities. T5Gemma 2 follows the adaptation recipe (via UL2) in T5Gemma -- adap...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "release",
        "research",
        "attention",
        "experiment",
        "model",
        "multimodal",
        "arxiv",
        "context",
        "embedding"
      ],
      "score": 1.0
    },
    {
      "title": "Integrating Large Language Models and Knowledge Graphs to Capture Political Viewpoints in News Media",
      "url": "https://arxiv.org/abs/2512.14887",
      "description": "arXiv:2512.14887v1 Announce Type: new \nAbstract: News sources play a central role in democratic societies by shaping political and social discourse through specific topics, viewpoints and voices. Understanding these dynamics is essential for assessing whether the media landscape offers a balanced an...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "paper",
        "model",
        "fine-tuning",
        "API",
        "large language model",
        "arxiv"
      ],
      "score": 1.0
    },
    {
      "title": "DrugRAG: Enhancing Pharmacy LLM Performance Through A Novel Retrieval-Augmented Generation Pipeline",
      "url": "https://arxiv.org/abs/2512.14896",
      "description": "arXiv:2512.14896v1 Announce Type: new \nAbstract: Objectives: To evaluate large language model (LLM) performance on pharmacy licensure-style question-answering (QA) tasks and develop an external knowledge integration method to improve their accuracy.\n  Methods: We benchmarked eleven existing LLMs wit...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "LLM",
        "retrieval",
        "RAG",
        "augmented",
        "model",
        "large language model",
        "prompt",
        "arxiv",
        "context",
        "GPT"
      ],
      "score": 1.0
    },
    {
      "title": "Multiscale Aggregated Hierarchical Attention (MAHA): A Game Theoretic and Optimization Driven Approach to Efficient Contextual Modeling in Large Language Models",
      "url": "https://arxiv.org/abs/2512.14925",
      "description": "arXiv:2512.14925v1 Announce Type: new \nAbstract: The quadratic computational complexity of MultiHead SelfAttention (MHSA) remains a fundamental bottleneck in scaling Large Language Models (LLMs) for longcontext tasks. While sparse and linearized attention mechanisms attempt to mitigate this, they of...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "transformer",
        "attention",
        "framework",
        "experiment",
        "paper",
        "analysis",
        "model",
        "large language model",
        "arxiv",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "Parameter Efficient Multimodal Instruction Tuning for Romanian Vision Language Models",
      "url": "https://arxiv.org/abs/2512.14926",
      "description": "arXiv:2512.14926v1 Announce Type: new \nAbstract: Focusing on low-resource languages is an essential step toward democratizing generative AI. In this work, we contribute to reducing the multimodal NLP resource gap for Romanian. We translate the widely known Flickr30k dataset into Romanian and further...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "multimodal_context",
      "keywords": [
        "LLM",
        "RAG",
        "image",
        "model",
        "fine-tuning",
        "multimodal",
        "arxiv",
        "instruction",
        "vision"
      ],
      "score": 1.0
    },
    {
      "title": "Cross-Tokenizer Likelihood Scoring Algorithms for Language Model Distillation",
      "url": "https://arxiv.org/abs/2512.14954",
      "description": "arXiv:2512.14954v1 Announce Type: new \nAbstract: Computing next-token likelihood ratios between two language models (LMs) is a standard task in training paradigms such as knowledge distillation. Since this requires both models to share the same probability space, it becomes challenging when the teac...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "chain_of_thought",
      "keywords": [
        "reasoning",
        "RAG",
        "alignment",
        "framework",
        "model",
        "memory",
        "arxiv"
      ],
      "score": 1.0
    },
    {
      "title": "Evaluating Large Language Models on Multimodal Chemistry Olympiad Exams",
      "url": "https://arxiv.org/abs/2512.14989",
      "description": "arXiv:2512.14989v1 Announce Type: new \nAbstract: Multimodal scientific reasoning remains a significant challenge for large language models (LLMs), particularly in chemistry, where problem-solving relies on symbolic diagrams, molecular structures, and structured visual data. Here, we systematically e...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "multimodal_context",
      "keywords": [
        "LLM",
        "chain-of-thought",
        "reasoning",
        "vision",
        "arxiv",
        "alignment",
        "image",
        "model",
        "prompting",
        "large language model",
        "prompt",
        "multimodal",
        "GPT"
      ],
      "score": 1.0
    },
    {
      "title": "DASH: Dialogue-Aware Similarity and Handshake Recognition for Topic Segmentation in Public-Channel Conversations",
      "url": "https://arxiv.org/abs/2512.15042",
      "description": "arXiv:2512.15042v1 Announce Type: new \nAbstract: Dialogue Topic Segmentation (DTS) is crucial for understanding task-oriented public-channel communications, such as maritime VHF dialogues, which feature informal speech and implicit transitions. To address the limitations of traditional methods, we p...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "reasoning",
        "release",
        "research",
        "framework",
        "experiment",
        "model",
        "arxiv",
        "context",
        "example"
      ],
      "score": 1.0
    },
    {
      "title": "SGM: Safety Glasses for Multimodal Large Language Models via Neuron-Level Detoxification",
      "url": "https://arxiv.org/abs/2512.15052",
      "description": "arXiv:2512.15052v1 Announce Type: new \nAbstract: Disclaimer: Samples in this paper may be harmful and cause discomfort.\n  Multimodal large language models (MLLMs) enable multimodal generation but inherit toxic, biased, and NSFW signals from weakly curated pretraining corpora, causing safety risks, e...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "reasoning",
        "framework",
        "experiment",
        "paper",
        "model",
        "cross-modal",
        "large language model",
        "multimodal",
        "arxiv"
      ],
      "score": 1.0
    },
    {
      "title": "Leveraging LLMs for Structured Data Extraction from Unstructured Patient Records",
      "url": "https://arxiv.org/abs/2512.13700",
      "description": "arXiv:2512.13700v1 Announce Type: new \nAbstract: Manual chart review remains an extremely time-consuming and resource-intensive component of clinical research, requiring experts to extract often complex information from unstructured electronic health record (EHR) narratives. We present a secure, mod...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "LLM",
        "retrieval",
        "RAG",
        "research",
        "augmented",
        "framework",
        "model",
        "large language model",
        "arxiv"
      ],
      "score": 1.0
    },
    {
      "title": "Blind Radio Mapping via Spatially Regularized Bayesian Trajectory Inference",
      "url": "https://arxiv.org/abs/2512.13701",
      "description": "arXiv:2512.13701v1 Announce Type: new \nAbstract: Radio maps enable intelligent wireless applications by capturing the spatial distribution of channel characteristics. However, conventional construction methods demand extensive location-labeled data, which are costly and impractical in many real-worl...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "RAG",
        "framework",
        "experiment",
        "paper",
        "model",
        "arxiv",
        "vision"
      ],
      "score": 1.0
    },
    {
      "title": "Adjudicator: Correcting Noisy Labels with a KG-Informed Council of LLM Agents",
      "url": "https://arxiv.org/abs/2512.13704",
      "description": "arXiv:2512.13704v1 Announce Type: new \nAbstract: The performance of production machine learning systems is fundamentally limited by the quality of their training data. In high-stakes industrial applications, noisy labels can degrade performance and erode user trust. This paper presents Adjudicator, ...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "paper",
        "analysis",
        "model",
        "large language model",
        "arxiv",
        "context",
        "product"
      ],
      "score": 1.0
    },
    {
      "title": "LoopBench: Discovering Emergent Symmetry Breaking Strategies with LLM Swarms",
      "url": "https://arxiv.org/abs/2512.13713",
      "description": "arXiv:2512.13713v1 Announce Type: new \nAbstract: Large Language Models (LLMs) are increasingly being utilized as autonomous agents, yet their ability to coordinate in distributed systems remains poorly understood. We introduce \\textbf{LoopBench}, a benchmark to evaluate LLM reasoning in distributed ...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "reasoning",
        "model",
        "memory",
        "large language model",
        "arxiv",
        "study"
      ],
      "score": 1.0
    },
    {
      "title": "AI-Powered Annotation Pipelines for Stabilizing Large Language Models: A Human-AI Synergy Approach",
      "url": "https://arxiv.org/abs/2512.13714",
      "description": "arXiv:2512.13714v1 Announce Type: new \nAbstract: LLM implementations are failing in highly regulated industries owing to instability issues, inconsistent reasoning, hallucinations and performance variability, especially in workflows. These reliability issues restrict safe use of LLM in areas that ne...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "reasoning",
        "framework",
        "paper",
        "model",
        "fine-tuning",
        "large language model",
        "arxiv",
        "RLHF",
        "vision"
      ],
      "score": 1.0
    },
    {
      "title": "ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making",
      "url": "https://arxiv.org/abs/2512.13716",
      "description": "arXiv:2512.13716v1 Announce Type: new \nAbstract: Personalized decision-making is essential for human-AI interaction, enabling AI agents to act in alignment with individual users' value preferences. As AI systems expand into real-world applications, adapting to personalized values beyond task complet...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "LLM",
        "alignment",
        "framework",
        "tool",
        "arxiv",
        "context",
        "GPT"
      ],
      "score": 1.0
    },
    {
      "title": "Compressed Causal Reasoning: Quantization and GraphRAG Effects on Interventional and Counterfactual Accuracy",
      "url": "https://arxiv.org/abs/2512.13725",
      "description": "arXiv:2512.13725v1 Announce Type: new \nAbstract: Causal reasoning in Large Language Models spanning association, intervention, and counterfactual inference is essential for reliable decision making in high stakes settings. As deployment shifts toward edge and resource constrained environments, quant...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "reasoning",
        "retrieval",
        "RAG",
        "augmented",
        "compression",
        "experiment",
        "model",
        "large language model",
        "arxiv",
        "study"
      ],
      "score": 1.0
    },
    {
      "title": "State-Dependent Refusal and Learned Incapacity in RLHF-Aligned Language Models",
      "url": "https://arxiv.org/abs/2512.13762",
      "description": "arXiv:2512.13762v1 Announce Type: new \nAbstract: Large language models (LLMs) are widely deployed as general-purpose tools, yet extended interaction can reveal behavioral patterns not captured by standard quantitative benchmarks. We present a qualitative case-study methodology for auditing policy-li...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "LLM",
        "alignment",
        "study",
        "framework",
        "model",
        "large language model",
        "tool",
        "arxiv",
        "RLHF",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts",
      "url": "https://arxiv.org/abs/2512.14706",
      "description": "arXiv:2512.14706v1 Announce Type: new \nAbstract: Neural architecture search (NAS) traditionally requires significant human expertise or automated trial-and-error to design deep learning models. We present NN-Caption, an LLM-guided neural architecture search pipeline that generates runnable image-cap...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "LLM",
        "template",
        "transformer",
        "image",
        "research",
        "model",
        "API",
        "prompt",
        "arxiv",
        "example"
      ],
      "score": 1.0
    },
    {
      "title": "Autonomous Source Knowledge Selection in Multi-Domain Adaptation",
      "url": "https://arxiv.org/abs/2512.14710",
      "description": "arXiv:2512.14710v1 Announce Type: new \nAbstract: Unsupervised multi-domain adaptation plays a key role in transfer learning by leveraging acquired rich source information from multiple source domains to solve target task from an unlabeled target domain. However, multiple source domains often contain...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "RAG",
        "experiment",
        "paper",
        "model",
        "multimodal",
        "arxiv",
        "vision"
      ],
      "score": 1.0
    },
    {
      "title": "SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI",
      "url": "https://arxiv.org/abs/2512.14712",
      "description": "arXiv:2512.14712v1 Announce Type: new \nAbstract: Sepsis accounts for nearly 20% of global ICU admissions, yet conventional prediction models often fail to effectively integrate heterogeneous data streams, remaining either siloed by modality or reliant on brittle early fusion. In this work, we presen...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "attention",
        "framework",
        "experiment",
        "analysis",
        "model",
        "cross-modal",
        "arxiv",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "Improving Underwater Acoustic Classification Through Learnable Gabor Filter Convolution and Attention Mechanisms",
      "url": "https://arxiv.org/abs/2512.14714",
      "description": "arXiv:2512.14714v1 Announce Type: new \nAbstract: Remotely detecting and classifying underwater acoustic targets is critical for environmental monitoring and defence. However, the complex nature of ship-radiated and environmental underwater noise poses significant challenges to accurate signal proces...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "attention",
        "experiment",
        "paper",
        "model",
        "arxiv"
      ],
      "score": 1.0
    },
    {
      "title": "How a Bit Becomes a Story: Semantic Steering via Differentiable Fault Injection",
      "url": "https://arxiv.org/abs/2512.14715",
      "description": "arXiv:2512.14715v1 Announce Type: new \nAbstract: Hard-to-detect hardware bit flips, from either malicious circuitry or bugs, have already been shown to make transformers vulnerable in non-generative tasks. This work, for the first time, investigates how low-level, bitwise perturbations (fault inject...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "multimodal_context",
      "keywords": [
        "LLM",
        "image",
        "transformer",
        "framework",
        "analysis",
        "model",
        "large language model",
        "arxiv",
        "vision"
      ],
      "score": 1.0
    },
    {
      "title": "Is GPT-OSS All You Need? Benchmarking Large Language Models for Financial Intelligence and the Surprising Efficiency Paradox",
      "url": "https://arxiv.org/abs/2512.14717",
      "description": "arXiv:2512.14717v1 Announce Type: new \nAbstract: The rapid adoption of large language models in financial services necessitates rigorous evaluation frameworks to assess their performance, efficiency, and practical applicability. This paper conducts a comprehensive evaluation of the GPT-OSS model fam...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "LLM",
        "framework",
        "experiment",
        "paper",
        "analysis",
        "model",
        "API",
        "large language model",
        "arxiv",
        "GPT",
        "product"
      ],
      "score": 1.0
    },
    {
      "title": "SEED: Spectral Entropy-Guided Evaluation of SpatialTemporal Dependencies for Multivariate Time Series Forecasting",
      "url": "https://arxiv.org/abs/2512.14718",
      "description": "arXiv:2512.14718v1 Announce Type: new \nAbstract: Effective multivariate time series forecasting often benefits from accurately modeling complex inter-variable dependencies. However, existing attention- or graph-based methods face three key issues: (a) strong temporal self-dependencies are often disr...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "RAG",
        "attention",
        "framework",
        "experiment",
        "model",
        "arxiv",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "Hybrid Attribution Priors for Explainable and Robust Model Training",
      "url": "https://arxiv.org/abs/2512.14719",
      "description": "arXiv:2512.14719v1 Announce Type: new \nAbstract: Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effect...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "RAG",
        "framework",
        "experiment",
        "analysis",
        "model",
        "few-shot",
        "arxiv",
        "vision"
      ],
      "score": 1.0
    },
    {
      "title": "Context-Engineering - \"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" â€” Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "url": "https://github.com/davidkimai/Context-Engineering",
      "description": "\"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" â€” Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "published_date": "2025-06-29T00:16:36+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "context window",
        "prompt engineering",
        "context",
        "prompt"
      ],
      "score": 1.0
    },
    {
      "title": "ThinkSound - [NeurIPS 2025] PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "url": "https://github.com/FunAudioLLM/ThinkSound",
      "description": "[NeurIPS 2025] PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "published_date": "2025-06-27T02:27:00+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "reasoning",
        "chain-of-thought",
        "audio",
        "CoT",
        "framework"
      ],
      "score": 1.0
    },
    {
      "title": "Cline-Recursive-Chain-of-Thought-System-CRCT- - A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "url": "https://github.com/RPG-fan/Cline-Recursive-Chain-of-Thought-System-CRCT-",
      "description": "A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "published_date": "2025-02-18T15:45:30+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "chain-of-thought",
        "context",
        "framework"
      ],
      "score": 1.0
    },
    {
      "title": "airweave - Context retrieval for AI agents across apps and databases",
      "url": "https://github.com/airweave-ai/airweave",
      "description": "Context retrieval for AI agents across apps and databases",
      "published_date": "2024-12-24T10:00:06+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "retrieval",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "LightRAG - [EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "url": "https://github.com/HKUDS/LightRAG",
      "description": "[EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "published_date": "2024-10-02T11:57:54+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "retrieval",
        "augmented",
        "RAG"
      ],
      "score": 1.0
    },
    {
      "title": "KAG - KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "url": "https://github.com/OpenSPG/KAG",
      "description": "KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "published_date": "2024-09-21T13:56:44+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "LLM",
        "retrieval",
        "reasoning",
        "RAG",
        "knowledge base",
        "framework",
        "model",
        "vector"
      ],
      "score": 1.0
    },
    {
      "title": "Kiln - Easily build AI systems with Evals, RAG, Agents, fine-tuning, synthetic data, and more.",
      "url": "https://github.com/Kiln-AI/Kiln",
      "description": "Easily build AI systems with Evals, RAG, Agents, fine-tuning, synthetic data, and more.",
      "published_date": "2024-07-23T23:10:13+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "fine-tuning",
        "RAG"
      ],
      "score": 1.0
    },
    {
      "title": "graphrag - A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "url": "https://github.com/microsoft/graphrag",
      "description": "A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "published_date": "2024-03-27T17:57:52+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "retrieval",
        "augmented",
        "RAG"
      ],
      "score": 1.0
    },
    {
      "title": "R2R - SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "url": "https://github.com/SciPhi-AI/R2R",
      "description": "SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "published_date": "2024-02-12T03:24:27+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "retrieval",
        "RAG",
        "augmented",
        "API",
        "product"
      ],
      "score": 1.0
    },
    {
      "title": "openlit - Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. ðŸš€ðŸ’» Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "url": "https://github.com/openlit/openlit",
      "description": "Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. ðŸš€ðŸ’» Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "published_date": "2024-01-23T17:40:59+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "LLM",
        "framework",
        "vector",
        "prompt",
        "platform"
      ],
      "score": 1.0
    },
    {
      "title": "A Bayesian latent class reinforcement learning framework to capture adaptive, feedback-driven travel behaviour",
      "url": "https://arxiv.org/abs/2512.14713",
      "description": "arXiv:2512.14713v1 Announce Type: new \nAbstract: Many travel decisions involve a degree of experience formation, where individuals learn their preferences over time. At the same time, there is extensive scope for heterogeneity across individual travellers, both in their underlying preferences and in...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "framework",
        "paper",
        "model",
        "arxiv",
        "context"
      ],
      "score": 0.8
    },
    {
      "title": "fastapi_mcp - Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "url": "https://github.com/tadata-org/fastapi_mcp",
      "description": "Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "published_date": "2025-03-08T11:15:43+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "model",
        "context",
        "tool",
        "API"
      ],
      "score": 0.8
    },
    {
      "title": "cosmos-reason1 - Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "url": "https://github.com/nvidia-cosmos/cosmos-reason1",
      "description": "Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "published_date": "2025-03-02T15:23:55+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "reasoning",
        "chain-of-thought",
        "model"
      ],
      "score": 0.8
    },
    {
      "title": "Meta Hierarchical Reinforcement Learning for Scalable Resource Management in O-RAN",
      "url": "https://arxiv.org/abs/2512.13715",
      "description": "arXiv:2512.13715v1 Announce Type: new \nAbstract: The increasing complexity of modern applications demands wireless networks capable of real time adaptability and efficient resource management. The Open Radio Access Network (O-RAN) architecture, with its RAN Intelligent Controller (RIC) modules, has ...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "framework",
        "paper",
        "analysis",
        "model",
        "arxiv"
      ],
      "score": 0.6
    },
    {
      "title": "Automatic Extraction of Rules for Generating Synthetic Patient Data From Real-World Population Data Using Glioblastoma as an Example",
      "url": "https://arxiv.org/abs/2512.14721",
      "description": "arXiv:2512.14721v1 Announce Type: new \nAbstract: The generation of synthetic data is a promising technology to make medical data available for secondary use in a privacy-compliant manner. A popular method for creating realistic patient data is the rule-based Synthea data generator. Synthea generates...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "research",
        "arxiv",
        "paper",
        "example"
      ],
      "score": 0.6
    },
    {
      "title": "excel-mcp-server - A Model Context Protocol server for Excel file manipulation",
      "url": "https://github.com/haris-musa/excel-mcp-server",
      "description": "A Model Context Protocol server for Excel file manipulation",
      "published_date": "2025-02-12T06:39:48+00:00",
      "source": "GitHub",
      "category": "industry_news",
      "keywords": [
        "context",
        "model"
      ],
      "score": 0.6
    },
    {
      "title": "mcp-agent - Build effective agents using Model Context Protocol and simple workflow patterns",
      "url": "https://github.com/lastmile-ai/mcp-agent",
      "description": "Build effective agents using Model Context Protocol and simple workflow patterns",
      "published_date": "2024-12-18T01:55:10+00:00",
      "source": "GitHub",
      "category": "industry_news",
      "keywords": [
        "context",
        "model"
      ],
      "score": 0.6
    },
    {
      "title": "AlphaCodium - Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "url": "https://github.com/Codium-ai/AlphaCodium",
      "description": "Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "published_date": "2024-01-14T15:17:18+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "prompt engineering",
        "prompt",
        "paper"
      ],
      "score": 0.6
    },
    {
      "title": "Mathematics and Coding are Universal AI Benchmarks",
      "url": "https://arxiv.org/abs/2512.13764",
      "description": "arXiv:2512.13764v1 Announce Type: new \nAbstract: We study the special role of mathematics and coding inside the moduli space of psychometric batteries for AI agents. Building on the AAI framework and GVU dynamics from previous works, we define the Mathematics Fiber and show that, when paired with fo...",
      "published_date": "2025-12-18T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "framework",
        "study",
        "arxiv"
      ],
      "score": 0.4
    },
    {
      "title": "Tokenization in Transformers v5: Simpler, Clearer, and More Modular",
      "url": "https://huggingface.co/blog/tokenizers",
      "description": "...",
      "published_date": "2025-12-18T00:00:00",
      "source": "Hugging Face Blog",
      "category": "prompt_engineering",
      "keywords": [
        "transformer"
      ],
      "score": 0.4
    },
    {
      "title": "optillm - Optimizing inference proxy for LLMs",
      "url": "https://github.com/algorithmicsuperintelligence/optillm",
      "description": "Optimizing inference proxy for LLMs",
      "published_date": "2024-08-22T19:46:07+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "LLM"
      ],
      "score": 0.4
    }
  ]
}