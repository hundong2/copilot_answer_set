<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Context Engineering Daily - Context Management</title>
    <link>https://your-username.github.io/context-engineering-news#context_management</link>
    <description>Latest Context Management news in Context Engineering</description>
    <language>en-us</language>
    <item>
      <title>Hold Onto That Thought: Assessing KV Cache Compression On Reasoning</title>
      <link>https://arxiv.org/abs/2512.12008</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.12008</guid>
      <description>arXiv:2512.12008v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with co...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; context, LLM, reasoning, arxiv, attention | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 16 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Context Management</category>
      <category>context</category>
      <category>LLM</category>
      <category>reasoning</category>
    </item>
    <item>
      <title>BLASST: Dynamic BLocked Attention Sparsity via Softmax Thresholding</title>
      <link>https://arxiv.org/abs/2512.12087</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.12087</guid>
      <description>arXiv:2512.12087v1 Announce Type: new 
Abstract: The growing demand for long-context inference capabilities in Large Language Models (LLMs) has intensified the computational and memory bottlenecks inherent to the standard attention mechanism. To address this challenge, we introduce BLASST, a drop-in...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; context, LLM, arxiv, attention, memory | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 16 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Context Management</category>
      <category>context</category>
      <category>LLM</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving</title>
      <link>https://arxiv.org/abs/2512.11920</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2512.11920</guid>
      <description>arXiv:2512.11920v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; arxiv, LLM, framework, memory, compression | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Tue, 16 Dec 2025 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Context Management</category>
      <category>arxiv</category>
      <category>LLM</category>
      <category>framework</category>
    </item>
  </channel>
</rss>