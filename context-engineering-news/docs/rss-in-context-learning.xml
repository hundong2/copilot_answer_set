<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Context Engineering Daily - In-Context Learning</title>
    <link>https://your-username.github.io/context-engineering-news#in_context_learning</link>
    <description>Latest In-Context Learning news in Context Engineering</description>
    <language>en-us</language>
    <item>
      <title>Verbalized Algorithms</title>
      <link>https://arxiv.org/abs/2509.08150</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2509.08150</guid>
      <description>arXiv:2509.08150v1 Announce Type: new 
Abstract: Instead of querying LLMs in a one-shot manner and hoping to get the right answer for a reasoning task, we propose a paradigm we call \emph{verbalized algorithms} (VAs), which leverage classical algorithms with established theoretical understanding. VA...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; example, arxiv, reasoning, RAG, LLM | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 11 Sep 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>In Context Learning</category>
      <category>example</category>
      <category>arxiv</category>
      <category>reasoning</category>
    </item>
    <item>
      <title>In-Context Learning Enhanced Credibility Transformer</title>
      <link>https://arxiv.org/abs/2509.08122</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2509.08122</guid>
      <description>arXiv:2509.08122v1 Announce Type: new 
Abstract: The starting point of our network architecture is the Credibility Transformer which extends the classical Transformer architecture by a credibility mechanism to improve model learning and predictive performance. This Credibility Transformer learns cre...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; example, context, ICL, arxiv, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 11 Sep 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>In Context Learning</category>
      <category>example</category>
      <category>context</category>
      <category>ICL</category>
    </item>
    <item>
      <title>No-Knowledge Alarms for Misaligned LLMs-as-Judges</title>
      <link>https://arxiv.org/abs/2509.08593</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2509.08593</guid>
      <description>arXiv:2509.08593v1 Announce Type: new 
Abstract: If we use LLMs as judges to evaluate the complex decisions of other LLMs, who or what monitors the judges? Infinite monitoring chains are inevitable whenever we do not know the ground truth of the decisions by experts and we do not want to trust them....&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; arxiv, example, LLM | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 60%&amp;lt;/small&amp;gt;</description>
      <pubDate>Thu, 11 Sep 2025 04:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>In Context Learning</category>
      <category>arxiv</category>
      <category>example</category>
      <category>LLM</category>
    </item>
  </channel>
</rss>