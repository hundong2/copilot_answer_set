{
  "generated_at": "2026-01-21T20:09:54.383834",
  "total_items": 47,
  "items": [
    {
      "title": "Context Discipline and Performance Correlation: Analyzing LLM Performance and Quality Degradation Under Varying Context Lengths",
      "url": "https://arxiv.org/abs/2601.11564",
      "description": "arXiv:2601.11564v1 Announce Type: new \nAbstract: The scaling trend in Large Language Models (LLMs) has prioritized increasing the maximum context window to facilitate complex, long-form reasoning and document analysis. However, managing this expanded context introduces severe computational overhead....",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "context window",
        "research",
        "model",
        "large language model",
        "paper",
        "arxiv",
        "analysis",
        "reasoning",
        "LLM",
        "context",
        "transformer"
      ],
      "score": 1.0
    },
    {
      "title": "Compass-Embedding v4: Robust Contrastive Learning for Multilingual E-commerce Embeddings",
      "url": "https://arxiv.org/abs/2601.11565",
      "description": "arXiv:2601.11565v1 Announce Type: new \nAbstract: As global e-commerce rapidly expands into emerging markets, the lack of high-quality semantic representations for low-resource languages has become a decisive bottleneck for retrieval, recommendation, and search systems. In this work, we present Compa...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "API",
        "retrieval",
        "vision",
        "model",
        "framework",
        "product",
        "arxiv",
        "RAG",
        "alignment",
        "LLM",
        "context",
        "embedding"
      ],
      "score": 1.0
    },
    {
      "title": "Measuring Stability Beyond Accuracy in Small Open-Source Medical Large Language Models for Pediatric Endocrinology",
      "url": "https://arxiv.org/abs/2601.11567",
      "description": "arXiv:2601.11567v1 Announce Type: new \nAbstract: Small open-source medical large language models (LLMs) offer promising opportunities for low-resource deployment and broader accessibility. However, their evaluation is often limited to accuracy on medical multiple choice question (MCQ) benchmarks, an...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "GPT",
        "model",
        "prompt",
        "framework",
        "arxiv",
        "reasoning",
        "LLM",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "An Empirical Analysis of Fine-Tuning Large Language Models on Bioinformatics Literature: PRSGPT and BioStarsGPT",
      "url": "https://arxiv.org/abs/2601.11573",
      "description": "arXiv:2601.11573v1 Announce Type: new \nAbstract: Large language models (LLMs) often lack specialized knowledge for complex bioinformatics applications. We present a reproducible pipeline for fine-tuning LLMs on specialized bioinformatics data, demonstrated through two use cases: PRSGPT, focused on p...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "GPT",
        "model",
        "prompt",
        "arxiv",
        "fine-tuning",
        "analysis",
        "LLM",
        "large language model",
        "tool"
      ],
      "score": 1.0
    },
    {
      "title": "Concept Attractors in LLMs and their Applications",
      "url": "https://arxiv.org/abs/2601.11575",
      "description": "arXiv:2601.11575v1 Announce Type: new \nAbstract: Large language models (LLMs) often map semantically related prompts to similar internal representations at specific layers, even when their surface forms differ widely. We show that this behavior can be explained through Iterated Function Systems (IFS...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "prompt_engineering",
      "keywords": [
        "model",
        "prompt",
        "arxiv",
        "RAG",
        "fine-tuning",
        "LLM",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "LimAgents: Multi-Agent LLMs for Generating Research Limitations",
      "url": "https://arxiv.org/abs/2601.11578",
      "description": "arXiv:2601.11578v1 Announce Type: new \nAbstract: Identifying and articulating limitations is essential for transparent and rigorous scientific research. However, zero-shot large language models (LLMs) approach often produce superficial or general limitation statements (e.g., dataset bias or generali...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "zero-shot",
        "GPT",
        "image",
        "research",
        "experiment",
        "model",
        "framework",
        "large language model",
        "paper",
        "arxiv",
        "RAG",
        "LLM",
        "context",
        "embedding"
      ],
      "score": 1.0
    },
    {
      "title": "Bielik 11B v3: Multilingual Large Language Model for European Languages",
      "url": "https://arxiv.org/abs/2601.11579",
      "description": "arXiv:2601.11579v1 Announce Type: new \nAbstract: We present Bielik 11B v3, a state-of-the-art language model highly optimized for the Polish language, while also maintaining strong capabilities in other European languages. This model extends the Mistral 7B v0.2 architecture, scaled to 11B parameters...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "chain_of_thought",
      "keywords": [
        "model",
        "arxiv",
        "fine-tuning",
        "reasoning",
        "large language model"
      ],
      "score": 1.0
    },
    {
      "title": "Speculative Decoding: Performance or Illusion?",
      "url": "https://arxiv.org/abs/2601.11580",
      "description": "arXiv:2601.11580v1 Announce Type: new \nAbstract: Speculative decoding (SD) has become a popular technique to accelerate Large Language Model (LLM) inference, yet its real-world effectiveness remains unclear as prior evaluations rely on research prototypes and unrealistically small batch sizes. We pr...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "research",
        "model",
        "large language model",
        "study",
        "product",
        "arxiv",
        "RAG",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "Entropic Context Shaping: Information-Theoretic Filtering for Context-Aware LLM Agents",
      "url": "https://arxiv.org/abs/2601.11585",
      "description": "arXiv:2601.11585v1 Announce Type: new \nAbstract: Context engineering for large language model (LLM) agents requires distinguishing pragmatically useful information from misleading distractors. We introduce Entropic Context Shaping (ECS), an information-theoretic framework that measures context utili...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "API",
        "model",
        "framework",
        "large language model",
        "arxiv",
        "RAG",
        "analysis",
        "LLM",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "MIMIC-RD: Can LLMs differentially diagnose rare diseases in real-world clinical settings?",
      "url": "https://arxiv.org/abs/2601.11559",
      "description": "arXiv:2601.11559v1 Announce Type: new \nAbstract: Despite rare diseases affecting 1 in 10 Americans, their differential diagnosis remains challenging. Due to their impressive recall abilities, large language models (LLMs) have been recently explored for differential diagnosis. Existing approaches to ...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "LLM",
        "large language model",
        "model"
      ],
      "score": 1.0
    },
    {
      "title": "Dynamical Systems Analysis Reveals Functional Regimes in Large Language Models",
      "url": "https://arxiv.org/abs/2601.11622",
      "description": "arXiv:2601.11622v1 Announce Type: new \nAbstract: Large language models perform text generation through high-dimensional internal dynamics, yet the temporal organisation of these dynamics remains poorly understood. Most interpretability approaches emphasise static representations or causal interventi...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "GPT",
        "transformer",
        "model",
        "arxiv",
        "analysis",
        "reasoning",
        "large language model",
        "attention"
      ],
      "score": 1.0
    },
    {
      "title": "Reasoning Stabilization Point: A Training-Time Signal for Stable Evidence and Shortcut Reliance",
      "url": "https://arxiv.org/abs/2601.11625",
      "description": "arXiv:2601.11625v1 Announce Type: new \nAbstract: Fine-tuning pretrained language models can improve task performance while subtly altering the evidence a model relies on. We propose a training-time interpretability view that tracks token-level attributions across finetuning epochs. We define explana...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "chain_of_thought",
      "keywords": [
        "model",
        "arxiv",
        "fine-tuning",
        "reasoning",
        "transformer"
      ],
      "score": 1.0
    },
    {
      "title": "PRISM: Learning Design Knowledge from Data for Stylistic Design Improvement",
      "url": "https://arxiv.org/abs/2601.11747",
      "description": "arXiv:2601.11747v1 Announce Type: new \nAbstract: Graphic design often involves exploring different stylistic directions, which can be time-consuming for non-experts. We address this problem of stylistically improving designs based on natural language instructions. While VLMs have shown initial succe...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "experiment",
        "instruction",
        "arxiv",
        "knowledge base",
        "RAG",
        "example",
        "alignment"
      ],
      "score": 1.0
    },
    {
      "title": "Risk-Aware Human-in-the-Loop Framework with Adaptive Intrusion Response for Autonomous Vehicles",
      "url": "https://arxiv.org/abs/2601.11781",
      "description": "arXiv:2601.11781v1 Announce Type: new \nAbstract: Autonomous vehicles must remain safe and effective when encountering rare long-tailed scenarios or cyber-physical intrusions during driving. We present RAIL, a risk-aware human-in-the-loop framework that turns heterogeneous runtime signals into calibr...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "in_context_learning",
      "keywords": [
        "ICL",
        "framework",
        "arxiv",
        "vector",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "A self-evolving multi-role collaborative framework with fine-grained difficulty guidance for innovative mathematical problem generation",
      "url": "https://arxiv.org/abs/2601.11792",
      "description": "arXiv:2601.11792v1 Announce Type: new \nAbstract: Mathematical problem generation (MPG) is a significant research direction in the field of intelligent education. In recent years, the rapid development of large language models (LLMs) has enabled new technological approaches to problem-generation task...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "API",
        "research",
        "experiment",
        "model",
        "framework",
        "large language model",
        "paper",
        "arxiv",
        "fine-tuning",
        "LLM",
        "memory"
      ],
      "score": 1.0
    },
    {
      "title": "AI Co-Scientist for Knowledge Synthesis in Medical Contexts: A Proof of Concept",
      "url": "https://arxiv.org/abs/2601.11825",
      "description": "arXiv:2601.11825v1 Announce Type: new \nAbstract: Research waste in biomedical science is driven by redundant studies, incomplete reporting, and the limited scalability of traditional evidence synthesis workflows. We present an AI co-scientist for scalable and transparent knowledge synthesis based on...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "rag_retrieval",
      "keywords": [
        "retrieval",
        "research",
        "model",
        "augmented",
        "framework",
        "study",
        "arxiv",
        "RAG",
        "reasoning",
        "platform",
        "vector",
        "memory",
        "context",
        "transformer"
      ],
      "score": 1.0
    },
    {
      "title": "CSyMR: Benchmarking Compositional Symbolic Muisc Reasoning With MIR Tool Integration",
      "url": "https://arxiv.org/abs/2601.11556",
      "description": "arXiv:2601.11556v1 Announce Type: new \nAbstract: Large Language Models (LLMs) are leveraged in symbolic music reasoning, yet existing benchmarks emphasize isolated knowledge or atomic analyses rather than the integrative compositional reasoning needed to connect musical structures. To address this, ...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "tools_frameworks",
      "keywords": [
        "experiment",
        "augmented",
        "model",
        "framework",
        "large language model",
        "arxiv",
        "RAG",
        "reasoning",
        "analysis",
        "LLM",
        "library",
        "tool"
      ],
      "score": 1.0
    },
    {
      "title": "AdaFRUGAL: Adaptive Memory-Efficient Training with Dynamic Control",
      "url": "https://arxiv.org/abs/2601.11568",
      "description": "arXiv:2601.11568v1 Announce Type: new \nAbstract: Training Large Language Models (LLMs) is highly memory-intensive due to optimizer state overhead. The FRUGAL framework mitigates this with gradient splitting, but its static hyperparameters -- the subspace ratio ($\\rho$) and update frequency ($T$) -- ...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "model",
        "framework",
        "large language model",
        "arxiv",
        "fine-tuning",
        "LLM",
        "memory"
      ],
      "score": 1.0
    },
    {
      "title": "Discrete Semantic States and Hamiltonian Dynamics in LLM Embedding Spaces",
      "url": "https://arxiv.org/abs/2601.11572",
      "description": "arXiv:2601.11572v1 Announce Type: new \nAbstract: We investigate the structure of Large Language Model (LLM) embedding spaces using mathematical concepts, particularly linear algebra and the Hamiltonian formalism, drawing inspiration from analogies with quantum mechanical systems. Motivated by the ob...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "model",
        "large language model",
        "arxiv",
        "analysis",
        "vector",
        "LLM",
        "tool",
        "embedding"
      ],
      "score": 1.0
    },
    {
      "title": "GRADE: Replacing Policy Gradients with Backpropagation for LLM Alignment",
      "url": "https://arxiv.org/abs/2601.11574",
      "description": "arXiv:2601.11574v1 Announce Type: new \nAbstract: Reinforcement learning from human feedback (RLHF) has become the dominant paradigm for aligning large language models with human preferences. However, policy gradient methods such as PPO suffer from high variance gradient estimates, requiring careful ...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "model",
        "large language model",
        "RLHF",
        "arxiv",
        "alignment",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "A Multimodal Data Processing Pipeline for MIMIC-IV Dataset",
      "url": "https://arxiv.org/abs/2601.11606",
      "description": "arXiv:2601.11606v1 Announce Type: new \nAbstract: The MIMIC-IV dataset is a large, publicly available electronic health record (EHR) resource widely used for clinical machine learning research. It comprises multiple modalities, including structured data, clinical notes, waveforms, and imaging data. W...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "multimodal",
        "research",
        "ICL",
        "release",
        "arxiv",
        "analysis",
        "alignment",
        "embedding"
      ],
      "score": 1.0
    },
    {
      "title": "Auxiliary-predicted Compress Memory Model(ApCM Model): A Neural Memory Storage Model Based on Invertible Compression and Learnable Prediction",
      "url": "https://arxiv.org/abs/2601.11609",
      "description": "arXiv:2601.11609v1 Announce Type: new \nAbstract: Current large language models (LLMs) generally lack an effective runtime memory mechanism,making it difficult to adapt to dynamic and personalized interaction requirements. To address this issue, this paper proposes a novel neural memory storage archi...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "context_management",
      "keywords": [
        "model",
        "compression",
        "large language model",
        "paper",
        "arxiv",
        "RAG",
        "LLM",
        "memory"
      ],
      "score": 1.0
    },
    {
      "title": "Integrating Temporal Context into Streaming Data for Human Activity Recognition in Smart Home",
      "url": "https://arxiv.org/abs/2601.11611",
      "description": "arXiv:2601.11611v1 Announce Type: new \nAbstract: With the global population ageing, it is crucial to enable individuals to live independently and safely in their homes. Using ubiquitous sensors such as Passive InfraRed sensors (PIR) and door sensors is drawing increasing interest for monitoring dail...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "experiment",
        "arxiv",
        "RAG",
        "vector",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "Context-Engineering - \"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" â€” Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "url": "https://github.com/davidkimai/Context-Engineering",
      "description": "\"Context engineering is the delicate art and science of filling the context window with just the right information for the next step.\" â€” Andrej Karpathy. A frontier, first-principles handbook inspired by Karpathy and 3Blue1Brown for moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.",
      "published_date": "2025-06-29T00:16:36+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "prompt",
        "context window",
        "prompt engineering",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "ThinkSound - [NeurIPS 2025] PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "url": "https://github.com/FunAudioLLM/ThinkSound",
      "description": "[NeurIPS 2025] PyTorch implementation of [ThinkSound], a unified framework for generating audio from any modality, guided by Chain-of-Thought (CoT) reasoning.",
      "published_date": "2025-06-27T02:27:00+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "chain-of-thought",
        "framework",
        "reasoning",
        "audio",
        "CoT"
      ],
      "score": 1.0
    },
    {
      "title": "mcp-context-forge - A Model Context Protocol (MCP) Gateway & Registry. Serves as a central management point for tools, resources, and prompts that can be accessed by MCP-compatible LLM applications. Converts REST API endpoints to MCP, composes virtual MCP servers with added security and observability, and converts between protocols (stdio, SSE, Streamable HTTP).",
      "url": "https://github.com/IBM/mcp-context-forge",
      "description": "A Model Context Protocol (MCP) Gateway & Registry. Serves as a central management point for tools, resources, and prompts that can be accessed by MCP-compatible LLM applications. Converts REST API endpoints to MCP, composes virtual MCP servers with added security and observability, and converts between protocols (stdio, SSE, Streamable HTTP).",
      "published_date": "2025-05-08T08:16:59+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "API",
        "model",
        "prompt",
        "LLM",
        "tool",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "PageIndex - ðŸ“‘ PageIndex: Document Index for Vectorless, Reasoning-based RAG",
      "url": "https://github.com/VectifyAI/PageIndex",
      "description": "ðŸ“‘ PageIndex: Document Index for Vectorless, Reasoning-based RAG",
      "published_date": "2025-04-01T10:53:54+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "vector",
        "RAG",
        "reasoning"
      ],
      "score": 1.0
    },
    {
      "title": "Cline-Recursive-Chain-of-Thought-System-CRCT- - A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "url": "https://github.com/RPG-fan/Cline-Recursive-Chain-of-Thought-System-CRCT-",
      "description": "A framework designed to manage context, dependencies, and tasks in large-scale Cline projects within VS Code",
      "published_date": "2025-02-18T15:45:30+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "chain-of-thought",
        "framework",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "airweave - Open-source context retrieval layer for AI agents",
      "url": "https://github.com/airweave-ai/airweave",
      "description": "Open-source context retrieval layer for AI agents",
      "published_date": "2024-12-24T10:00:06+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "retrieval",
        "context"
      ],
      "score": 1.0
    },
    {
      "title": "LightRAG - [EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "url": "https://github.com/HKUDS/LightRAG",
      "description": "[EMNLP2025] \"LightRAG: Simple and Fast Retrieval-Augmented Generation\"",
      "published_date": "2024-10-02T11:57:54+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "retrieval",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "KAG - KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "url": "https://github.com/OpenSPG/KAG",
      "description": "KAG is a logical form-guided reasoning and retrieval framework based on OpenSPG engine and LLMs.  It is used to build logical reasoning and factual Q&A solutions for professional domain knowledge bases. It can effectively overcome the shortcomings of the traditional RAG vector similarity calculation model.",
      "published_date": "2024-09-21T13:56:44+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "retrieval",
        "model",
        "framework",
        "knowledge base",
        "RAG",
        "reasoning",
        "vector",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "Kiln - Build, Evaluate, and Optimize AI Systems. Includes evals, RAG, agents, fine-tuning, synthetic data generation, dataset management, MCP, and more.",
      "url": "https://github.com/Kiln-AI/Kiln",
      "description": "Build, Evaluate, and Optimize AI Systems. Includes evals, RAG, agents, fine-tuning, synthetic data generation, dataset management, MCP, and more.",
      "published_date": "2024-07-23T23:10:13+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "fine-tuning"
      ],
      "score": 1.0
    },
    {
      "title": "graphrag - A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "url": "https://github.com/microsoft/graphrag",
      "description": "A modular graph-based Retrieval-Augmented Generation (RAG) system",
      "published_date": "2024-03-27T17:57:52+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "RAG",
        "retrieval",
        "augmented"
      ],
      "score": 1.0
    },
    {
      "title": "R2R - SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "url": "https://github.com/SciPhi-AI/R2R",
      "description": "SoTA production-ready AI retrieval system. Agentic Retrieval-Augmented Generation (RAG) with a RESTful API.",
      "published_date": "2024-02-12T03:24:27+00:00",
      "source": "GitHub",
      "category": "rag_retrieval",
      "keywords": [
        "API",
        "retrieval",
        "augmented",
        "product",
        "RAG"
      ],
      "score": 1.0
    },
    {
      "title": "openlit - Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. ðŸš€ðŸ’» Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "url": "https://github.com/openlit/openlit",
      "description": "Open source platform for AI Engineering: OpenTelemetry-native LLM Observability, GPU Monitoring, Guardrails, Evaluations, Prompt Management, Vault, Playground. ðŸš€ðŸ’» Integrates with 50+ LLM Providers, VectorDBs, Agent Frameworks and GPUs.",
      "published_date": "2024-01-23T17:40:59+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "prompt",
        "framework",
        "platform",
        "vector",
        "LLM"
      ],
      "score": 1.0
    },
    {
      "title": "Enhancing the QA Model through a Multi-domain Debiasing Framework",
      "url": "https://arxiv.org/abs/2601.11581",
      "description": "arXiv:2601.11581v1 Announce Type: new \nAbstract: Question-answering (QA) models have advanced significantly in machine reading comprehension but often exhibit biases that hinder their performance, particularly with complex queries in adversarial conditions. This study evaluates the ELECTRA-small mod...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "model",
        "framework",
        "study",
        "arxiv",
        "reasoning",
        "context"
      ],
      "score": 0.8
    },
    {
      "title": "Mixture-of-Experts as Soft Clustering: A Dual Jacobian-PCA Spectral Geometry Perspective",
      "url": "https://arxiv.org/abs/2601.11616",
      "description": "arXiv:2601.11616v1 Announce Type: new \nAbstract: Mixture-of-Experts (MoE) architectures are commonly motivated by efficiency and conditional computation, but their effect on the geometry of learned functions and representations remains poorly characterized. In this work, we study MoEs through a geom...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "arxiv",
        "study",
        "RAG"
      ],
      "score": 0.8
    },
    {
      "title": "fastapi_mcp - Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "url": "https://github.com/tadata-org/fastapi_mcp",
      "description": "Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!",
      "published_date": "2025-03-08T11:15:43+00:00",
      "source": "GitHub",
      "category": "tools_frameworks",
      "keywords": [
        "API",
        "tool",
        "context",
        "model"
      ],
      "score": 0.8
    },
    {
      "title": "cosmos-reason1 - Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "url": "https://github.com/nvidia-cosmos/cosmos-reason1",
      "description": "Cosmos-Reason1 models understand the physical common sense and generate appropriate embodied decisions in natural language through long chain-of-thought reasoning processes.",
      "published_date": "2025-03-02T15:23:55+00:00",
      "source": "GitHub",
      "category": "chain_of_thought",
      "keywords": [
        "chain-of-thought",
        "reasoning",
        "model"
      ],
      "score": 0.8
    },
    {
      "title": "POLARIS: Typed Planning and Governed Execution for Agentic AI in Back-Office Automation",
      "url": "https://arxiv.org/abs/2601.11816",
      "description": "arXiv:2601.11816v1 Announce Type: new \nAbstract: Enterprise back office workflows require agentic systems that are auditable, policy-aligned, and operationally predictable, capabilities that generic multi-agent setups often fail to deliver. We present POLARIS (Policy-Aware LLM Agentic Reasoning for ...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "chain_of_thought",
      "keywords": [
        "arxiv",
        "LLM",
        "reasoning",
        "framework"
      ],
      "score": 0.6
    },
    {
      "title": "Hindsight Preference Replay Improves Preference-Conditioned Multi-Objective Reinforcement Learning",
      "url": "https://arxiv.org/abs/2601.11604",
      "description": "arXiv:2601.11604v1 Announce Type: new \nAbstract: Multi-objective reinforcement learning (MORL) enables agents to optimize vector-valued rewards while respecting user preferences. CAPQL, a preference-conditioned actor-critic method, achieves this by conditioning on weight vectors w and restricts data...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "multimodal_context",
      "keywords": [
        "vector",
        "vision",
        "arxiv"
      ],
      "score": 0.6
    },
    {
      "title": "excel-mcp-server - A Model Context Protocol server for Excel file manipulation",
      "url": "https://github.com/haris-musa/excel-mcp-server",
      "description": "A Model Context Protocol server for Excel file manipulation",
      "published_date": "2025-02-12T06:39:48+00:00",
      "source": "GitHub",
      "category": "industry_news",
      "keywords": [
        "context",
        "model"
      ],
      "score": 0.6
    },
    {
      "title": "mcp-agent - Build effective agents using Model Context Protocol and simple workflow patterns",
      "url": "https://github.com/lastmile-ai/mcp-agent",
      "description": "Build effective agents using Model Context Protocol and simple workflow patterns",
      "published_date": "2024-12-18T01:55:10+00:00",
      "source": "GitHub",
      "category": "industry_news",
      "keywords": [
        "context",
        "model"
      ],
      "score": 0.6
    },
    {
      "title": "AlphaCodium - Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "url": "https://github.com/Codium-ai/AlphaCodium",
      "description": "Official implementation for the paper: \"Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\"\"",
      "published_date": "2024-01-14T15:17:18+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "prompt",
        "prompt engineering",
        "paper"
      ],
      "score": 0.6
    },
    {
      "title": "Multi-agent DRL-based Lane Change Decision Model for Cooperative Planning in Mixed Traffic",
      "url": "https://arxiv.org/abs/2601.11809",
      "description": "arXiv:2601.11809v1 Announce Type: new \nAbstract: Connected automated vehicles (CAVs) possess the ability to communicate and coordinate with one another, enabling cooperative platooning that enhances both energy efficiency and traffic flow. However, during the initial stage of CAV deployment, the spa...",
      "published_date": "2026-01-21T05:00:00",
      "source": "arXiv",
      "category": "research_papers",
      "keywords": [
        "model",
        "ICL",
        "framework",
        "study",
        "arxiv"
      ],
      "score": 0.4
    },
    {
      "title": "Differential Transformer V2",
      "url": "https://huggingface.co/blog/microsoft/diff-attn-v2",
      "description": "...",
      "published_date": "2026-01-20T03:20:57",
      "source": "Hugging Face Blog",
      "category": "prompt_engineering",
      "keywords": [
        "transformer"
      ],
      "score": 0.4
    },
    {
      "title": "optillm - Optimizing inference proxy for LLMs",
      "url": "https://github.com/algorithmicsuperintelligence/optillm",
      "description": "Optimizing inference proxy for LLMs",
      "published_date": "2024-08-22T19:46:07+00:00",
      "source": "GitHub",
      "category": "prompt_engineering",
      "keywords": [
        "LLM"
      ],
      "score": 0.4
    }
  ]
}