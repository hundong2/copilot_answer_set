<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Context Engineering Daily - Research Papers</title>
    <link>https://your-username.github.io/context-engineering-news#research_papers</link>
    <description>Latest Research Papers news in Context Engineering</description>
    <language>en-us</language>
    <item>
      <title>MALTopic: Multi-Agent LLM Topic Modeling Framework</title>
      <link>https://arxiv.org/abs/2601.15299</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.15299</guid>
      <description>arXiv:2601.15299v1 Announce Type: new 
Abstract: Topic modeling is a crucial technique for extracting latent themes from unstructured text data, particularly valuable in analyzing survey responses. However, traditional methods often only consider free-text responses and do not natively incorporate s...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, LLM, RAG, arxiv, context | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 23 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>model</category>
      <category>LLM</category>
      <category>RAG</category>
    </item>
    <item>
      <title>Intelligence Degradation in Long-Context LLMs: Critical Threshold Determination via Natural Length Distribution Analysis</title>
      <link>https://arxiv.org/abs/2601.15300</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.15300</guid>
      <description>arXiv:2601.15300v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit catastrophic performance degradation when processing contexts approaching certain critical thresholds, even when information remains relevant. This intelligence degradation-defined as over 30% drop in task performa...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; large language model, model, LLM, paper, experiment | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 23 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>large language model</category>
      <category>model</category>
      <category>LLM</category>
    </item>
    <item>
      <title>No Reliable Evidence of Self-Reported Sentience in Small Large Language Models</title>
      <link>https://arxiv.org/abs/2601.15334</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.15334</guid>
      <description>arXiv:2601.15334v1 Announce Type: new 
Abstract: Whether language models possess sentience has no empirical answer. But whether they believe themselves to be sentient can, in principle, be tested. We do so by querying several open-weights models about their own consciousness, and then verifying thei...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; large language model, GPT, arxiv, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 23 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>large language model</category>
      <category>GPT</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>From Quotes to Concepts: Axial Coding of Political Debates with Ensemble LMs</title>
      <link>https://arxiv.org/abs/2601.15338</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.15338</guid>
      <description>arXiv:2601.15338v1 Announce Type: new 
Abstract: Axial coding is a commonly used qualitative analysis method that enhances document understanding by organizing sentence-level open codes into broader categories. In this paper, we operationalize axial coding with large language models (LLMs). Extendin...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; large language model, model, LLM, paper, embedding | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 23 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>large language model</category>
      <category>model</category>
      <category>LLM</category>
    </item>
    <item>
      <title>Gated Sparse Attention: Combining Computational Efficiency with Training Stability for Long-Context Language Models</title>
      <link>https://arxiv.org/abs/2601.15305</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.15305</guid>
      <description>arXiv:2601.15305v1 Announce Type: new 
Abstract: The computational burden of attention in long-context language models has motivated two largely independent lines of work: sparse attention mechanisms that reduce complexity by attending to selected tokens, and gated attention variants that improve tr...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, attention, experiment, arxiv, context | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 23 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>model</category>
      <category>attention</category>
      <category>experiment</category>
    </item>
    <item>
      <title>Uncovering Latent Bias in LLM-Based Emergency Department Triage Through Proxy Variables</title>
      <link>https://arxiv.org/abs/2601.15306</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.15306</guid>
      <description>arXiv:2601.15306v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have enabled their integration into clinical decision-making; however, hidden biases against patients across racial, social, economic, and clinical backgrounds persist. In this study, we investigate bias...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; large language model, model, LLM, arxiv, context | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 23 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>large language model</category>
      <category>model</category>
      <category>LLM</category>
    </item>
    <item>
      <title>Replayable Financial Agents: A Determinism-Faithfulness Assurance Harness for Tool-Using LLM Agents</title>
      <link>https://arxiv.org/abs/2601.15322</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.15322</guid>
      <description>arXiv:2601.15322v1 Announce Type: new 
Abstract: LLM agents struggle with regulatory audit replay: when asked to reproduce a flagged transaction decision with identical inputs, most deployments fail to return consistent results. This paper introduces the Determinism-Faithfulness Assurance Harness (D...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, LLM, paper, experiment, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 23 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>model</category>
      <category>LLM</category>
      <category>paper</category>
    </item>
    <item>
      <title>Logic Programming on Knowledge Graph Networks And its Application in Medical Domain</title>
      <link>https://arxiv.org/abs/2601.15347</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.15347</guid>
      <description>arXiv:2601.15347v1 Announce Type: new 
Abstract: The rash development of knowledge graph research has brought big driving force to its application in many areas, including the medicine and healthcare domain. However, we have found that the application of some major information processing techniques ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; example, reasoning, paper, vector, experiment | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 23 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>example</category>
      <category>reasoning</category>
      <category>paper</category>
    </item>
    <item>
      <title>GeMM-GAN: A Multimodal Generative Model Conditioned on Histopathology Images and Clinical Descriptions for Gene Expression Profile Generation</title>
      <link>https://arxiv.org/abs/2601.15392</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.15392</guid>
      <description>arXiv:2601.15392v1 Announce Type: new 
Abstract: Biomedical research increasingly relies on integrating diverse data modalities, including gene expression profiles, medical images, and clinical metadata. While medical images and clinical metadata are routinely collected in clinical practice, gene ex...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; framework, model, image, API, vector | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 23 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>framework</category>
      <category>model</category>
      <category>image</category>
    </item>
    <item>
      <title>Language Models Entangle Language and Culture</title>
      <link>https://arxiv.org/abs/2601.15337</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.15337</guid>
      <description>arXiv:2601.15337v1 Announce Type: new 
Abstract: Users should not be systemically disadvantaged by the language they use for interacting with LLMs; i.e. users across languages should get responses of similar quality irrespective of language used. In this work, we create a set of real-world open-ende...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, LLM, arxiv, context, analysis | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 23 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>model</category>
      <category>LLM</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>FedUMM: A General Framework for Federated Learning with Unified Multimodal Models</title>
      <link>https://arxiv.org/abs/2601.15390</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.15390</guid>
      <description>arXiv:2601.15390v1 Announce Type: new 
Abstract: Unified multimodal models (UMMs) are emerging as strong foundation models that can do both generation and understanding tasks in a single architecture. However, they are typically trained in centralized settings where all training and downstream datas...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, paper, fine-tuning, research, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 23 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>model</category>
      <category>paper</category>
      <category>fine-tuning</category>
    </item>
    <item>
      <title>DeepSurvey-Bench: Evaluating Academic Value of Automatically Generated Scientific Survey</title>
      <link>https://arxiv.org/abs/2601.15307</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.15307</guid>
      <description>arXiv:2601.15307v1 Announce Type: new 
Abstract: The rapid development of automated scientific survey generation technology has made it increasingly important to establish a comprehensive benchmark to evaluate the quality of generated surveys.Nearly all existing evaluation benchmarks rely on flawed ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; API, experiment, research, arxiv, analysis | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 80%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 23 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>API</category>
      <category>experiment</category>
      <category>research</category>
    </item>
    <item>
      <title>CASL: Concept-Aligned Sparse Latents for Interpreting Diffusion Models</title>
      <link>https://arxiv.org/abs/2601.15441</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.15441</guid>
      <description>arXiv:2601.15441v1 Announce Type: new 
Abstract: Internal activations of diffusion models encode rich semantic information, but interpreting such representations remains challenging. While Sparse Autoencoders (SAEs) have shown promise in disentangling latent representations, existing SAE-based metho...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, image, experiment, alignment, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 80%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 23 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>model</category>
      <category>image</category>
      <category>experiment</category>
    </item>
    <item>
      <title>Learning from Synthetic Data: Limitations of ERM</title>
      <link>https://arxiv.org/abs/2601.15468</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.15468</guid>
      <description>arXiv:2601.15468v1 Announce Type: new 
Abstract: The prevalence and low cost of LLMs have led to a rise of synthetic content. From review sites to court documents, ``natural&amp;#x27;&amp;#x27; content has been contaminated by data points that appear similar to natural data, but are in fact LLM-generated. In this wor...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, example, LLM, arxiv, study | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 80%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 23 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>model</category>
      <category>example</category>
      <category>LLM</category>
    </item>
    <item>
      <title>Lattice: A Confidence-Gated Hybrid System for Uncertainty-Aware Sequential Prediction with Behavioral Archetypes</title>
      <link>https://arxiv.org/abs/2601.15423</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2601.15423</guid>
      <description>arXiv:2601.15423v1 Announce Type: new 
Abstract: We introduce Lattice, a hybrid sequential prediction system that conditionally activates learned behavioral structure using binary confidence gating. The system clusters behavior windows into behavioral archetypes and uses binary confidence gating to ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; transformer, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 60%&amp;lt;/small&amp;gt;</description>
      <pubDate>Fri, 23 Jan 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>transformer</category>
      <category>arxiv</category>
    </item>
  </channel>
</rss>