<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Context Engineering Daily - Research Papers</title>
    <link>https://your-username.github.io/context-engineering-news#research_papers</link>
    <description>Latest Research Papers news in Context Engineering</description>
    <language>en-us</language>
    <item>
      <title>Overview of PAN 2026: Voight-Kampff Generative AI Detection, Text Watermarking, Multi-Author Writing Style Analysis, Generative Plagiarism Detection, and Reasoning Trajectory Detection</title>
      <link>https://arxiv.org/abs/2602.09147</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.09147</guid>
      <description>arXiv:2602.09147v1 Announce Type: new 
Abstract: The goal of the PAN workshop is to advance computational stylometry and text forensics via objective and reproducible evaluation. In 2026, we run the following five tasks: (1) Voight-Kampff Generative AI Detection, particularly in mixed and obfuscated...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; retrieval, platform, analysis, experiment, LLM | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 11 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>retrieval</category>
      <category>platform</category>
      <category>analysis</category>
    </item>
    <item>
      <title>Don&amp;#x27;t Shoot The Breeze: Topic Continuity Model Using Nonlinear Naive Bayes With Attention</title>
      <link>https://arxiv.org/abs/2602.09312</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.09312</guid>
      <description>arXiv:2602.09312v1 Announce Type: new 
Abstract: Utilizing Large Language Models (LLM) as chatbots in diverse business scenarios often presents the challenge of maintaining topic continuity. Abrupt shifts in topics can lead to poor user experiences and inefficient utilization of computational resour...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; large language model, attention, experiment, model, LLM | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 11 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>large language model</category>
      <category>attention</category>
      <category>experiment</category>
    </item>
    <item>
      <title>Understanding Risk and Dependency in AI Chatbot Use from User Discourse</title>
      <link>https://arxiv.org/abs/2602.09339</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.09339</guid>
      <description>arXiv:2602.09339v1 Announce Type: new 
Abstract: Generative AI systems are increasingly embedded in everyday life, yet empirical understanding of how psychological risk associated with AI use emerges, is experienced, and is regulated by users remains limited. We present a large-scale computational t...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; research, analysis, framework, context, LLM | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 11 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>research</category>
      <category>analysis</category>
      <category>framework</category>
    </item>
    <item>
      <title>Digital Linguistic Bias in Spanish: Evidence from Lexical Variation in LLMs</title>
      <link>https://arxiv.org/abs/2602.09346</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.09346</guid>
      <description>arXiv:2602.09346v1 Announce Type: new 
Abstract: This study examines the extent to which Large Language Models (LLMs) capture geographic lexical variation in Spanish, a language that exhibits substantial regional variation. Treating LLMs as virtual informants, we probe their dialectal knowledge usin...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; large language model, study, LLM, model, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 11 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>large language model</category>
      <category>study</category>
      <category>LLM</category>
    </item>
    <item>
      <title>Unsupervised Cross-Lingual Part-of-Speech Tagging with Monolingual Corpora Only</title>
      <link>https://arxiv.org/abs/2602.09366</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.09366</guid>
      <description>arXiv:2602.09366v1 Announce Type: new 
Abstract: Due to the scarcity of part-of-speech annotated data, existing studies on low-resource languages typically adopt unsupervised approaches for POS tagging. Among these, POS tag projection with word alignment method transfers POS tags from a high-resourc...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; RAG, experiment, framework, arxiv, alignment | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 11 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>RAG</category>
      <category>experiment</category>
      <category>framework</category>
    </item>
    <item>
      <title>LLM-FSM: Scaling Large Language Models for Finite-State Reasoning in RTL Code Generation</title>
      <link>https://arxiv.org/abs/2602.07032</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.07032</guid>
      <description>arXiv:2602.07032v1 Announce Type: new 
Abstract: Finite-state reasoning, the ability to understand and implement state-dependent behavior, is central to hardware design. In this paper, we present LLM-FSM, a benchmark that evaluates how well large language models (LLMs) can recover finite-state machi...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; large language model, prompt, example, experiment, fine-tuning | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 11 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>large language model</category>
      <category>prompt</category>
      <category>example</category>
    </item>
    <item>
      <title>ST-Raptor: An Agentic System for Semi-Structured Table QA</title>
      <link>https://arxiv.org/abs/2602.07034</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.07034</guid>
      <description>arXiv:2602.07034v1 Announce Type: new 
Abstract: Semi-structured table question answering (QA) is a challenging task that requires (1) precise extraction of cell contents and positions and (2) accurate recovery of key implicit logical structures, hierarchical relationships, and semantic associations...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; demonstration, multimodal, analysis, experiment, LLM | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 11 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>demonstration</category>
      <category>multimodal</category>
      <category>analysis</category>
    </item>
    <item>
      <title>Enhanced Graph Transformer with Serialized Graph Tokens</title>
      <link>https://arxiv.org/abs/2602.09065</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.09065</guid>
      <description>arXiv:2602.09065v1 Announce Type: new 
Abstract: Transformers have demonstrated success in graph learning, particularly for node-level tasks. However, existing methods encounter an information bottleneck when generating graph-level representations. The prevalent single token paradigm fails to fully ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; transformer, attention, RAG, experiment, model | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 11 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>transformer</category>
      <category>attention</category>
      <category>RAG</category>
    </item>
    <item>
      <title>Distributed Hybrid Parallelism for Large Language Models: Comparative Study and System Design Guide</title>
      <link>https://arxiv.org/abs/2602.09109</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.09109</guid>
      <description>arXiv:2602.09109v1 Announce Type: new 
Abstract: With the rapid growth of large language models (LLMs), a wide range of methods have been developed to distribute computation and memory across hardware devices for efficient training and inference. While existing surveys provide descriptive overviews ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; research, large language model, API, study, analysis | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 11 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>research</category>
      <category>large language model</category>
      <category>API</category>
    </item>
    <item>
      <title>Benchmarking the Energy Savings with Speculative Decoding Strategies</title>
      <link>https://arxiv.org/abs/2602.09113</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.09113</guid>
      <description>arXiv:2602.09113v1 Announce Type: new 
Abstract: Speculative decoding has emerged as an effective method to reduce latency and inference cost of LLM inferences. However, there has been inadequate attention towards the energy requirements of these models. To address this gap, this paper presents a co...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; attention, analysis, model, LLM, paper | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 100%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 11 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>attention</category>
      <category>analysis</category>
      <category>model</category>
    </item>
    <item>
      <title>Beyond Uniform Credit: Causal Credit Assignment for Policy Optimization</title>
      <link>https://arxiv.org/abs/2602.09331</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.09331</guid>
      <description>arXiv:2602.09331v1 Announce Type: new 
Abstract: Policy gradient methods for language model reasoning, such as GRPO and DAPO, assign uniform credit to all generated tokens - the filler phrase &amp;quot;Let me think&amp;quot; receives the same gradient update as the critical calculation &amp;quot;23 + 45 = 68.&amp;quot; We propose coun...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; research, analysis, experiment, model, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 80%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 11 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>research</category>
      <category>analysis</category>
      <category>experiment</category>
    </item>
    <item>
      <title>Measuring Inclusion in Interaction: Inclusion Analytics for Human-AI Collaborative Learning</title>
      <link>https://arxiv.org/abs/2602.09269</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.09269</guid>
      <description>arXiv:2602.09269v1 Announce Type: new 
Abstract: Inclusion, equity, and access are widely valued in AI and education, yet are often assessed through coarse sample descriptors or post-hoc self-reports that miss how inclusion is shaped moment by moment in collaborative problem solving (CPS). In this p...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; arxiv, experiment, framework, paper | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 60%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 11 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>arxiv</category>
      <category>experiment</category>
      <category>framework</category>
    </item>
    <item>
      <title>PreFlect: From Retrospective to Prospective Reflection in Large Language Model Agents</title>
      <link>https://arxiv.org/abs/2602.07187</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.07187</guid>
      <description>arXiv:2602.07187v1 Announce Type: new 
Abstract: Advanced large language model agents typically adopt self-reflection for improving performance, where agents iteratively analyze past actions to correct errors. However, existing reflective approaches are inherently retrospective: agents act, observe ...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; large language model, model, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 60%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 11 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>large language model</category>
      <category>model</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>DMamba: Decomposition-enhanced Mamba for Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2602.09081</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.09081</guid>
      <description>arXiv:2602.09081v1 Announce Type: new 
Abstract: State Space Models (SSMs), particularly Mamba, have shown potential in long-term time series forecasting. However, existing Mamba-based architectures often struggle with datasets characterized by non-stationary patterns. A key observation from time se...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, experiment, paper, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 60%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 11 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>model</category>
      <category>experiment</category>
      <category>paper</category>
    </item>
    <item>
      <title>From Adam to Adam-Like Lagrangians: Second-Order Nonlocal Dynamics</title>
      <link>https://arxiv.org/abs/2602.09101</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.09101</guid>
      <description>arXiv:2602.09101v1 Announce Type: new 
Abstract: In this paper, we derive an accelerated continuous-time formulation of Adam by modeling it as a second-order integro-differential dynamical system. We relate this inertial nonlocal model to an existing first-order nonlocal Adam flow through an $\alpha...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, paper, arxiv, example | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 40%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 11 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>model</category>
      <category>paper</category>
      <category>arxiv</category>
    </item>
    <item>
      <title>Importance inversion transfer identifies shared principles for cross-domain learning</title>
      <link>https://arxiv.org/abs/2602.09116</link>
      <guid isPermaLink="false">https://arxiv.org/abs/2602.09116</guid>
      <description>arXiv:2602.09116v1 Announce Type: new 
Abstract: The capacity to transfer knowledge across scientific domains relies on shared organizational principles. However, existing transfer-learning methodologies often fail to bridge radically heterogeneous systems, particularly under severe data scarcity or...&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&amp;lt;small&amp;gt;&amp;lt;strong&amp;gt;Source:&amp;lt;/strong&amp;gt; arXiv | &amp;lt;strong&amp;gt;Keywords:&amp;lt;/strong&amp;gt; model, study, framework, arxiv | &amp;lt;strong&amp;gt;Relevance:&amp;lt;/strong&amp;gt; 40%&amp;lt;/small&amp;gt;</description>
      <pubDate>Wed, 11 Feb 2026 05:00:00 </pubDate>
      <author>noreply@contextengineering.news (arXiv)</author>
      <category>Research Papers</category>
      <category>model</category>
      <category>study</category>
      <category>framework</category>
    </item>
  </channel>
</rss>